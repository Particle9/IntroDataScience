{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38af0ad",
   "metadata": {},
   "source": [
    "# 1) Risk & Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51396cc1",
   "metadata": {},
   "source": [
    "### 1.1 Loss & Risk Metrics\n",
    "\n",
    "**What is this?**\n",
    "Loss functions measure how wrong a prediction is compared to the true value. Risk is the average (expected) loss across all predictions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Mean Squared Error (MSE)**: Used for regression. Penalizes large errors heavily. Formula: $(y_{true} - y_{pred})^2$\n",
    "- **Mean Absolute Error (MAE)**: Used for regression. Less sensitive to outliers than MSE. Formula: $|y_{true} - y_{pred}|$\n",
    "- **Zero-One Loss**: Used for classification. Returns 1 if prediction is wrong, 0 if correct.\n",
    "- **Log Loss (Binary Cross-Entropy)**: Used for probabilistic binary classification. Penalizes confident wrong predictions heavily. Formula: $-(y \\log(p) + (1-y)\\log(1-p))$\n",
    "- **Average Risk**: The mean of all losses - tells you overall model performance.\n",
    "\n",
    "**When to use:**\n",
    "- Use MSE when you want to penalize large errors more\n",
    "- Use MAE when outliers shouldn't dominate your loss\n",
    "- Use Zero-One for simple classification accuracy\n",
    "- Use Log Loss when you have probability predictions and want to encourage confident correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6894fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mean Squared Error\n",
    "def loss_squared(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "# Mean Absolute Error\n",
    "def loss_absolute(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    return np.abs(y_true - y_pred)\n",
    "\n",
    "# Zero-One Loss\n",
    "def loss_zero_one(y_true, y_pred_label):\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    y_pred_label = np.asarray(y_pred_label, int)\n",
    "    return (y_true != y_pred_label).astype(float)\n",
    "\n",
    "# Log Loss for Binary Classification\n",
    "def loss_logloss_binary(y_true, y_pred_prob, eps=1e-12):\n",
    "    # y_true in {0,1}, y_pred_prob in [0,1]\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    p = np.asarray(y_pred_prob, float)\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return -(y_true*np.log(p) + (1-y_true)*np.log(1-p))\n",
    "\n",
    "# Average Risk (Expected Loss)\n",
    "def average_risk(y_true, y_pred, loss_fn):\n",
    "    # returns the average loss\n",
    "    losses = loss_fn(y_true, y_pred)\n",
    "    return float(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6b4f766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE losses: [0.01 0.09 0.04 0.04 0.01]\n",
      "Average MSE risk: 0.03799999999999999\n",
      "MAE losses: [0.1 0.3 0.2 0.2 0.1]\n",
      "Average MAE risk: 0.18\n",
      "Zero-One losses: [0. 0. 1. 0. 0.]\n",
      "Average 0-1 risk (error rate): 0.2\n",
      "Log losses: [0.10536052 0.10536052 0.22314355 0.22314355 0.16251893]\n",
      "Average log loss: 0.1639054126883694\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for loss functions and average risk\n",
    "y_true = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred = np.array([1.1, 2.3, 2.8, 4.2, 5.1])\n",
    "\n",
    "# Mean Squared Error\n",
    "mse_losses = loss_squared(y_true, y_pred)\n",
    "print(f\"MSE losses: {mse_losses}\")\n",
    "print(f\"Average MSE risk: {average_risk(y_true, y_pred, loss_squared)}\")\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae_losses = loss_absolute(y_true, y_pred)\n",
    "print(f\"MAE losses: {mae_losses}\")\n",
    "print(f\"Average MAE risk: {average_risk(y_true, y_pred, loss_absolute)}\")\n",
    "\n",
    "# Zero-One Loss (classification)\n",
    "y_true_class = np.array([0, 1, 1, 0, 1])\n",
    "y_pred_class = np.array([0, 1, 0, 0, 1])\n",
    "zo_losses = loss_zero_one(y_true_class, y_pred_class)\n",
    "print(f\"Zero-One losses: {zo_losses}\")\n",
    "print(f\"Average 0-1 risk (error rate): {average_risk(y_true_class, y_pred_class, loss_zero_one)}\")\n",
    "\n",
    "# Log Loss (binary classification)\n",
    "y_true_binary = np.array([1, 0, 1, 0, 1])\n",
    "y_pred_prob = np.array([0.9, 0.1, 0.8, 0.2, 0.85])\n",
    "ll_losses = loss_logloss_binary(y_true_binary, y_pred_prob)\n",
    "print(f\"Log losses: {ll_losses}\")\n",
    "print(f\"Average log loss: {average_risk(y_true_binary, y_pred_prob, loss_logloss_binary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df79af64",
   "metadata": {},
   "source": [
    "### 1.2 Train Test Validation Split\n",
    "\n",
    "**What is this?**\n",
    "Splitting your data into separate sets to train your model and evaluate its performance fairly.\n",
    "\n",
    "**Why we need it:**\n",
    "- **Training set**: Used to train/fit the model\n",
    "- **Validation set**: Used during model development to tune hyperparameters and make decisions\n",
    "- **Test set**: Used only once at the end to get an unbiased estimate of model performance\n",
    "\n",
    "**Key points:**\n",
    "- Default split is 70% train, 15% validation, 15% test\n",
    "- Always shuffle data before splitting (unless time-series)\n",
    "- Use `random_state` for reproducibility\n",
    "- The validation set helps prevent overfitting during model selection\n",
    "- Never touch the test set until final evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cc29f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_test_split(X, y=None, train_frac=0.7, val_frac=0.15, test_frac=0.15, shuffle=True, random_state=None):\n",
    "    \"\"\"\n",
    "    Split X (and optional y) into train/val/test using scikit-learn.\n",
    "    Returns (X_train, X_val, X_test) or (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    fracs = float(train_frac) + float(val_frac) + float(test_frac)\n",
    "    if not np.isclose(fracs, 1.0):\n",
    "        raise ValueError(\"train_frac + val_frac + test_frac must sum to 1.0\")\n",
    "\n",
    "    # first split off the training set\n",
    "    test_plus_val = val_frac + test_frac\n",
    "    if y is None:\n",
    "        X_train, X_temp = train_test_split(\n",
    "            X, train_size=train_frac, test_size=test_plus_val,\n",
    "            random_state=random_state, shuffle=shuffle\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            X, y, train_size=train_frac, test_size=test_plus_val,\n",
    "            random_state=random_state, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "    # handle edge cases where val or test fraction is zero\n",
    "    if np.isclose(test_plus_val, 0.0):\n",
    "        # no val/test portion\n",
    "        X_val, X_test = X_temp[:0], X_temp[:0]\n",
    "        if y is not None:\n",
    "            y_val, y_test = y_temp[:0], y_temp[:0]\n",
    "    elif np.isclose(val_frac, 0.0):\n",
    "        X_val, X_test = X_temp[:0], X_temp\n",
    "        if y is not None:\n",
    "            y_val, y_test = y_temp[:0], y_temp\n",
    "    elif np.isclose(test_frac, 0.0):\n",
    "        X_val, X_test = X_temp, X_temp[:0]\n",
    "        if y is not None:\n",
    "            y_val, y_test = y_temp, y_temp[:0]\n",
    "    else:\n",
    "        # split the temp set into val and test according to their relative proportions\n",
    "        test_rel = test_frac / (val_frac + test_frac)\n",
    "        if y is None:\n",
    "            X_val, X_test = train_test_split(\n",
    "                X_temp, test_size=test_rel, random_state=random_state, shuffle=shuffle\n",
    "            )\n",
    "        else:\n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_temp, y_temp, test_size=test_rel, random_state=random_state, shuffle=shuffle\n",
    "            )\n",
    "\n",
    "    if y is None:\n",
    "        return X_train, X_val, X_test\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52fd3060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 70\n",
      "Validation set size: 15\n",
      "Test set size: 15\n"
     ]
    }
   ],
   "source": [
    "# Sample call for train_val_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create sample data\n",
    "X_sample, y_sample = make_classification(n_samples=100, n_features=5, n_informative=3, random_state=42)\n",
    "\n",
    "# Split with default proportions (70/15/15)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X_sample, y_sample, \n",
    "    train_frac=0.7, val_frac=0.15, test_frac=0.15, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e673d2",
   "metadata": {},
   "source": [
    "### 1.3 Grid Search\n",
    "\n",
    "**What is this?**\n",
    "A systematic way to find the best hyperparameters for your model by trying all combinations of parameter values.\n",
    "\n",
    "**How it works:**\n",
    "1. Define a grid of hyperparameter values to try\n",
    "2. For each combination, train the model using k-fold cross-validation\n",
    "3. Evaluate performance using a scoring metric\n",
    "4. Return the best combination\n",
    "\n",
    "**Key parameters:**\n",
    "- `param_grid`: Dictionary mapping parameter names to lists of values to try\n",
    "- `cv`: Number of cross-validation folds (default: 5)\n",
    "- `scoring`: Metric to optimize (e.g., 'accuracy', 'f1', 'neg_mean_squared_error')\n",
    "- `n_jobs=-1`: Use all CPU cores for parallel processing\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}\n",
    "# This tries 6 combinations: 3 C values × 2 kernels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0905676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search(model, param_grid, X_train, y_train, cv=5, scoring=None, verbose=1, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Perform grid search to find the best hyperparameters for a given model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator object\n",
    "        The model to tune (e.g., sklearn model or any model with fit/predict methods)\n",
    "    param_grid : dict\n",
    "        Dictionary with parameter names (str) as keys and lists of parameter settings to try\n",
    "    X_train : array-like\n",
    "        Training features\n",
    "    y_train : array-like\n",
    "        Training labels\n",
    "    cv : int, default=5\n",
    "        Number of cross-validation folds\n",
    "    scoring : str or callable, default=None\n",
    "        Scoring metric (e.g., 'accuracy', 'neg_mean_squared_error', 'f1')\n",
    "    verbose : int, default=1\n",
    "        Controls the verbosity level\n",
    "    n_jobs : int, default=-1\n",
    "        Number of jobs to run in parallel (-1 uses all processors)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : estimator\n",
    "        The model with the best hyperparameters fitted on the training data\n",
    "    best_params : dict\n",
    "        The best hyperparameters found\n",
    "    best_score : float\n",
    "        The best cross-validation score\n",
    "    grid_search_results : GridSearchCV object\n",
    "        The full GridSearchCV object with all results\n",
    "    \"\"\"\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        n_jobs=n_jobs,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    return grid.best_estimator_, grid.best_params_, grid.best_score_, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "288c06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Sample call for grid_search\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create sample data\n",
    "X_sample, y_sample = make_classification(n_samples=100, n_features=5, random_state=42)\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "best_model, best_params, best_score, grid_results = grid_search(\n",
    "    model=SVC(),\n",
    "    param_grid=param_grid,\n",
    "    X_train=X_sample,\n",
    "    y_train=y_sample,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best cross-validation score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e524f3c",
   "metadata": {},
   "source": [
    "### 1.4 Cross-Validation\n",
    "\n",
    "**What is this?**\n",
    "A resampling technique to evaluate model performance by splitting data into multiple train/test folds.\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "1. Split data into k equal-sized folds\n",
    "2. For each fold i: Train on (k-1) folds, test on fold i\n",
    "3. Average the k test scores\n",
    "\n",
    "**Benefits:**\n",
    "- More reliable than single train/test split\n",
    "- Uses all data for both training and testing\n",
    "- Reduces variance in performance estimates\n",
    "\n",
    "**Common k values:**\n",
    "- k=5: Standard choice (good bias-variance trade-off)\n",
    "- k=10: More computation but less bias\n",
    "- k=n (Leave-One-Out): Maximum data usage but high variance\n",
    "\n",
    "**Stratified CV:** Maintains class proportions in each fold (important for imbalanced data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93cea8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "\n",
    "def cross_validate_model(model, X, y, cv=5, scoring='accuracy', stratified=True):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on a model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator object\n",
    "        The model to evaluate\n",
    "    X : array-like\n",
    "        Features\n",
    "    y : array-like\n",
    "        Target labels\n",
    "    cv : int, default=5\n",
    "        Number of folds\n",
    "    scoring : str, default='accuracy'\n",
    "        Scoring metric\n",
    "    stratified : bool, default=True\n",
    "        Use stratified folds (maintains class proportions)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    scores : array\n",
    "        Cross-validation scores for each fold\n",
    "    mean_score : float\n",
    "        Mean of CV scores\n",
    "    std_score : float\n",
    "        Standard deviation of CV scores\n",
    "    \"\"\"\n",
    "    if stratified and hasattr(y, 'dtype') and y.dtype in [int, 'int64', 'int32']:\n",
    "        cv_splitter = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        cv_splitter = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    scores = cross_val_score(model, X, y, cv=cv_splitter, scoring=scoring)\n",
    "    \n",
    "    return scores, float(np.mean(scores)), float(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93d021a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [1.   0.95 1.   1.   1.  ]\n",
      "Mean score: 0.9900 (+/- 0.0200)\n"
     ]
    }
   ],
   "source": [
    "# Sample call for cross_validate_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create sample data\n",
    "X_sample, y_sample = make_classification(n_samples=100, n_features=5, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores, mean_score, std_score = cross_validate_model(\n",
    "    model=model,\n",
    "    X=X_sample,\n",
    "    y=y_sample,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    stratified=True\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean score: {mean_score:.4f} (+/- {std_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a49e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8c7dd28",
   "metadata": {},
   "source": [
    "# 2) Estimation, Bias-Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45618ce",
   "metadata": {},
   "source": [
    "### 2.1 Monte-Carlo Estimate of Bias/Variance\n",
    "\n",
    "**What is this?**\n",
    "A simulation approach to understand how well an estimator (like sample mean) performs compared to the true value.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Bias**: How far off your estimator is on average from the true value\n",
    "  - Formula: $\\text{Bias} = E[\\hat{\\theta}] - \\theta_{true}$\n",
    "- **Variance**: How much your estimator varies across different samples\n",
    "  - Formula: $\\text{Var}(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2]$\n",
    "- **MSE**: Combines both errors: $\\text{MSE} = \\text{Bias}^2 + \\text{Variance}$\n",
    "\n",
    "**How it works:**\n",
    "1. Generate many random samples (e.g., 2000)\n",
    "2. Compute your estimate on each sample\n",
    "3. Calculate bias (mean of estimates - true value)\n",
    "4. Calculate variance (spread of estimates)\n",
    "\n",
    "**Interpretation:**\n",
    "- Low bias + Low variance = Good estimator ✓\n",
    "- High bias = Systematically wrong (underfitting)\n",
    "- High variance = Unstable predictions (overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "777d4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_bias_variance(estimator_fn, sampler_fn, true_value, n_trials=2000, seed=0):\n",
    "    \"\"\"\n",
    "    estimator_fn(sample) -> estimate (scalar)\n",
    "    sampler_fn(rng) -> one sample dataset (any structure)\n",
    "    true_value: the actual value we're trying to estimate\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ests = []\n",
    "    for _ in range(n_trials):\n",
    "        sample = sampler_fn(rng)\n",
    "        ests.append(estimator_fn(sample))\n",
    "    ests = np.asarray(ests, float)\n",
    "    bias = float(np.mean(ests) - true_value)\n",
    "    variance = float(np.var(ests, ddof=0))\n",
    "    mse = float(np.mean((ests - true_value) ** 2))\n",
    "    return {\"bias\": bias, \"variance\": variance, \"mse\": mse, \"ests\": ests}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8142ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: 0.008174\n",
      "Variance: 0.034397\n",
      "MSE: 0.034464\n"
     ]
    }
   ],
   "source": [
    "# Sample call for estimate_bias_variance\n",
    "# Example: estimating mean of normal distribution\n",
    "true_mean = 5.0\n",
    "estimator_fn = lambda sample: np.mean(sample)\n",
    "sampler_fn = lambda rng: rng.normal(true_mean, 1.0, size=30)\n",
    "\n",
    "result = estimate_bias_variance(estimator_fn, sampler_fn, true_mean, n_trials=1000, seed=42)\n",
    "print(f\"Bias: {result['bias']:.6f}\")\n",
    "print(f\"Variance: {result['variance']:.6f}\")\n",
    "print(f\"MSE: {result['mse']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75589acf",
   "metadata": {},
   "source": [
    "### 2.2 Bootstrap Estimate\n",
    "\n",
    "**What is this?**\n",
    "A resampling technique to estimate the uncertainty of a statistic (like mean, median) without knowing the underlying distribution.\n",
    "\n",
    "**How it works:**\n",
    "1. Take your original dataset of size n\n",
    "2. Create many \"bootstrap samples\" by randomly sampling with replacement\n",
    "3. Compute your statistic on each bootstrap sample\n",
    "4. Analyze the distribution of these statistics\n",
    "\n",
    "**Bootstrap Confidence Interval:**\n",
    "- Uses percentiles of bootstrap estimates\n",
    "- For 95% CI with α=0.05: take 2.5th and 97.5th percentiles\n",
    "- No assumptions about the data distribution needed!\n",
    "\n",
    "**When to use:**\n",
    "- When you don't know the theoretical distribution\n",
    "- When sample size is too small for asymptotic theory\n",
    "- To estimate standard errors of complex statistics\n",
    "\n",
    "**Key insight:** Bootstrap simulates what would happen if you could collect many datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "939c78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_estimates(estimator_fn, data, n_boot=2000, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    data = np.asarray(data)\n",
    "    n = len(data)\n",
    "    ests = np.empty(n_boot, float)\n",
    "    for b in range(n_boot):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        ests[b] = estimator_fn(data[idx])\n",
    "    return ests\n",
    "\n",
    "def bootstrap_ci(ests, alpha=0.05):\n",
    "    ests = np.asarray(ests, float)\n",
    "    lo = float(np.quantile(ests, alpha/2))\n",
    "    hi = float(np.quantile(ests, 1 - alpha/2))\n",
    "    return (lo, hi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d114653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap mean estimate: 31.9324\n",
      "Bootstrap std of mean: 1.7993\n",
      "95% Bootstrap CI for mean: (28.4000, 35.6000)\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for bootstrap functions\n",
    "data_sample = np.array([23, 25, 27, 29, 31, 33, 35, 37, 39, 41])\n",
    "\n",
    "# Bootstrap estimates of the mean\n",
    "ests = bootstrap_estimates(np.mean, data_sample, n_boot=1000, seed=42)\n",
    "print(f\"Bootstrap mean estimate: {np.mean(ests):.4f}\")\n",
    "print(f\"Bootstrap std of mean: {np.std(ests):.4f}\")\n",
    "\n",
    "# Bootstrap confidence interval\n",
    "ci = bootstrap_ci(ests, alpha=0.05)\n",
    "print(f\"95% Bootstrap CI for mean: ({ci[0]:.4f}, {ci[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209c3a9",
   "metadata": {},
   "source": [
    "### 2.3 Mean and Standard Error\n",
    "\n",
    "**What is this?**\n",
    "Measures of central tendency and uncertainty in your sample estimate.\n",
    "\n",
    "**Formulas:**\n",
    "- **Mean**: $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$\n",
    "- **Standard Error**: $SE = \\frac{s}{\\sqrt{n}}$ where $s$ is sample standard deviation\n",
    "\n",
    "**Standard Error vs Standard Deviation:**\n",
    "- **Standard Deviation (s)**: Measures spread of data points\n",
    "- **Standard Error (SE)**: Measures uncertainty in the mean estimate\n",
    "- SE gets smaller as sample size increases: $SE \\propto 1/\\sqrt{n}$\n",
    "\n",
    "**Interpretation:**\n",
    "- Smaller SE = More confident in our mean estimate\n",
    "- 95% CI is approximately: mean ± 2×SE (for large samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56a96d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_se(x):\n",
    "    x = np.asarray(x, float)\n",
    "    m = float(np.mean(x))\n",
    "    se = float(np.std(x, ddof=1) / np.sqrt(len(x)))\n",
    "    return m, se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ec79fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean: 24.3333\n",
      "Standard error: 2.7437\n"
     ]
    }
   ],
   "source": [
    "# Sample call for mean_and_se\n",
    "sample_data = np.array([12, 15, 18, 22, 25, 28, 30, 33, 36])\n",
    "mean, se = mean_and_se(sample_data)\n",
    "print(f\"Sample mean: {mean:.4f}\")\n",
    "print(f\"Standard error: {se:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d6e5a",
   "metadata": {},
   "source": [
    "### 2.4 Confidence Intervals (t-distribution)\n",
    "\n",
    "**What is this?**\n",
    "When sample size is small and population variance is unknown, use t-distribution for more accurate confidence intervals.\n",
    "\n",
    "**Formula:** $\\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot \\frac{s}{\\sqrt{n}}$\n",
    "\n",
    "**Key differences from normal distribution:**\n",
    "- Heavier tails (accounts for uncertainty in variance estimate)\n",
    "- Converges to normal as sample size increases\n",
    "- Degrees of freedom: $df = n - 1$\n",
    "\n",
    "**When to use:**\n",
    "- Sample size < 30 AND population variance unknown\n",
    "- Normality assumption holds (or n ≥ 15 with symmetric data)\n",
    "\n",
    "**Rule of thumb:**\n",
    "- n < 30: Use t-distribution\n",
    "- n ≥ 30: t ≈ normal (can use either)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5dcbb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def confidence_interval_t(x, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Compute confidence interval using t-distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Sample data\n",
    "    confidence : float, default=0.95\n",
    "        Confidence level (e.g., 0.95 for 95% CI)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    (lower, upper) : tuple\n",
    "        Confidence interval bounds\n",
    "    mean : float\n",
    "        Sample mean\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, float)\n",
    "    n = len(x)\n",
    "    mean = float(np.mean(x))\n",
    "    se = float(np.std(x, ddof=1) / np.sqrt(n))\n",
    "    \n",
    "    # Critical value from t-distribution\n",
    "    alpha = 1 - confidence\n",
    "    t_crit = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "    \n",
    "    margin = t_crit * se\n",
    "    return (mean - margin, mean + margin), mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28ca462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean: 22.5000\n",
      "95% t-distribution CI: (16.6101, 28.3899)\n"
     ]
    }
   ],
   "source": [
    "# Sample call for confidence_interval_t\n",
    "small_sample = np.array([15, 18, 21, 24, 27, 30])\n",
    "ci_t, mean_t = confidence_interval_t(small_sample, confidence=0.95)\n",
    "print(f\"Sample mean: {mean_t:.4f}\")\n",
    "print(f\"95% t-distribution CI: ({ci_t[0]:.4f}, {ci_t[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bf928",
   "metadata": {},
   "source": [
    "### 2.5 Central Limit Theorem (CLT)\n",
    "\n",
    "**What is this?**\n",
    "One of the most important theorems in statistics: the sum (or average) of many independent random variables approaches a normal distribution, regardless of the original distribution!\n",
    "\n",
    "**Theorem:** If $X_1, X_2, \\ldots, X_n$ are i.i.d. with mean $\\mu$ and variance $\\sigma^2$, then:\n",
    "$$\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} N(0, 1)$$\n",
    "\n",
    "Or equivalently: $\\bar{X} \\approx N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)$ for large $n$\n",
    "\n",
    "**Why it matters:**\n",
    "- Explains why normal distribution appears everywhere in nature\n",
    "- Justifies using normal-based confidence intervals for large samples\n",
    "- Foundation for hypothesis testing and inference\n",
    "\n",
    "**Key Requirements:**\n",
    "1. Independence: Samples must be independent\n",
    "2. Identical distribution: Same distribution for all $X_i$\n",
    "3. Finite variance: $\\sigma^2 < \\infty$\n",
    "4. Large sample size: Usually $n \\geq 30$ is sufficient\n",
    "\n",
    "**Rule of thumb:**\n",
    "- Original distribution nearly symmetric: $n \\geq 15$ may suffice\n",
    "- Original distribution very skewed: Need $n \\geq 50$ or more\n",
    "- Bernoulli/binary data: Need $np \\geq 5$ and $n(1-p) \\geq 5$\n",
    "\n",
    "**Practical implications:**\n",
    "- Sample means are approximately normal for large $n$\n",
    "- Can use z-tests and normal-based CIs even when data isn't normal\n",
    "- Larger samples → Better normal approximation\n",
    "\n",
    "**When NOT to rely on CLT:**\n",
    "- Heavy-tailed distributions (infinite variance)\n",
    "- Small sample sizes with highly skewed data\n",
    "- Strong dependencies between observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29c9e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clt_normal_approx(sample_mean, population_std, n):\n",
    "    \"\"\"\n",
    "    Get normal approximation parameters for sample mean using CLT.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_mean : float\n",
    "        Observed sample mean (estimate of μ)\n",
    "    population_std : float\n",
    "        Population standard deviation σ\n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mean : float\n",
    "        Mean of approximate normal distribution (= sample_mean)\n",
    "    std : float\n",
    "        Standard deviation of sample mean (= σ/√n)\n",
    "    \"\"\"\n",
    "    std_of_mean = population_std / np.sqrt(n)\n",
    "    return sample_mean, std_of_mean\n",
    "\n",
    "def clt_confidence_interval(sample_mean, population_std, n, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Compute confidence interval for population mean using CLT.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample_mean : float\n",
    "        Observed sample mean\n",
    "    population_std : float\n",
    "        Population standard deviation (known)\n",
    "    n : int\n",
    "        Sample size\n",
    "    confidence : float\n",
    "        Confidence level (e.g., 0.95 for 95% CI)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    (lower, upper) : tuple\n",
    "        Confidence interval bounds\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    std_of_mean = population_std / np.sqrt(n)\n",
    "    alpha = 1 - confidence\n",
    "    z_crit = stats.norm.ppf(1 - alpha/2)\n",
    "    \n",
    "    margin = z_crit * std_of_mean\n",
    "    return (sample_mean - margin, sample_mean + margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fd6695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx. distribution: N(50.00, 1.0000)\n",
      "95% CLT-based CI: (48.0400, 51.9600)\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for CLT functions\n",
    "sample_mean_obs = 50.0\n",
    "population_std_known = 10.0\n",
    "sample_size = 100\n",
    "\n",
    "# Get normal approximation parameters\n",
    "mean_approx, std_approx = clt_normal_approx(sample_mean_obs, population_std_known, sample_size)\n",
    "print(f\"Approx. distribution: N({mean_approx:.2f}, {std_approx:.4f})\")\n",
    "\n",
    "# Compute confidence interval using CLT\n",
    "ci_clt = clt_confidence_interval(sample_mean_obs, population_std_known, sample_size, confidence=0.95)\n",
    "print(f\"95% CLT-based CI: ({ci_clt[0]:.4f}, {ci_clt[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d99512",
   "metadata": {},
   "source": [
    "### 2.6 Log Likelihood\n",
    "\n",
    "**What is this?**\n",
    "The logarithm of the likelihood function, used to measure how well a statistical model fits observed data.\n",
    "\n",
    "**Likelihood Function:** For data $X = \\{x_1, x_2, \\ldots, x_n\\}$ and parameter $\\theta$:\n",
    "$$L(\\theta | X) = \\prod_{i=1}^{n} f(x_i | \\theta)$$\n",
    "\n",
    "**Log Likelihood:** \n",
    "$$\\ell(\\theta | X) = \\log L(\\theta | X) = \\sum_{i=1}^{n} \\log f(x_i | \\theta)$$\n",
    "\n",
    "**Why use log instead of plain likelihood?**\n",
    "1. **Numerical stability**: Products of small probabilities → underflow\n",
    "2. **Computational efficiency**: Sum is faster than product\n",
    "3. **Mathematical convenience**: Derivatives are simpler\n",
    "4. **Monotonic transformation**: Same maximizer as likelihood\n",
    "\n",
    "**Properties:**\n",
    "- Higher log-likelihood = Better fit to data\n",
    "- Log-likelihood is typically negative (since probabilities < 1)\n",
    "- Often reported as negative log-likelihood (NLL) to minimize\n",
    "\n",
    "**Common distributions:**\n",
    "\n",
    "**Gaussian/Normal:** $\\ell = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$\n",
    "\n",
    "**Bernoulli/Binary:** $\\ell = \\sum_{i=1}^{n} [y_i \\log(p) + (1-y_i)\\log(1-p)]$\n",
    "\n",
    "**Poisson:** $\\ell = \\sum_{i=1}^{n} [x_i \\log(\\lambda) - \\lambda - \\log(x_i!)]$\n",
    "\n",
    "**When to use:**\n",
    "- Parameter estimation (MLE)\n",
    "- Model comparison (higher is better)\n",
    "- Deep learning (minimize negative log-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3ea2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_gaussian(data, mu, sigma):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood for Gaussian/Normal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Observed data points\n",
    "    mu : float\n",
    "        Mean parameter\n",
    "    sigma : float\n",
    "        Standard deviation parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Log-likelihood value\n",
    "    \"\"\"\n",
    "    data = np.asarray(data, float)\n",
    "    n = len(data)\n",
    "    \n",
    "    ll = -0.5 * n * np.log(2 * np.pi * sigma**2)\n",
    "    ll -= np.sum((data - mu)**2) / (2 * sigma**2)\n",
    "    \n",
    "    return float(ll)\n",
    "\n",
    "def log_likelihood_bernoulli(labels, p):\n",
    "    \"\"\"\n",
    "    Compute log-likelihood for Bernoulli distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : array-like\n",
    "        Binary outcomes (0 or 1)\n",
    "    p : float\n",
    "        Success probability parameter (0 < p < 1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Log-likelihood value\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels)\n",
    "    eps = 1e-12  # Avoid log(0)\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    \n",
    "    ll = np.sum(labels * np.log(p) + (1 - labels) * np.log(1 - p))\n",
    "    return float(ll)\n",
    "\n",
    "def negative_log_likelihood(log_likelihood):\n",
    "    \"\"\"\n",
    "    Convert log-likelihood to negative log-likelihood (for minimization).\n",
    "    \"\"\"\n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e32f5051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian log-likelihood: -3.1290\n",
      "Negative log-likelihood: 3.1290\n",
      "Bernoulli log-likelihood: -5.3030\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for log likelihood functions\n",
    "# Gaussian log-likelihood\n",
    "gaussian_data = np.array([1.2, 2.3, 1.9, 2.5, 2.1])\n",
    "ll_gauss = log_likelihood_gaussian(gaussian_data, mu=2.0, sigma=0.5)\n",
    "print(f\"Gaussian log-likelihood: {ll_gauss:.4f}\")\n",
    "print(f\"Negative log-likelihood: {negative_log_likelihood(ll_gauss):.4f}\")\n",
    "\n",
    "# Bernoulli log-likelihood\n",
    "binary_data = np.array([1, 1, 0, 1, 0, 1, 1, 0])\n",
    "ll_bern = log_likelihood_bernoulli(binary_data, p=0.6)\n",
    "print(f\"Bernoulli log-likelihood: {ll_bern:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f197f7",
   "metadata": {},
   "source": [
    "### 2.7 Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "**What is this?**\n",
    "A method to estimate parameters by finding values that maximize the likelihood (or log-likelihood) of observing the data.\n",
    "\n",
    "**Principle:** Choose $\\hat{\\theta}$ that makes observed data most probable:\n",
    "$$\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} \\ell(\\theta | X) = \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log f(x_i | \\theta)$$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Write down the log-likelihood function $\\ell(\\theta | X)$\n",
    "2. Take derivative with respect to $\\theta$: $\\frac{d\\ell}{d\\theta}$\n",
    "3. Set equal to zero and solve: $\\frac{d\\ell}{d\\theta} = 0$\n",
    "4. Verify it's a maximum (check second derivative < 0)\n",
    "\n",
    "**Common MLE Solutions:**\n",
    "\n",
    "**Normal distribution:**\n",
    "- $\\hat{\\mu}_{MLE} = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ (sample mean)\n",
    "- $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2$ (biased sample variance)\n",
    "\n",
    "**Bernoulli distribution:**\n",
    "- $\\hat{p}_{MLE} = \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$ (proportion of successes)\n",
    "\n",
    "**Poisson distribution:**\n",
    "- $\\hat{\\lambda}_{MLE} = \\bar{x}$ (sample mean)\n",
    "\n",
    "**Properties of MLE:**\n",
    "- **Consistent**: Converges to true parameter as $n \\to \\infty$\n",
    "- **Asymptotically normal**: $\\hat{\\theta}_{MLE} \\sim N(\\theta, I(\\theta)^{-1}/n)$ for large $n$\n",
    "- **Asymptotically efficient**: Lowest possible variance among consistent estimators\n",
    "- **Invariant**: If $\\hat{\\theta}$ is MLE, then $g(\\hat{\\theta})$ is MLE of $g(\\theta)$\n",
    "\n",
    "**Limitations:**\n",
    "- May not exist or be unique\n",
    "- Can be biased for small samples\n",
    "- Requires knowing the parametric family\n",
    "- May be computationally difficult (need numerical optimization)\n",
    "\n",
    "**When to use:**\n",
    "- Standard approach for parametric estimation\n",
    "- When you have a probabilistic model of your data\n",
    "- Foundation for logistic regression, neural networks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5bb3b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_gaussian(data):\n",
    "    \"\"\"\n",
    "    Compute MLE for Gaussian/Normal distribution parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Observed data points\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mu_hat : float\n",
    "        MLE estimate of mean\n",
    "    sigma_hat : float\n",
    "        MLE estimate of standard deviation (biased estimator)\n",
    "    \"\"\"\n",
    "    data = np.asarray(data, float)\n",
    "    mu_hat = float(np.mean(data))\n",
    "    sigma_hat = float(np.std(data, ddof=0))  # MLE uses ddof=0 (biased)\n",
    "    return mu_hat, sigma_hat\n",
    "\n",
    "def mle_bernoulli(data):\n",
    "    \"\"\"\n",
    "    Compute MLE for Bernoulli distribution parameter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Binary outcomes (0 or 1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    p_hat : float\n",
    "        MLE estimate of success probability\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    p_hat = float(np.mean(data))\n",
    "    return p_hat\n",
    "\n",
    "def mle_poisson(data):\n",
    "    \"\"\"\n",
    "    Compute MLE for Poisson distribution parameter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Count data (non-negative integers)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    lambda_hat : float\n",
    "        MLE estimate of rate parameter\n",
    "    \"\"\"\n",
    "    data = np.asarray(data, float)\n",
    "    lambda_hat = float(np.mean(data))\n",
    "    return lambda_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05905ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian MLE - mu: 2.0000, sigma: 0.2828\n",
      "Bernoulli MLE - p: 0.7000\n",
      "Poisson MLE - lambda: 2.3000\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for MLE functions\n",
    "# MLE for Gaussian\n",
    "gaussian_sample = np.array([1.5, 2.3, 1.8, 2.1, 1.9, 2.4, 2.0])\n",
    "mu_mle, sigma_mle = mle_gaussian(gaussian_sample)\n",
    "print(f\"Gaussian MLE - mu: {mu_mle:.4f}, sigma: {sigma_mle:.4f}\")\n",
    "\n",
    "# MLE for Bernoulli\n",
    "bernoulli_sample = np.array([1, 1, 0, 1, 1, 0, 1, 0, 1, 1])\n",
    "p_mle = mle_bernoulli(bernoulli_sample)\n",
    "print(f\"Bernoulli MLE - p: {p_mle:.4f}\")\n",
    "\n",
    "# MLE for Poisson\n",
    "poisson_sample = np.array([2, 3, 1, 2, 4, 2, 3, 2, 1, 3])\n",
    "lambda_mle = mle_poisson(poisson_sample)\n",
    "print(f\"Poisson MLE - lambda: {lambda_mle:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d42dc",
   "metadata": {},
   "source": [
    "# 3) Sampling and Random Number Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25208f",
   "metadata": {},
   "source": [
    "### 3.1 Basic Random Number Generator\n",
    "\n",
    "**What is this?**\n",
    "Creates a random number generator (RNG) with a seed for reproducible randomness.\n",
    "\n",
    "**Why use a seed?**\n",
    "- Makes your random experiments reproducible\n",
    "- Same seed = Same sequence of \"random\" numbers\n",
    "- Essential for debugging and scientific reproducibility\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "rng = make_rng(seed=42)  # Fixed seed\n",
    "samples = rng.normal(0, 1, size=100)  # Will always give same 100 numbers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65ed07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_rng(seed=0):\n",
    "    return np.random.default_rng(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f06d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random samples: [ 0.30471708 -1.03998411  0.7504512   0.94056472 -1.95103519]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for make_rng\n",
    "rng = make_rng(seed=42)\n",
    "random_samples = rng.normal(0, 1, size=5)\n",
    "print(f\"Random samples: {random_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0e24e",
   "metadata": {},
   "source": [
    "### 3.2 LCG (Linear Congruential Generator)\n",
    "\n",
    "**What is this?**\n",
    "A simple algorithm for generating pseudo-random numbers using a linear recurrence relation.\n",
    "\n",
    "**Formula:** $u_{n+1} = (a \\cdot u_n + c) \\mod m$\n",
    "\n",
    "**How it works:**\n",
    "1. Start with a seed value\n",
    "2. Apply the formula repeatedly\n",
    "3. Normalize by dividing by m to get values in [0, 1]\n",
    "\n",
    "**Parameters:**\n",
    "- `a = 1103515245`: Multiplier\n",
    "- `c = 12345`: Increment\n",
    "- `m = 2^31`: Modulus\n",
    "- These specific values are used in many standard libraries\n",
    "\n",
    "**Limitations:**\n",
    "- Not cryptographically secure\n",
    "- Can have patterns in high dimensions\n",
    "- Modern RNGs (like numpy's) are better for serious work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54fea2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcg(size, seed=1, a=1103515245, c=12345, m=2**31):\n",
    "    u = seed\n",
    "    out = []\n",
    "    for _ in range(size):\n",
    "        u = (a*u + c) % m\n",
    "        out.append(u / m)\n",
    "    return np.array(out, float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebbec4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCG samples: [0.65515405 0.30481432 0.67496063 0.10676848 0.51657445 0.48966634\n",
      " 0.6024722  0.36995476 0.25666706 0.37418221]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for LCG\n",
    "lcg_samples = lcg(size=10, seed=12345)\n",
    "print(f\"LCG samples: {lcg_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978650e9",
   "metadata": {},
   "source": [
    "### 3.3 Inversion Sampling\n",
    "\n",
    "**What is this?**\n",
    "A method to generate random samples from any distribution by using its inverse CDF (cumulative distribution function).\n",
    "\n",
    "**Algorithm:**\n",
    "1. Generate uniform random variable $U \\sim \\text{Uniform}(0,1)$\n",
    "2. Apply inverse CDF: $X = F^{-1}(U)$\n",
    "3. Result: $X$ follows the desired distribution!\n",
    "\n",
    "**Why it works:**\n",
    "- Mathematical property: If $U$ is uniform, then $F^{-1}(U)$ follows distribution $F$\n",
    "- This is called the probability integral transform\n",
    "\n",
    "**When to use:**\n",
    "- When you can compute or approximate the inverse CDF\n",
    "- Examples: Exponential, Weibull, Cauchy distributions\n",
    "\n",
    "**Example - Exponential distribution:**\n",
    "- CDF: $F(x) = 1 - e^{-\\lambda x}$\n",
    "- Inverse: $F^{-1}(u) = -\\frac{1}{\\lambda}\\log(1-u)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27997749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_by_inversion(n, inv_cdf_fn, rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    u = rng.uniform(0, 1, size=n)\n",
    "    return inv_cdf_fn(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b9f7dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential samples (mean=0.4434, expected=0.5)\n"
     ]
    }
   ],
   "source": [
    "# Sample call for sample_by_inversion\n",
    "# Sample from exponential distribution with rate=2.0\n",
    "inv_cdf_exp = lambda u: -np.log(1 - u) / 2.0\n",
    "exp_samples = sample_by_inversion(n=100, inv_cdf_fn=inv_cdf_exp, rng=make_rng(42))\n",
    "print(f\"Exponential samples (mean={np.mean(exp_samples):.4f}, expected=0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251e41c",
   "metadata": {},
   "source": [
    "### 3.4 Rejection Sampling\n",
    "\n",
    "**What is this?**\n",
    "A method to sample from a complex target distribution $f(x)$ by using a simpler proposal distribution $g(x)$.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Sample $x$ from proposal distribution $g$\n",
    "2. Sample $u \\sim \\text{Uniform}(0,1)$\n",
    "3. Accept $x$ if $u \\leq \\frac{f(x)}{M \\cdot g(x)}$, otherwise reject and try again\n",
    "4. Repeat until you have enough accepted samples\n",
    "\n",
    "**Key Requirements:**\n",
    "- Need constant $M$ such that $f(x) \\leq M \\cdot g(x)$ for all $x$\n",
    "- Smaller $M$ = Higher acceptance rate = More efficient\n",
    "- $g(x)$ should be easy to sample from\n",
    "\n",
    "**When to use:**\n",
    "- Target distribution is hard to sample from directly\n",
    "- But you can evaluate its PDF\n",
    "- Common when inverse CDF is not available\n",
    "\n",
    "**Efficiency:**\n",
    "- Acceptance rate = $1/M$\n",
    "- If $M$ is too large, you'll reject most samples (wasteful!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0448d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sample(n, sample_g, pdf_f, pdf_g, M, rng=None, max_tries=10_000_000):\n",
    "    \"\"\"\n",
    "    sample_g(rng) -> one sample x\n",
    "    pdf_f(x) -> target density value\n",
    "    pdf_g(x) -> proposal density value\n",
    "    M: constant such that pdf_f(x) <= M * pdf_g(x) for all x\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    out = []\n",
    "    tries = 0\n",
    "    while len(out) < n and tries < max_tries:\n",
    "        x = sample_g(rng)\n",
    "        u = rng.uniform(0, 1)\n",
    "        accept_prob = pdf_f(x) / (M * pdf_g(x))\n",
    "        if u <= accept_prob:\n",
    "            out.append(x)\n",
    "        tries += 1\n",
    "    if len(out) < n:\n",
    "        raise RuntimeError(\"Rejection sampling failed: increase max_tries or fix M/proposal.\")\n",
    "    return np.array(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f119fac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejection sampling: generated 500 samples\n",
      "Sample mean: 0.2756 (expected=0.2857)\n"
     ]
    }
   ],
   "source": [
    "# Sample call for rejection_sample\n",
    "# Sample from Beta(2,5) using uniform proposal\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# Target: Beta(2,5), Proposal: Uniform(0,1)\n",
    "pdf_f = lambda x: beta_dist.pdf(x, 2, 5)\n",
    "pdf_g = lambda x: 1.0  # Uniform(0,1) density\n",
    "sample_g = lambda rng: rng.uniform(0, 1)\n",
    "M = 2.5  # Envelope constant\n",
    "\n",
    "rejection_samples = rejection_sample(n=500, sample_g=sample_g, pdf_f=pdf_f, pdf_g=pdf_g, M=M, rng=make_rng(42))\n",
    "print(f\"Rejection sampling: generated {len(rejection_samples)} samples\")\n",
    "print(f\"Sample mean: {np.mean(rejection_samples):.4f} (expected={2/(2+5):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd697922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "936b6b34",
   "metadata": {},
   "source": [
    "### 3.5 Importance Sampling\n",
    "\n",
    "**What is this?**\n",
    "A variance reduction technique for Monte Carlo estimation by sampling from a better-chosen distribution.\n",
    "\n",
    "**Problem:** Standard MC can be inefficient when the important region (where $h(x)f(x)$ is large) is rare.\n",
    "\n",
    "**Solution:** Sample from proposal $g(x)$ that concentrates on important regions.\n",
    "\n",
    "**Formula:** $E_f[h(X)] = E_g\\left[h(X)\\frac{f(X)}{g(X)}\\right]$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Sample $x_1, \\ldots, x_n$ from proposal $g$\n",
    "2. Compute importance weights: $w_i = \\frac{f(x_i)}{g(x_i)}$\n",
    "3. Estimate: $\\hat{E}[h(X)] = \\frac{1}{n}\\sum_{i=1}^{n} h(x_i)w_i$\n",
    "\n",
    "**When to use:**\n",
    "- Rare event simulation (e.g., estimating tail probabilities)\n",
    "- When you can't sample directly from $f$\n",
    "- To reduce variance compared to standard MC\n",
    "\n",
    "**Key insight:** Good proposal $g$ puts more samples where $h(x)f(x)$ is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0fa4af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_sampling(n, sample_g, pdf_f, pdf_g, h_fn, rng=None):\n",
    "    \"\"\"\n",
    "    Estimate E_f[h(X)] using importance sampling with proposal g.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n : int\n",
    "        Number of samples\n",
    "    sample_g : callable\n",
    "        Function to sample from proposal: sample_g(rng) -> x\n",
    "    pdf_f : callable\n",
    "        Target density: pdf_f(x) -> float\n",
    "    pdf_g : callable\n",
    "        Proposal density: pdf_g(x) -> float\n",
    "    h_fn : callable\n",
    "        Function to compute expectation of: h_fn(x) -> float\n",
    "    rng : numpy RNG, optional\n",
    "        Random number generator\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    estimate : float\n",
    "        Importance sampling estimate\n",
    "    weights : array\n",
    "        Importance weights for each sample\n",
    "    samples : array\n",
    "        Generated samples\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    \n",
    "    samples = [sample_g(rng) for _ in range(n)]\n",
    "    weights = np.array([pdf_f(x) / pdf_g(x) for x in samples])\n",
    "    h_values = np.array([h_fn(x) for x in samples])\n",
    "    \n",
    "    estimate = float(np.mean(weights * h_values))\n",
    "    \n",
    "    return estimate, weights, np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d5cc8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance sampling estimate of E[X^2]: 0.9653 (expected=1.0)\n"
     ]
    }
   ],
   "source": [
    "# Sample call for importance_sampling\n",
    "# Estimate E[X^2] for X ~ N(0,1) using proposal N(0,2)\n",
    "from scipy.stats import norm\n",
    "\n",
    "pdf_f = lambda x: norm.pdf(x, 0, 1)\n",
    "pdf_g = lambda x: norm.pdf(x, 0, 2)\n",
    "sample_g = lambda rng: rng.normal(0, 2)\n",
    "h_fn = lambda x: x**2\n",
    "\n",
    "estimate, weights, samples = importance_sampling(n=1000, sample_g=sample_g, pdf_f=pdf_f, pdf_g=pdf_g, h_fn=h_fn, rng=make_rng(42))\n",
    "print(f\"Importance sampling estimate of E[X^2]: {estimate:.4f} (expected=1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c105ce7",
   "metadata": {},
   "source": [
    "### 3.6 Monte-Carlo Integration\n",
    "\n",
    "**What is this?**\n",
    "A method to estimate expectations (integrals) using random sampling instead of analytical computation.\n",
    "\n",
    "**Goal:** Estimate $E[h(X)] = \\int h(x)f(x)dx$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Draw samples $x_1, x_2, \\ldots, x_n$ from distribution $f$\n",
    "2. Compute $h(x_i)$ for each sample\n",
    "3. Average: $\\hat{E}[h(X)] = \\frac{1}{n}\\sum_{i=1}^{n} h(x_i)$\n",
    "\n",
    "**Why it works:**\n",
    "- Law of Large Numbers: Sample average converges to true expectation\n",
    "- Accuracy improves with $\\sqrt{n}$\n",
    "\n",
    "**When to use:**\n",
    "- High-dimensional integrals (curse of dimensionality)\n",
    "- Complex functions where analytical integration is impossible\n",
    "- Estimating probabilities, expected values, etc.\n",
    "\n",
    "**Example:** Estimating $\\pi$ by sampling points in a square and counting how many fall in a circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a1ebe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_expectation(samples, h_fn):\n",
    "    samples = np.asarray(samples)\n",
    "    vals = np.array([h_fn(x) for x in samples])\n",
    "    return float(np.mean(vals)), np.asarray(vals, float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbd0a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo estimate of E[X^2]: 28.3358 (expected=29.0000)\n"
     ]
    }
   ],
   "source": [
    "# Sample call for monte_carlo_expectation\n",
    "rng_mc = make_rng(42)\n",
    "samples_mc = rng_mc.normal(5, 2, size=1000)\n",
    "h_func = lambda x: x**2\n",
    "estimate_mc, vals = monte_carlo_expectation(samples_mc, h_func)\n",
    "print(f\"Monte Carlo estimate of E[X^2]: {estimate_mc:.4f} (expected={5**2 + 2**2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36752fb3",
   "metadata": {},
   "source": [
    "### 3.7 Hoeffding's Inequality\n",
    "\n",
    "**What is this?**\n",
    "A probability inequality that gives confidence bounds for the mean of bounded random variables.\n",
    "\n",
    "**Formula:** For $n$ i.i.d. samples $X_i \\in [a,b]$ with sample mean $\\bar{X}$:\n",
    "$$P(|\\bar{X} - \\mu| \\geq \\epsilon) \\leq 2e^{-2n\\epsilon^2/(b-a)^2}$$\n",
    "\n",
    "**Confidence Interval:**\n",
    "With probability $1-\\alpha$, the true mean $\\mu$ is in:\n",
    "$$\\bar{X} \\pm (b-a)\\sqrt{\\frac{\\log(2/\\alpha)}{2n}}$$\n",
    "\n",
    "**Key Properties:**\n",
    "- Works for ANY bounded distribution (no assumptions!)\n",
    "- Confidence interval width decreases as $1/\\sqrt{n}$\n",
    "- Wider range $(b-a)$ = Wider confidence interval\n",
    "\n",
    "**When to use:**\n",
    "- When you don't know the distribution\n",
    "- When you need distribution-free guarantees\n",
    "- Trade-off: Conservative (wider than parametric CIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6a41c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def hoeffding_ci(values, a, b, alpha=0.05):\n",
    "    values = np.asarray(values, float)\n",
    "    n = len(values)\n",
    "    mean = float(np.mean(values))\n",
    "    radius = (b - a) * math.sqrt(math.log(2/alpha) / (2*n))\n",
    "    return (mean - radius, mean + radius), mean, radius\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e79bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean: 5.0668\n",
      "95% Hoeffding CI: (3.7087, 6.4249)\n",
      "Radius: 1.3581\n"
     ]
    }
   ],
   "source": [
    "# Sample call for hoeffding_ci\n",
    "bounded_values = np.random.uniform(0, 10, size=100)\n",
    "ci_hoef, mean_hoef, radius_hoef = hoeffding_ci(bounded_values, a=0, b=10, alpha=0.05)\n",
    "print(f\"Sample mean: {mean_hoef:.4f}\")\n",
    "print(f\"95% Hoeffding CI: ({ci_hoef[0]:.4f}, {ci_hoef[1]:.4f})\")\n",
    "print(f\"Radius: {radius_hoef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "821b6706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoeffding CI: (0.0198, 0.9802), radius=0.4802\n"
     ]
    }
   ],
   "source": [
    "# Sample call for hoeffding_ci\n",
    "bounded_values = np.array([0.2, 0.5, 0.3, 0.8, 0.6, 0.4, 0.7, 0.5])\n",
    "ci_hoeff, mean_hoeff, radius_hoeff = hoeffding_ci(bounded_values, a=0.0, b=1.0, alpha=0.05)\n",
    "print(f\"Hoeffding CI: ({ci_hoeff[0]:.4f}, {ci_hoeff[1]:.4f}), radius={radius_hoeff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b213c",
   "metadata": {},
   "source": [
    "### 3.8 Markov's Inequality\n",
    "\n",
    "**What is this?**\n",
    "A basic probability inequality for non-negative random variables that gives an upper bound on tail probabilities.\n",
    "\n",
    "**Formula:** For non-negative random variable $X$ and $a > 0$:\n",
    "$$P(X \\geq a) \\leq \\frac{E[X]}{a}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Probability that $X$ is at least $a$ times bigger than its mean\n",
    "- Very general: works for ANY non-negative distribution\n",
    "- Trade-off: Often very loose (conservative bound)\n",
    "\n",
    "**Example:**\n",
    "- If average income is \\$50K, probability someone earns ≥ \\$500K is at most 50K/500K = 10%\n",
    "- Actual probability might be much lower!\n",
    "\n",
    "**Key Properties:**\n",
    "- Only requires mean to exist\n",
    "- Useful when distribution is unknown\n",
    "- Foundation for other inequalities (Chebyshev's)\n",
    "\n",
    "**When to use:**\n",
    "- Quick rough bounds when you only know the mean\n",
    "- Proving theoretical results\n",
    "- When tighter bounds (like Hoeffding's) don't apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f55cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_bound(mean_value, threshold):\n",
    "    \"\"\"\n",
    "    Compute Markov's inequality upper bound on P(X >= threshold).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mean_value : float\n",
    "        Expected value E[X], must be positive\n",
    "    threshold : float\n",
    "        Value a > 0 for which we want P(X >= a)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Upper bound on P(X >= threshold)\n",
    "    \"\"\"\n",
    "    if mean_value <= 0 or threshold <= 0:\n",
    "        raise ValueError(\"Both mean_value and threshold must be positive\")\n",
    "    return float(mean_value / threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "daf9e919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov bound: P(X >= 50.0) <= 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Sample call for markov_bound\n",
    "mean_val = 10.0\n",
    "threshold_val = 50.0\n",
    "markov_prob_bound = markov_bound(mean_val, threshold_val)\n",
    "print(f\"Markov bound: P(X >= {threshold_val}) <= {markov_prob_bound:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036aacd",
   "metadata": {},
   "source": [
    "### 3.9 Chebyshev's Inequality\n",
    "\n",
    "**What is this?**\n",
    "A stronger inequality than Markov's that bounds how far a random variable deviates from its mean.\n",
    "\n",
    "**Formula:** For any random variable $X$ with mean $\\mu$ and variance $\\sigma^2$:\n",
    "$$P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}$$\n",
    "\n",
    "Or equivalently:\n",
    "$$P(|X - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}$$\n",
    "\n",
    "**Confidence Interval:**\n",
    "With probability at least $1-\\alpha$, $X$ is within:\n",
    "$$\\mu \\pm \\frac{\\sigma}{\\sqrt{\\alpha}}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- At least 75% of data is within 2 standard deviations ($k=2$)\n",
    "- At least 89% is within 3 standard deviations ($k=3$)\n",
    "- Works for ANY distribution!\n",
    "\n",
    "**Comparison with other inequalities:**\n",
    "- Markov's: Only needs mean, only for non-negative variables\n",
    "- Chebyshev's: Needs mean & variance, works for all variables, tighter bound\n",
    "- Hoeffding's: Needs bounded range, even tighter for sample means\n",
    "\n",
    "**When to use:**\n",
    "- Distribution unknown but you have mean and variance\n",
    "- Quick check for outliers\n",
    "- Proving convergence in probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2f560c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev_bound(variance, epsilon):\n",
    "    \"\"\"\n",
    "    Compute Chebyshev's inequality upper bound on P(|X - mu| >= epsilon).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    variance : float\n",
    "        Variance of the random variable\n",
    "    epsilon : float\n",
    "        Deviation threshold (> 0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Upper bound on P(|X - mu| >= epsilon)\n",
    "    \"\"\"\n",
    "    if variance <= 0 or epsilon <= 0:\n",
    "        raise ValueError(\"Variance and epsilon must be positive\")\n",
    "    return float(variance / (epsilon ** 2))\n",
    "\n",
    "def chebyshev_ci(mean, std, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute confidence interval using Chebyshev's inequality.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mean : float\n",
    "        Mean of the distribution\n",
    "    std : float\n",
    "        Standard deviation\n",
    "    alpha : float, default=0.05\n",
    "        Confidence level (e.g., 0.05 for 95% CI)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    (lower, upper) : tuple\n",
    "        Confidence interval bounds\n",
    "    k : float\n",
    "        Number of standard deviations\n",
    "    \"\"\"\n",
    "    k = 1.0 / np.sqrt(alpha)\n",
    "    margin = k * std\n",
    "    return (mean - margin, mean + margin), k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ddca20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chebyshev bound: P(|X - mu| >= 3.0) <= 0.4444\n",
      "Chebyshev CI: (5.2786, 94.7214), k=4.4721 std deviations\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for Chebyshev inequality\n",
    "variance_val = 4.0\n",
    "epsilon_val = 3.0\n",
    "cheb_bound = chebyshev_bound(variance_val, epsilon_val)\n",
    "print(f\"Chebyshev bound: P(|X - mu| >= {epsilon_val}) <= {cheb_bound:.4f}\")\n",
    "\n",
    "# Chebyshev CI\n",
    "mean_cheb = 50.0\n",
    "std_cheb = 10.0\n",
    "ci_cheb, k_cheb = chebyshev_ci(mean_cheb, std_cheb, alpha=0.05)\n",
    "print(f\"Chebyshev CI: ({ci_cheb[0]:.4f}, {ci_cheb[1]:.4f}), k={k_cheb:.4f} std deviations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a17acc",
   "metadata": {},
   "source": [
    "# 4) Markov Chain & Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33765e93",
   "metadata": {},
   "source": [
    "### 4.1 Validate Transition Matrix\n",
    "\n",
    "**What is this?**\n",
    "Checks if a matrix is a valid Markov chain transition matrix.\n",
    "\n",
    "**Requirements for validity:**\n",
    "1. **Non-negative entries**: All probabilities $P_{ij} \\geq 0$\n",
    "2. **Row stochastic**: Each row sums to 1 (sum of transition probabilities from any state = 1)\n",
    "\n",
    "**Interpretation:**\n",
    "- $P_{ij}$ = Probability of transitioning from state $i$ to state $j$\n",
    "- Each row represents a probability distribution over next states\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "P = [[0.7, 0.3],\n",
    "     [0.4, 0.6]]  # Valid: rows sum to 1, all non-negative\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6cc0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_transition_matrix(P, tol=1e-10):\n",
    "    P = np.asarray(P, float)\n",
    "    if (P < -tol).any():\n",
    "        return False\n",
    "    return np.allclose(P.sum(axis=1), 1.0, atol=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3a526b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid transition matrix: True\n",
      "Invalid transition matrix: False\n"
     ]
    }
   ],
   "source": [
    "# Sample call for is_transition_matrix\n",
    "P_valid = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "P_invalid = np.array([[0.7, 0.4], [0.4, 0.6]])\n",
    "print(f\"Valid transition matrix: {is_transition_matrix(P_valid)}\")\n",
    "print(f\"Invalid transition matrix: {is_transition_matrix(P_invalid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbe1d6",
   "metadata": {},
   "source": [
    "### 4.2 Estimate Transition Matrix from Path\n",
    "\n",
    "**What is this?**\n",
    "Learn the transition probabilities of a Markov chain from observed state sequences.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Count transitions: How many times did we go from state $i$ to state $j$?\n",
    "2. Normalize each row: $\\hat{P}_{ij} = \\frac{\\text{count}(i \\to j)}{\\sum_k \\text{count}(i \\to k)}$\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Path: [0, 1, 1, 0, 1]\n",
    "Transitions: 0→1 (2 times), 1→1 (1 time), 1→0 (1 time)\n",
    "Matrix: P[0,1] = 2/2 = 1.0\n",
    "        P[1,0] = 1/2 = 0.5\n",
    "        P[1,1] = 1/2 = 0.5\n",
    "```\n",
    "\n",
    "**Use case:** Learning transition probabilities from real data (e.g., customer behavior, weather patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40fd442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_matrix(states, n_states=None):\n",
    "    s = np.asarray(states, dtype=int)\n",
    "    if n_states is None:\n",
    "        n_states = int(s.max()) + 1\n",
    "\n",
    "    counts = np.zeros((n_states, n_states), dtype=float)\n",
    "    for a, b in zip(s[:-1], s[1:]):\n",
    "        counts[a, b] += 1\n",
    "\n",
    "    row_sums = counts.sum(axis=1, keepdims=True)\n",
    "    P_hat = np.divide(counts, row_sums, out=np.zeros_like(counts), where=row_sums > 0)\n",
    "    return P_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0c97151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated transition matrix:\n",
      "[[0.25 0.75 0.  ]\n",
      " [0.4  0.4  0.2 ]\n",
      " [0.   0.5  0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for estimate_transition_matrix\n",
    "observed_states = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 2, 2, 1])\n",
    "P_estimated = estimate_transition_matrix(observed_states, n_states=3)\n",
    "print(f\"Estimated transition matrix:\\n{P_estimated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95777db0",
   "metadata": {},
   "source": [
    "### 4.3 Simulate Chain\n",
    "\n",
    "**What is this?**\n",
    "Generate a random path/trajectory through a Markov chain.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Start at initial state $x_0$\n",
    "2. At each step, randomly choose next state according to $P[x_t, \\cdot]$\n",
    "3. Repeat for $T$ steps\n",
    "\n",
    "**Parameters:**\n",
    "- `P`: Transition matrix\n",
    "- `x0`: Starting state\n",
    "- `T`: Number of transitions to simulate\n",
    "\n",
    "**Output:** Sequence of states $[x_0, x_1, x_2, \\ldots, x_T]$\n",
    "\n",
    "**Use cases:**\n",
    "- Simulating random processes (stock prices, weather)\n",
    "- Monte Carlo estimation of chain properties\n",
    "- Testing chain convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f7a4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_chain(P, x0, T, rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    P = np.asarray(P, float)\n",
    "    x = int(x0)\n",
    "    path = [x]\n",
    "    for _ in range(T):\n",
    "        x = rng.choice(P.shape[0], p=P[x])\n",
    "        path.append(int(x))\n",
    "    return np.array(path, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "31e29d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated path: [0 0 0 1 1 0 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for simulate_chain\n",
    "P_sim = np.array([[0.8, 0.2], [0.3, 0.7]])\n",
    "path_sim = simulate_chain(P_sim, x0=0, T=10, rng=make_rng(42))\n",
    "print(f\"Simulated path: {path_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac8b59",
   "metadata": {},
   "source": [
    "### 4.4 N-Step Transition Probability\n",
    "\n",
    "**What is this?**\n",
    "Computes the probability of being in state $j$ after exactly $n$ steps, starting from state $i$.\n",
    "\n",
    "**Formula:**\n",
    "$$P^{(n)}_{ij} = (P^n)_{ij}$$\n",
    "\n",
    "where $P^n$ is the transition matrix raised to the $n$-th power.\n",
    "\n",
    "**Why it works:**\n",
    "- $P^1 = P$: 1-step transition probabilities\n",
    "- $P^2 = P \\times P$: 2-step transition probabilities  \n",
    "- $P^n$: n-step transition probabilities (Chapman-Kolmogorov equation)\n",
    "\n",
    "**Example:**\n",
    "If you're in state 0, what's the probability of being in state 1 after 5 steps?\n",
    "```python\n",
    "prob = n_step_probability(P, start_state=0, end_state=1, n=5)\n",
    "```\n",
    "\n",
    "**Use cases:**\n",
    "- Long-term predictions without computing full stationary distribution\n",
    "- Finite-horizon forecasting (e.g., \"What's my customer state in 6 months?\")\n",
    "- Understanding transient behavior before convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dbbb8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_probability(P, start_state, end_state, n):\n",
    "    \"\"\"\n",
    "    Calculate probability of being in end_state after n steps from start_state.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like, shape (n_states, n_states)\n",
    "        Transition probability matrix\n",
    "    start_state : int\n",
    "        Starting state index\n",
    "    end_state : int\n",
    "        Target state index\n",
    "    n : int\n",
    "        Number of steps\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Probability of transition from start_state to end_state in n steps\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    P_n = np.linalg.matrix_power(P, n)\n",
    "    return P_n[start_state, end_state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcbafee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being in state 1 after 5 steps from state 0: 0.4275\n"
     ]
    }
   ],
   "source": [
    "# Sample call for n_step_probability\n",
    "P_nstep = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "prob_5step = n_step_probability(P_nstep, start_state=0, end_state=1, n=5)\n",
    "print(f\"Probability of being in state 1 after 5 steps from state 0: {prob_5step:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7eef13",
   "metadata": {},
   "source": [
    "### 4.5 First Passage Time Probability\n",
    "\n",
    "**What is this?**\n",
    "Computes the probability of reaching state $j$ **for the first time** after exactly $n$ steps, starting from state $i$.\n",
    "\n",
    "**Difference from n-step probability:**\n",
    "- **N-step probability**: Probability of being in state $j$ after $n$ steps (may have visited before)\n",
    "- **First passage time**: Probability of reaching state $j$ for the **first time** at step $n$\n",
    "\n",
    "**Formula:**\n",
    "$$f^{(n)}_{ij} = P^{(n)}_{ij} - \\sum_{k=1}^{n-1} f^{(k)}_{ij} \\cdot P^{(n-k)}_{jj}$$\n",
    "\n",
    "where:\n",
    "- $f^{(1)}_{ij} = P_{ij}$ (1-step is always first time if $i \\neq j$)\n",
    "- For $i = j$: first return time to the same state\n",
    "\n",
    "**Example:**\n",
    "If you start in state 0, what's the probability you reach state 2 for the first time at exactly step 5?\n",
    "```python\n",
    "prob = first_passage_probability(P, start_state=0, target_state=2, n=5)\n",
    "```\n",
    "\n",
    "**Use cases:**\n",
    "- Time-to-event analysis (e.g., \"When will customer first churn?\")\n",
    "- Expected hitting times\n",
    "- Analyzing recurrence properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d35c0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_passage_probability(P, start_state, target_state, n):\n",
    "    \"\"\"\n",
    "    Calculate probability of reaching target_state for the FIRST time \n",
    "    at exactly step n, starting from start_state.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like, shape (n_states, n_states)\n",
    "        Transition probability matrix\n",
    "    start_state : int\n",
    "        Starting state index\n",
    "    target_state : int\n",
    "        Target state index to reach for first time\n",
    "    n : int\n",
    "        Number of steps (must reach at exactly this step)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float : First passage probability\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    \n",
    "    if n == 1:\n",
    "        # First step: just direct transition probability\n",
    "        return P[start_state, target_state]\n",
    "    \n",
    "    # Compute first passage probabilities recursively\n",
    "    f = {}  # f[(k, i, j)] = first passage prob from i to j at step k\n",
    "    \n",
    "    # Base case: 1-step first passage\n",
    "    for i in range(P.shape[0]):\n",
    "        for j in range(P.shape[0]):\n",
    "            f[(1, i, j)] = P[i, j]\n",
    "    \n",
    "    # Recursive computation for steps 2 to n\n",
    "    for k in range(2, n + 1):\n",
    "        for i in range(P.shape[0]):\n",
    "            for j in range(P.shape[0]):\n",
    "                # P^(k)_{ij} - sum of (first reach j at step m) * (stay around j until step k)\n",
    "                P_k_ij = n_step_probability(P, i, j, k)\n",
    "                correction = sum(f[(m, i, j)] * n_step_probability(P, j, j, k - m) \n",
    "                               for m in range(1, k))\n",
    "                f[(k, i, j)] = P_k_ij - correction\n",
    "    \n",
    "    return f[(n, start_state, target_state)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0b32bbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of first reaching state 1 at exactly step 3: 0.1470\n"
     ]
    }
   ],
   "source": [
    "# Sample call for first_passage_probability\n",
    "P_fpp = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "fpp = first_passage_probability(P_fpp, start_state=0, target_state=1, n=3)\n",
    "print(f\"Probability of first reaching state 1 at exactly step 3: {fpp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e6010",
   "metadata": {},
   "source": [
    "### 4.6 K-th Passage Time Probability\n",
    "\n",
    "**What is this?**\n",
    "Computes the probability of reaching state $j$ **for the k-th time** after exactly $n$ steps, starting from state $i$.\n",
    "\n",
    "**Generalizes first passage time:**\n",
    "- $k=1$: First time reaching the state\n",
    "- $k=2$: Second time reaching the state\n",
    "- $k=3$: Third time reaching the state, etc.\n",
    "\n",
    "**Formula (recursive):**\n",
    "$$f^{(n)}_k(i \\to j) = \\sum_{m=1}^{n-1} f^{(m)}_{k-1}(i \\to j) \\cdot f^{(n-m)}_1(j \\to j)$$\n",
    "\n",
    "where:\n",
    "- $f^{(n)}_1(i \\to j)$ is the first passage probability\n",
    "- $f^{(n)}_k(i \\to j)$ builds on previous visits\n",
    "\n",
    "**Intuition:**\n",
    "To reach $j$ for the k-th time at step $n$:\n",
    "1. Reach $j$ for the (k-1)-th time at some earlier step $m$\n",
    "2. Then reach $j$ again (first return) in the remaining $n-m$ steps\n",
    "\n",
    "**Example:**\n",
    "What's the probability of visiting state 1 for the **second time** at exactly step 10?\n",
    "```python\n",
    "prob = kth_passage_probability(P, start_state=0, target_state=1, k=2, n=10)\n",
    "```\n",
    "\n",
    "**Use cases:**\n",
    "- Repeated event analysis (e.g., \"When will customer churn for the 2nd time?\")\n",
    "- Multi-visit patterns\n",
    "- Understanding state recurrence frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3761249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kth_passage_probability(P, start_state, target_state, k, n):\n",
    "    \"\"\"\n",
    "    Calculate probability of reaching target_state for the K-TH time \n",
    "    at exactly step n, starting from start_state.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like, shape (n_states, n_states)\n",
    "        Transition probability matrix\n",
    "    start_state : int\n",
    "        Starting state index\n",
    "    target_state : int\n",
    "        Target state index\n",
    "    k : int\n",
    "        Which visit (1=first time, 2=second time, etc.)\n",
    "    n : int\n",
    "        Number of steps (must reach for k-th time at exactly this step)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float : K-th passage probability\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    \n",
    "    if k < 1 or n < k:\n",
    "        return 0.0  # Impossible to have k visits in less than k steps\n",
    "    \n",
    "    # Memoization: f_cache[(visit, step, i, j)] = prob of visit-th passage from i to j at step\n",
    "    f_cache = {}\n",
    "    \n",
    "    def f_passage(visit, step, i, j):\n",
    "        \"\"\"Compute visit-th passage probability from i to j at exactly step.\"\"\"\n",
    "        if (visit, step, i, j) in f_cache:\n",
    "            return f_cache[(visit, step, i, j)]\n",
    "        \n",
    "        if visit == 1:\n",
    "            # First passage: use the previous function logic\n",
    "            result = first_passage_probability(P, i, j, step)\n",
    "        else:\n",
    "            # K-th passage: sum over all ways to reach (k-1)-th visit, then first return\n",
    "            result = 0.0\n",
    "            for m in range(visit - 1, step):  # Need at least (visit-1) steps for (k-1) visits\n",
    "                # Reach j for (visit-1)-th time at step m\n",
    "                prob_prev = f_passage(visit - 1, m, i, j)\n",
    "                # Then first return from j to j in (step - m) steps\n",
    "                if step - m > 0:\n",
    "                    prob_return = first_passage_probability(P, j, j, step - m)\n",
    "                    result += prob_prev * prob_return\n",
    "        \n",
    "        f_cache[(visit, step, i, j)] = result\n",
    "        return result\n",
    "    \n",
    "    return f_passage(k, n, start_state, target_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d650c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 2nd visit to state 1 at step 6: 0.0926\n"
     ]
    }
   ],
   "source": [
    "# Sample call for kth_passage_probability\n",
    "P_kpp = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "kpp_2nd = kth_passage_probability(P_kpp, start_state=0, target_state=1, k=2, n=6)\n",
    "print(f\"Probability of 2nd visit to state 1 at step 6: {kpp_2nd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e54546",
   "metadata": {},
   "source": [
    "### 4.7 Irreducibility Check\n",
    "\n",
    "**What is this?**\n",
    "A Markov chain is **irreducible** if you can reach any state from any other state (eventually).\n",
    "\n",
    "**Why it matters:**\n",
    "- Irreducible chains have a unique stationary distribution\n",
    "- Non-irreducible chains can get \"stuck\" in subsets of states\n",
    "\n",
    "**Algorithm:**\n",
    "Uses graph reachability: For each state, check if all other states are reachable following the transition graph.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Irreducible: [0→1→2→0] (circular)\n",
    "Not irreducible: [0⇄1] [2⇄3] (two separate groups)\n",
    "```\n",
    "\n",
    "**Testing:** The function performs BFS/DFS from each state to verify full connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "89d92a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_irreducible(P):\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    adj = (P > 0)\n",
    "\n",
    "    def reachable(start):\n",
    "        seen = set([start])\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            u = stack.pop()\n",
    "            for v in np.where(adj[u])[0]:\n",
    "                if v not in seen:\n",
    "                    seen.add(int(v))\n",
    "                    stack.append(int(v))\n",
    "        return seen\n",
    "\n",
    "    for s in range(n):\n",
    "        if len(reachable(s)) != n:\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c6111ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First matrix irreducible: True\n",
      "Second matrix irreducible: False\n"
     ]
    }
   ],
   "source": [
    "# Sample call for is_irreducible\n",
    "P_irr = np.array([[0.8, 0.2], [0.3, 0.7]])\n",
    "P_not_irr = np.array([[1.0, 0.0], [0.5, 0.5]])\n",
    "print(f\"First matrix irreducible: {is_irreducible(P_irr)}\")\n",
    "print(f\"Second matrix irreducible: {is_irreducible(P_not_irr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3d5e3f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mat_A = np.array([[0.8,0.2,0,0],\n",
    "                  [0.6,0.2,0.2,0],\n",
    "                  [0,0.4,0,0.6],\n",
    "                  [0,0,0.8,0.2]])\n",
    "\n",
    "is_irreducible(Mat_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddf41b",
   "metadata": {},
   "source": [
    "### 4.8 Communicating Classes\n",
    "\n",
    "**What is this?**\n",
    "States $i$ and $j$ **communicate** if you can go from $i$ to $j$ and from $j$ to $i$ (not necessarily in one step).\n",
    "\n",
    "**Communicating Class:**\n",
    "A maximal set of states that all communicate with each other.\n",
    "\n",
    "**Properties:**\n",
    "- Communication is an equivalence relation (reflexive, symmetric, transitive)\n",
    "- Every state belongs to exactly one communicating class\n",
    "- Irreducible chain = Single communicating class (all states communicate)\n",
    "\n",
    "**Why it matters:**\n",
    "- Identifies independent parts of the chain\n",
    "- States in different classes don't interact\n",
    "- Each class can have its own stationary distribution\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "States: {0, 1, 2, 3}\n",
    "Transitions: 0⇄1, 2⇄3 (but no path between {0,1} and {2,3})\n",
    "Classes: {0, 1} and {2, 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c310cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_communicating_classes(P):\n",
    "    \"\"\"\n",
    "    Find all communicating classes in a Markov chain.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    classes : list of sets\n",
    "        List of communicating classes (each class is a set of state indices)\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    adj = (P > 0)\n",
    "    \n",
    "    def reachable_from(start):\n",
    "        \"\"\"Find all states reachable from start.\"\"\"\n",
    "        seen = set([start])\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            u = stack.pop()\n",
    "            for v in np.where(adj[u])[0]:\n",
    "                if v not in seen:\n",
    "                    seen.add(int(v))\n",
    "                    stack.append(int(v))\n",
    "        return seen\n",
    "    \n",
    "    # Find communicating classes using Union-Find approach\n",
    "    classes = []\n",
    "    visited = set()\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i in visited:\n",
    "            continue\n",
    "        \n",
    "        # Find all states reachable from i\n",
    "        reachable_i = reachable_from(i)\n",
    "        \n",
    "        # Find states that can reach i (i.e., i is reachable from them)\n",
    "        can_reach_i = set()\n",
    "        for j in range(n):\n",
    "            if i in reachable_from(j):\n",
    "                can_reach_i.add(j)\n",
    "        \n",
    "        # Communicating class = intersection\n",
    "        comm_class = reachable_i & can_reach_i\n",
    "        \n",
    "        classes.append(comm_class)\n",
    "        visited.update(comm_class)\n",
    "    \n",
    "    return classes\n",
    "\n",
    "def classify_chain_structure(P):\n",
    "    \"\"\"\n",
    "    Classify the structure of a Markov chain.\n",
    "    \n",
    "    Returns information about communicating classes and irreducibility.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    classes = find_communicating_classes(P)\n",
    "    \n",
    "    is_irreducible_chain = (len(classes) == 1)\n",
    "    \n",
    "    return {\n",
    "        \"num_classes\": len(classes),\n",
    "        \"classes\": classes,\n",
    "        \"is_irreducible\": is_irreducible_chain,\n",
    "        \"class_sizes\": [len(c) for c in classes]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2e403d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communicating classes: [{0, 1}, {2, 3}]\n",
      "Number of classes: 2\n",
      "Is irreducible: False\n"
     ]
    }
   ],
   "source": [
    "# Sample call for find_communicating_classes\n",
    "P_comm = np.array([[0.5, 0.5, 0, 0], \n",
    "                   [0.5, 0.5, 0, 0],\n",
    "                   [0, 0, 0.6, 0.4],\n",
    "                   [0, 0, 0.3, 0.7]])\n",
    "classes = find_communicating_classes(P_comm)\n",
    "print(f\"Communicating classes: {classes}\")\n",
    "\n",
    "# Classify chain structure\n",
    "structure = classify_chain_structure(P_comm)\n",
    "print(f\"Number of classes: {structure['num_classes']}\")\n",
    "print(f\"Is irreducible: {structure['is_irreducible']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570fcda",
   "metadata": {},
   "source": [
    "### 4.9 Transient and Recurrent States\n",
    "\n",
    "**What is this?**\n",
    "Classification of states based on whether the chain will definitely return to them.\n",
    "\n",
    "**Definitions:**\n",
    "- **Recurrent state**: Starting from state $i$, the chain returns to $i$ with probability 1\n",
    "  - Guaranteed to revisit infinitely often\n",
    "- **Transient state**: Starting from state $i$, there's a chance of never returning\n",
    "  - Visited only finitely many times (eventually leaves forever)\n",
    "\n",
    "**Mathematical Test:**\n",
    "State $i$ is recurrent if and only if:\n",
    "$$\\sum_{n=1}^{\\infty} P^n_{ii} = \\infty$$\n",
    "\n",
    "**Key Properties:**\n",
    "- In a finite irreducible chain, all states are recurrent\n",
    "- Transient states lead to recurrent states\n",
    "- If state $i$ is recurrent and communicates with $j$, then $j$ is also recurrent\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Random walk on integers: All states are recurrent (1D)\n",
    "Random walk on 2D grid: All states are recurrent\n",
    "Random walk on 3D grid: All states are transient!\n",
    "```\n",
    "\n",
    "**Practical importance:**\n",
    "- Recurrent states form the \"core\" of the chain\n",
    "- Transient states are temporary (system eventually settles in recurrent states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3d44f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_states_transient_recurrent(P, max_steps=1000, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Classify states as transient or recurrent using finite approximation.\n",
    "    \n",
    "    For finite chains: A state is recurrent if starting from it, you can return to it.\n",
    "    More precisely, we check if the sum of return probabilities diverges.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    max_steps : int, default=1000\n",
    "        Maximum number of steps to check\n",
    "    tol : float, default=1e-10\n",
    "        Tolerance for numerical comparisons\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    classification : dict\n",
    "        Dictionary with 'recurrent' and 'transient' state sets\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    recurrent = set()\n",
    "    transient = set()\n",
    "    \n",
    "    # For each state, compute sum of P^n[i,i]\n",
    "    P_power = P.copy()\n",
    "    cumulative_return_prob = np.diag(P).copy()\n",
    "    \n",
    "    for step in range(2, max_steps + 1):\n",
    "        P_power = P_power @ P\n",
    "        cumulative_return_prob += np.diag(P_power)\n",
    "    \n",
    "    # In finite chains, we use a heuristic:\n",
    "    # If sum is very large, state is recurrent\n",
    "    # More precisely: check if state can reach itself\n",
    "    for i in range(n):\n",
    "        # Check if i can return to itself\n",
    "        can_return = cumulative_return_prob[i] > tol\n",
    "        \n",
    "        # For finite chains: state is recurrent if it's in a closed communicating class\n",
    "        # Simple heuristic: if cumulative return probability is high, it's recurrent\n",
    "        if cumulative_return_prob[i] > max_steps * 0.01:  # Heuristic threshold\n",
    "            recurrent.add(i)\n",
    "        else:\n",
    "            transient.add(i)\n",
    "    \n",
    "    return {\n",
    "        \"recurrent\": recurrent,\n",
    "        \"transient\": transient,\n",
    "        \"cumulative_return_probs\": cumulative_return_prob\n",
    "    }\n",
    "\n",
    "def find_absorbing_states(P, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Find absorbing states in a Markov chain.\n",
    "    \n",
    "    An absorbing state is a state that, once entered, cannot be left (P[i,i] = 1).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    tol : float, default=1e-10\n",
    "        Tolerance for checking P[i,i] = 1\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    absorbing_states : set\n",
    "        Set of absorbing state indices\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    absorbing = set()\n",
    "    for i in range(n):\n",
    "        if abs(P[i, i] - 1.0) < tol:\n",
    "            absorbing.add(i)\n",
    "    \n",
    "    return absorbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "02c64364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent states: {1, 2}\n",
      "Transient states: {0}\n",
      "Absorbing states: {2}\n"
     ]
    }
   ],
   "source": [
    "# Sample call for classify_states_transient_recurrent\n",
    "P_trans = np.array([[0.5, 0.5, 0],\n",
    "                    [0, 0.6, 0.4],\n",
    "                    [0, 0, 1.0]])  # State 2 is absorbing\n",
    "classification = classify_states_transient_recurrent(P_trans, max_steps=100)\n",
    "print(f\"Recurrent states: {classification['recurrent']}\")\n",
    "print(f\"Transient states: {classification['transient']}\")\n",
    "\n",
    "# Find absorbing states\n",
    "absorbing = find_absorbing_states(P_trans)\n",
    "print(f\"Absorbing states: {absorbing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de623e7",
   "metadata": {},
   "source": [
    "### 4.10 Absorbing States and Absorbing Chains\n",
    "\n",
    "**What is this?**\n",
    "An **absorbing state** is a state you can never leave once you enter it.\n",
    "\n",
    "**Mathematical definition:** State $i$ is absorbing if $P_{ii} = 1$ (and $P_{ij} = 0$ for $j \\neq i$)\n",
    "\n",
    "**Absorbing Chain:**\n",
    "A chain with:\n",
    "1. At least one absorbing state\n",
    "2. From every state, you can reach some absorbing state\n",
    "\n",
    "**Standard Form:**\n",
    "Absorbing chains can be written as:\n",
    "$$P = \\begin{pmatrix} Q & R \\\\ 0 & I \\end{pmatrix}$$\n",
    "\n",
    "Where:\n",
    "- $Q$: Transitions between transient states\n",
    "- $R$: Transitions from transient to absorbing\n",
    "- $I$: Identity matrix (absorbing states stay put)\n",
    "\n",
    "**Fundamental Matrix:** $N = (I - Q)^{-1}$\n",
    "- $N_{ij}$ = Expected number of times in transient state $j$ starting from $i$\n",
    "\n",
    "**Absorption Probabilities:** $B = NR$\n",
    "- $B_{ij}$ = Probability of absorbing into state $j$ starting from transient state $i$\n",
    "\n",
    "**Examples:**\n",
    "- Gambler's ruin: Broke ($0) and Rich (target) are absorbing\n",
    "- Random walk with barriers\n",
    "- Customer churn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "606dc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_absorbing_chain(P, absorbing_states=None, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Analyze an absorbing Markov chain.\n",
    "    \n",
    "    Computes fundamental matrix and absorption probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    absorbing_states : set or list, optional\n",
    "        Indices of absorbing states. If None, automatically detected.\n",
    "    tol : float, default=1e-10\n",
    "        Tolerance for numerical operations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result : dict\n",
    "        Dictionary containing:\n",
    "        - 'absorbing_states': Set of absorbing state indices\n",
    "        - 'transient_states': List of transient state indices\n",
    "        - 'N': Fundamental matrix (expected time in each transient state)\n",
    "        - 'B': Absorption probability matrix\n",
    "        - 'expected_steps': Expected steps to absorption from each transient state\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    # Find absorbing states if not provided\n",
    "    if absorbing_states is None:\n",
    "        absorbing_states = find_absorbing_states(P, tol)\n",
    "    \n",
    "    absorbing_states = set(absorbing_states)\n",
    "    transient_states = [i for i in range(n) if i not in absorbing_states]\n",
    "    \n",
    "    if len(absorbing_states) == 0:\n",
    "        return {\n",
    "            \"absorbing_states\": set(),\n",
    "            \"transient_states\": list(range(n)),\n",
    "            \"N\": None,\n",
    "            \"B\": None,\n",
    "            \"expected_steps\": None,\n",
    "            \"message\": \"No absorbing states found\"\n",
    "        }\n",
    "    \n",
    "    if len(transient_states) == 0:\n",
    "        return {\n",
    "            \"absorbing_states\": absorbing_states,\n",
    "            \"transient_states\": [],\n",
    "            \"N\": None,\n",
    "            \"B\": None,\n",
    "            \"expected_steps\": None,\n",
    "            \"message\": \"All states are absorbing\"\n",
    "        }\n",
    "    \n",
    "    # Reorder states: transient first, then absorbing\n",
    "    absorbing_list = sorted(absorbing_states)\n",
    "    state_order = transient_states + absorbing_list\n",
    "    \n",
    "    # Reorder transition matrix\n",
    "    P_reordered = P[np.ix_(state_order, state_order)]\n",
    "    \n",
    "    # Extract Q (transient to transient) and R (transient to absorbing)\n",
    "    n_transient = len(transient_states)\n",
    "    Q = P_reordered[:n_transient, :n_transient]\n",
    "    R = P_reordered[:n_transient, n_transient:]\n",
    "    \n",
    "    # Compute fundamental matrix N = (I - Q)^(-1)\n",
    "    I = np.eye(n_transient)\n",
    "    try:\n",
    "        N = np.linalg.inv(I - Q)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return {\n",
    "            \"absorbing_states\": absorbing_states,\n",
    "            \"transient_states\": transient_states,\n",
    "            \"N\": None,\n",
    "            \"B\": None,\n",
    "            \"expected_steps\": None,\n",
    "            \"message\": \"Cannot compute fundamental matrix (I-Q is singular)\"\n",
    "        }\n",
    "    \n",
    "    # Compute absorption probabilities B = NR\n",
    "    B = N @ R\n",
    "    \n",
    "    # Expected number of steps to absorption\n",
    "    expected_steps = N @ np.ones(n_transient)\n",
    "    \n",
    "    return {\n",
    "        \"absorbing_states\": absorbing_states,\n",
    "        \"transient_states\": transient_states,\n",
    "        \"N\": N,  # Fundamental matrix\n",
    "        \"B\": B,  # Absorption probabilities\n",
    "        \"expected_steps\": expected_steps,\n",
    "        \"Q\": Q,\n",
    "        \"R\": R\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1cfb4089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing states: {0, 4}\n",
      "Transient states: [1, 2, 3]\n",
      "Expected steps to absorption from each transient state: [3.30769231 3.84615385 2.53846154]\n",
      "Absorption probabilities (to each absorbing state):\n",
      "[[0.58461538 0.41538462]\n",
      " [0.30769231 0.69230769]\n",
      " [0.12307692 0.87692308]]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for analyze_absorbing_chain\n",
    "# Gambler's ruin: States 0 and 4 are absorbing (broke and rich)\n",
    "P_gambler = np.array([[1.0, 0.0, 0.0, 0.0, 0.0],  # State 0: broke (absorbing)\n",
    "                      [0.4, 0.0, 0.6, 0.0, 0.0],  # State 1: $1\n",
    "                      [0.0, 0.4, 0.0, 0.6, 0.0],  # State 2: $2\n",
    "                      [0.0, 0.0, 0.4, 0.0, 0.6],  # State 3: $3\n",
    "                      [0.0, 0.0, 0.0, 0.0, 1.0]]) # State 4: rich (absorbing)\n",
    "\n",
    "result = analyze_absorbing_chain(P_gambler)\n",
    "print(f\"Absorbing states: {result['absorbing_states']}\")\n",
    "print(f\"Transient states: {result['transient_states']}\")\n",
    "if result['expected_steps'] is not None:\n",
    "    print(f\"Expected steps to absorption from each transient state: {result['expected_steps']}\")\n",
    "    print(f\"Absorption probabilities (to each absorbing state):\\n{result['B']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeaf3c7",
   "metadata": {},
   "source": [
    "### 4.11 Periodicity Check\n",
    "\n",
    "**What is this?**\n",
    "A chain is **aperiodic** if states don't have cyclical return patterns. Periodicity matters for convergence.\n",
    "\n",
    "**Period of a state:** $d = \\gcd\\{n : P^n_{ii} > 0\\}$\n",
    "- Period 1 = Aperiodic (can return at any time)\n",
    "- Period > 1 = Periodic (returns only at multiples of d)\n",
    "\n",
    "**Why it matters:**\n",
    "- Aperiodic + Irreducible → Convergence to unique stationary distribution\n",
    "- Periodic chains oscillate (don't converge to stationary distribution)\n",
    "\n",
    "**Making chain aperiodic:**\n",
    "Add self-loops (e.g., set $P_{ii} > 0$ for some state)\n",
    "\n",
    "**Example:**\n",
    "- [0→1→0]: Period 2 (alternates)\n",
    "- [0→0, 0→1, 1→0]: Period 1 (aperiodic, has self-loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f25a2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def gcd_list(numbers):\n",
    "    \"\"\"Compute GCD of a list of numbers.\"\"\"\n",
    "    result = numbers[0]\n",
    "    for num in numbers[1:]:\n",
    "        result = math.gcd(result, num)\n",
    "    return result\n",
    "\n",
    "def state_period(P, state):\n",
    "    \"\"\"\n",
    "    Compute the period of a specific state in a Markov chain.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    state : int\n",
    "        State to check period for\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    period : int\n",
    "        Period of the state (1 = aperiodic)\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    max_steps = 100  # Check up to this many steps\n",
    "    \n",
    "    # Find all n where P^n[state, state] > 0\n",
    "    return_times = []\n",
    "    P_power = P.copy()\n",
    "    \n",
    "    for step in range(1, max_steps + 1):\n",
    "        if P_power[state, state] > 1e-10:\n",
    "            return_times.append(step)\n",
    "        P_power = P_power @ P\n",
    "    \n",
    "    if len(return_times) == 0:\n",
    "        return float('inf')  # Never returns\n",
    "    \n",
    "    return gcd_list(return_times)\n",
    "\n",
    "def is_aperiodic(P):\n",
    "    \"\"\"Check if chain is aperiodic (all states have period 1).\"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    for state in range(n):\n",
    "        if state_period(P, state) > 1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def all_state_periods(P):\n",
    "    \"\"\"\n",
    "    Compute the period for each state in a Markov chain.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    periods : array\n",
    "        Array where periods[i] is the period of state i\n",
    "        (period = 1 means aperiodic, period = inf means never returns)\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    periods = np.zeros(n)\n",
    "    for state in range(n):\n",
    "        periods[state] = state_period(P, state)\n",
    "    \n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dc138056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period of state 0 in periodic chain: 2\n",
      "Second chain is aperiodic: True\n",
      "All state periods in periodic chain: [2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for periodicity functions\n",
    "P_periodic = np.array([[0, 1], [1, 0]])  # Alternating (period 2)\n",
    "P_aperiodic = np.array([[0.5, 0.5], [0.5, 0.5]])  # Has self-loops (period 1)\n",
    "\n",
    "period_0 = state_period(P_periodic, 0)\n",
    "print(f\"Period of state 0 in periodic chain: {period_0}\")\n",
    "\n",
    "is_aper = is_aperiodic(P_aperiodic)\n",
    "print(f\"Second chain is aperiodic: {is_aper}\")\n",
    "\n",
    "all_periods = all_state_periods(P_periodic)\n",
    "print(f\"All state periods in periodic chain: {all_periods}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489dd71",
   "metadata": {},
   "source": [
    "### 4.12 Stationary Distribution\n",
    "\n",
    "**What is this?**\n",
    "The stationary (or invariant) distribution is a probability distribution over states that remains unchanged as the Markov chain evolves. Once the chain reaches this distribution, it stays in it forever. This represents the equilibrium behavior of the system.\n",
    "\n",
    "**Mathematical Definition:** \n",
    "\n",
    "A probability distribution $\\pi = [\\pi_1, \\pi_2, \\ldots, \\pi_n]$ is stationary if:\n",
    "\n",
    "$$\\pi^T P = \\pi^T$$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$$\\sum_{j=1}^{n} \\pi_j P_{ji} = \\pi_i \\quad \\text{for all } i$$\n",
    "\n",
    "where:\n",
    "- $\\pi$ is a row vector of probabilities\n",
    "- $P$ is the transition matrix\n",
    "- $\\pi_i \\geq 0$ for all $i$ and $\\sum_{i=1}^{n} \\pi_i = 1$\n",
    "\n",
    "**Key Properties:**\n",
    "\n",
    "1. **Eigenvector Interpretation:** $\\pi$ is a left eigenvector of $P$ with eigenvalue 1\n",
    "   - Equivalently, $\\pi$ is a right eigenvector of $P^T$ with eigenvalue 1\n",
    "   \n",
    "2. **Uniqueness:** For irreducible, aperiodic chains, the stationary distribution is unique\n",
    "\n",
    "3. **Convergence:** $\\lim_{n \\to \\infty} P^n = \\begin{bmatrix} \\pi^T \\\\ \\pi^T \\\\ \\vdots \\\\ \\pi^T \\end{bmatrix}$ (each row converges to $\\pi^T$)\n",
    "\n",
    "**How to Find It:**\n",
    "\n",
    "**Method 1: Eigenvalue Approach**\n",
    "1. Find eigenvector $v$ of $P^T$ corresponding to eigenvalue $\\lambda = 1$\n",
    "2. Normalize: $\\pi = \\frac{v}{\\sum_i v_i}$ so that $\\sum_i \\pi_i = 1$\n",
    "\n",
    "**Method 2: System of Linear Equations**\n",
    "1. Solve $\\pi^T P = \\pi^T$ which gives $(n-1)$ independent equations\n",
    "2. Add constraint $\\sum_{i=1}^{n} \\pi_i = 1$\n",
    "3. Solve the resulting system\n",
    "\n",
    "**Method 3: Power Method**\n",
    "- Compute $P^n$ for large $n$; any row gives $\\pi^T$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\pi_i$ = Long-run proportion of time the chain spends in state $i$\n",
    "- $\\pi_i$ = Limiting probability of being in state $i$ after many steps\n",
    "- After running the chain long enough, state probabilities converge to $\\pi$ regardless of initial state\n",
    "- For irreducible, aperiodic chains, this convergence is guaranteed\n",
    "\n",
    "**Detailed Example:** \n",
    "\n",
    "Weather Model with states $\\{$Sunny, Rainy$\\}$ and transition matrix:\n",
    "\n",
    "$$P = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.4 & 0.6 \\end{bmatrix}$$\n",
    "\n",
    "To find $\\pi = [\\pi_S, \\pi_R]$, solve:\n",
    "- $0.8\\pi_S + 0.4\\pi_R = \\pi_S$ → $0.2\\pi_S = 0.4\\pi_R$ → $\\pi_S = 2\\pi_R$\n",
    "- $\\pi_S + \\pi_R = 1$\n",
    "\n",
    "Solution: $\\pi = [0.667, 0.333]$ or $[\\frac{2}{3}, \\frac{1}{3}]$\n",
    "\n",
    "**Interpretation:** In the long run, expect 66.7% sunny days and 33.3% rainy days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "95a7a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "\n",
    "\n",
    "def has_stationary_distribution(P):\n",
    "    \"\"\"\n",
    "    Check if a Markov chain has a unique stationary distribution.\n",
    "    \n",
    "    A finite Markov chain has a unique stationary distribution if and only if\n",
    "    it is irreducible and aperiodic (i.e., ergodic).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    has_stationary : bool\n",
    "        True if chain has a unique stationary distribution\n",
    "    reason : str\n",
    "        Explanation of why the chain does or doesn't have stationary distribution\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    \n",
    "    # Check if it's a valid transition matrix\n",
    "    if not is_transition_matrix(P):\n",
    "        return False, \"Not a valid transition matrix\"\n",
    "    \n",
    "    # Check irreducibility\n",
    "    if not is_irreducible(P):\n",
    "        return False, \"Chain is not irreducible (some states are not reachable from others)\"\n",
    "    \n",
    "    # Check aperiodicity\n",
    "    if not is_aperiodic(P):\n",
    "        return False, \"Chain is not aperiodic (has periodic states)\"\n",
    "    \n",
    "    return True, \"Chain is ergodic (irreducible and aperiodic) - has unique stationary distribution\"\n",
    "\n",
    "def stationary_distribution(P):\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    w, v = eig(P.T)\n",
    "    k = np.argmin(np.abs(w - 1))\n",
    "    pi = np.real(v[:, k])\n",
    "    pi = np.abs(pi)  # Take absolute value instead of maximum with 0\n",
    "    pi = pi / pi.sum()\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a6b69e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has stationary distribution: True\n",
      "Reason: Chain is ergodic (irreducible and aperiodic) - has unique stationary distribution\n",
      "Stationary distribution: [0.57142857 0.42857143]\n",
      "π after one step: [0.57142857 0.42857143]\n",
      "Distribution unchanged: True\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for stationary distribution\n",
    "P_stat = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "\n",
    "# Check if has stationary distribution\n",
    "has_stat, reason = has_stationary_distribution(P_stat)\n",
    "print(f\"Has stationary distribution: {has_stat}\")\n",
    "print(f\"Reason: {reason}\")\n",
    "\n",
    "# Compute stationary distribution\n",
    "pi = stationary_distribution(P_stat)\n",
    "print(f\"Stationary distribution: {pi}\")\n",
    "\n",
    "# Verify it's stationary: π^T P = π^T\n",
    "pi_after = pi @ P_stat\n",
    "print(f\"π after one step: {pi_after}\")\n",
    "print(f\"Distribution unchanged: {np.allclose(pi, pi_after)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2650db",
   "metadata": {},
   "source": [
    "### Reversibility and Detailed Balance\n",
    "\n",
    "**What is this?**\n",
    "A Markov chain is **reversible** if the forward and backward processes are statistically identical. This is formalized through the **detailed balance condition**.\n",
    "\n",
    "**Detailed Balance Condition:**\n",
    "A chain with transition matrix $P$ and stationary distribution $\\pi$ satisfies detailed balance if:\n",
    "$$\\pi_i P_{ij} = \\pi_j P_{ji} \\quad \\text{for all states } i, j$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\pi_i P_{ij}$ = Long-run rate of transitions from $i$ to $j$\n",
    "- $\\pi_j P_{ji}$ = Long-run rate of transitions from $j$ to $i$\n",
    "- Detailed balance means these rates are equal (equilibrium at individual transition level)\n",
    "\n",
    "**Why it matters:**\n",
    "- **Checking stationary distribution**: If detailed balance holds, $\\pi$ is stationary\n",
    "- **MCMC algorithms**: Many samplers (Metropolis-Hastings) rely on reversibility\n",
    "- **Symmetric chains**: If $P = P^T$ (symmetric), the chain is reversible with $\\pi = \\text{uniform}$\n",
    "\n",
    "**Key Properties:**\n",
    "- Reversible + Irreducible → Unique stationary distribution\n",
    "- Not all chains are reversible (e.g., deterministic cycles)\n",
    "- Reversibility is sufficient but not necessary for stationarity\n",
    "\n",
    "**Example:**\n",
    "Random walk on undirected graph is reversible; directed cycle is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36c7b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_detailed_balance(P, pi, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Check if a Markov chain satisfies detailed balance condition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    pi : array-like\n",
    "        Stationary distribution (or candidate stationary distribution)\n",
    "    tol : float, default=1e-8\n",
    "        Numerical tolerance for equality check\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    is_reversible : bool\n",
    "        True if detailed balance holds\n",
    "    max_violation : float\n",
    "        Maximum absolute difference |pi_i * P_ij - pi_j * P_ji|\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    pi = np.asarray(pi, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    # Check detailed balance: pi[i] * P[i,j] = pi[j] * P[j,i] for all i,j\n",
    "    max_violation = 0.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            forward_rate = pi[i] * P[i, j]\n",
    "            backward_rate = pi[j] * P[j, i]\n",
    "            violation = abs(forward_rate - backward_rate)\n",
    "            max_violation = max(max_violation, violation)\n",
    "    \n",
    "    is_reversible = (max_violation < tol)\n",
    "    \n",
    "    return is_reversible, float(max_violation)\n",
    "\n",
    "def is_reversible_chain(P, pi=None, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Check if a Markov chain is reversible.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    pi : array-like, optional\n",
    "        Stationary distribution. If None, computes it automatically\n",
    "    tol : float, default=1e-8\n",
    "        Numerical tolerance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    reversible : bool\n",
    "        True if chain is reversible\n",
    "    message : str\n",
    "        Explanation\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    \n",
    "    # Check if it's a valid transition matrix\n",
    "    if not is_transition_matrix(P):\n",
    "        return False, \"Not a valid transition matrix\"\n",
    "    \n",
    "    # Get stationary distribution if not provided\n",
    "    if pi is None:\n",
    "        # Check if chain has unique stationary distribution\n",
    "        has_stat, reason = has_stationary_distribution(P)\n",
    "        if not has_stat:\n",
    "            return False, f\"Cannot check reversibility: {reason}\"\n",
    "        pi = stationary_distribution(P)\n",
    "    \n",
    "    # Check detailed balance\n",
    "    is_rev, max_viol = check_detailed_balance(P, pi, tol)\n",
    "    \n",
    "    if is_rev:\n",
    "        return True, f\"Chain is reversible (max violation: {max_viol:.2e})\"\n",
    "    else:\n",
    "        return False, f\"Chain is not reversible (max violation: {max_viol:.2e})\"\n",
    "\n",
    "def check_symmetry(P, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Check if transition matrix is symmetric (P = P^T).\n",
    "    Symmetric chains are always reversible with uniform stationary distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    tol : float, default=1e-8\n",
    "        Numerical tolerance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    is_symmetric : bool\n",
    "        True if P = P^T\n",
    "    max_diff : float\n",
    "        Maximum absolute difference |P_ij - P_ji|\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    diff = P - P.T\n",
    "    max_diff = float(np.max(np.abs(diff)))\n",
    "    is_symmetric = (max_diff < tol)\n",
    "    \n",
    "    return is_symmetric, max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "03f5e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed balance holds: True\n",
      "Max violation: 0.000000\n",
      "Chain is reversible: True\n",
      "Message: Chain is reversible (max violation: 2.78e-17)\n",
      "Matrix is symmetric: True\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for reversibility and detailed balance\n",
    "P_rev = np.array([[0.7, 0.3], [0.3, 0.7]])  # Symmetric (reversible)\n",
    "pi_rev = stationary_distribution(P_rev)\n",
    "\n",
    "# Check detailed balance\n",
    "is_rev, max_viol = check_detailed_balance(P_rev, pi_rev)\n",
    "print(f\"Detailed balance holds: {is_rev}\")\n",
    "print(f\"Max violation: {max_viol:.6f}\")\n",
    "\n",
    "# Check if reversible\n",
    "reversible, msg = is_reversible_chain(P_rev)\n",
    "print(f\"Chain is reversible: {reversible}\")\n",
    "print(f\"Message: {msg}\")\n",
    "\n",
    "# Check symmetry\n",
    "is_sym, max_diff = check_symmetry(P_rev)\n",
    "print(f\"Matrix is symmetric: {is_sym}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df21e4",
   "metadata": {},
   "source": [
    "### Mean Return Time\n",
    "\n",
    "**What is this?**\n",
    "The expected number of steps to return to a state, starting from that state.\n",
    "\n",
    "**Mathematical definition:** \n",
    "For state $i$: $m_i = E[\\text{time to return to } i | X_0 = i]$\n",
    "\n",
    "**Relationship to Stationary Distribution:**\n",
    "For an irreducible, aperiodic chain:\n",
    "$$\\pi_i = \\frac{1}{m_i}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $m_i = 10$ means on average, the chain returns to state $i$ every 10 steps\n",
    "- $\\pi_i = 0.1$ means state $i$ is visited 10% of the time\n",
    "- States with higher stationary probability have shorter return times\n",
    "\n",
    "**Why it matters:**\n",
    "- Understanding how \"frequently\" states are visited\n",
    "- Quality control: How often does the system return to a desired state?\n",
    "- Queueing: Average time between customer arrivals\n",
    "\n",
    "**Example:**\n",
    "Weather with $\\pi_{\\text{sunny}} = 0.7$:\n",
    "- Mean return time to sunny = $1/0.7 \\approx 1.43$ days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "918a410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_return_times(P, pi=None):\n",
    "    \"\"\"\n",
    "    Compute mean return time for each state.\n",
    "    \n",
    "    For an ergodic chain, mean return time m_i = 1 / pi_i.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    pi : array-like, optional\n",
    "        Stationary distribution. If None, computes it automatically.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    return_times : array\n",
    "        Mean return time for each state\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    \n",
    "    # Get stationary distribution\n",
    "    if pi is None:\n",
    "        has_stat, reason = has_stationary_distribution(P)\n",
    "        if not has_stat:\n",
    "            raise ValueError(f\"Cannot compute return times: {reason}\")\n",
    "        pi = stationary_distribution(P)\n",
    "    \n",
    "    pi = np.asarray(pi, float)\n",
    "    \n",
    "    # Mean return time = 1 / pi_i (for ergodic chains)\n",
    "    return_times = np.zeros(len(pi))\n",
    "    for i in range(len(pi)):\n",
    "        if pi[i] > 1e-10:\n",
    "            return_times[i] = 1.0 / pi[i]\n",
    "        else:\n",
    "            return_times[i] = np.inf  # State not visited in stationary distribution\n",
    "    \n",
    "    return return_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "71d2c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationary distribution: [0.6 0.4]\n",
      "Mean return times: [1.66666667 2.5       ]\n",
      "Verification: 1/π = [1.66666667 2.5       ]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for mean_return_times\n",
    "P_return = np.array([[0.8, 0.2], [0.3, 0.7]])\n",
    "pi_return = stationary_distribution(P_return)\n",
    "return_times = mean_return_times(P_return, pi_return)\n",
    "print(f\"Stationary distribution: {pi_return}\")\n",
    "print(f\"Mean return times: {return_times}\")\n",
    "print(f\"Verification: 1/π = {1.0/pi_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46eb97",
   "metadata": {},
   "source": [
    "### Reducing and Reversing Markov Chains\n",
    "\n",
    "**What is this?**\n",
    "Techniques for simplifying Markov chains or constructing the time-reversed process.\n",
    "\n",
    "**1. Canonical Form (Reducing to Standard Form)**\n",
    "\n",
    "For chains with both transient and recurrent states, reorder the transition matrix into canonical form:\n",
    "$$P = \\begin{pmatrix} Q & R \\\\ 0 & S \\end{pmatrix}$$\n",
    "\n",
    "Where:\n",
    "- $Q$: Transitions among transient states\n",
    "- $R$: Transitions from transient to recurrent\n",
    "- $S$: Transitions among recurrent states\n",
    "- $0$: Recurrent states can't return to transient\n",
    "\n",
    "**2. Lumping States (State Aggregation)**\n",
    "\n",
    "Combine states into groups if they behave similarly:\n",
    "- **Lumpable**: If for all states in a group, transition probabilities to other groups are identical\n",
    "- Reduces state space complexity\n",
    "- Preserves Markov property if done correctly\n",
    "\n",
    "**3. Time-Reversed Chain**\n",
    "\n",
    "The **reversed chain** has transition matrix $\\tilde{P}$ where:\n",
    "$$\\tilde{P}_{ij} = \\frac{\\pi_j P_{ji}}{\\pi_i}$$\n",
    "\n",
    "**Properties:**\n",
    "- If original chain has stationary distribution $\\pi$, reversed chain has the same $\\pi$\n",
    "- Chain is reversible ⟺ Original and reversed chains are identical\n",
    "- Used in MCMC diagnostics and theoretical analysis\n",
    "\n",
    "**When to use:**\n",
    "- **Canonical form**: Analyzing chains with absorbing states\n",
    "- **Lumping**: Simplifying large state spaces\n",
    "- **Reversing**: Testing reversibility, understanding detailed balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "251ca3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonical_form(P, transient_states=None):\n",
    "    \"\"\"\n",
    "    Convert transition matrix to canonical form.\n",
    "    \n",
    "    Reorders states so transient states come first, then recurrent states.\n",
    "    Results in block matrix: [[Q, R], [0, S]]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    transient_states : list, optional\n",
    "        List of transient state indices. If None, automatically detected.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    P_canonical : array\n",
    "        Reordered transition matrix in canonical form\n",
    "    state_order : array\n",
    "        New ordering of states\n",
    "    partition : dict\n",
    "        Dictionary with 'transient' and 'recurrent' state indices in new order\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    # Detect transient and recurrent states if not provided\n",
    "    if transient_states is None:\n",
    "        classification = classify_states_transient_recurrent(P)\n",
    "        transient_states = list(classification['transient'])\n",
    "    \n",
    "    transient_states = list(transient_states)\n",
    "    recurrent_states = [i for i in range(n) if i not in transient_states]\n",
    "    \n",
    "    # Create new state ordering: transient first, then recurrent\n",
    "    state_order = transient_states + recurrent_states\n",
    "    \n",
    "    # Reorder transition matrix\n",
    "    P_canonical = P[np.ix_(state_order, state_order)]\n",
    "    \n",
    "    n_transient = len(transient_states)\n",
    "    \n",
    "    return P_canonical, np.array(state_order), {\n",
    "        'transient': list(range(n_transient)),\n",
    "        'recurrent': list(range(n_transient, n)),\n",
    "        'n_transient': n_transient,\n",
    "        'n_recurrent': len(recurrent_states)\n",
    "    }\n",
    "\n",
    "def reverse_chain(P, pi=None):\n",
    "    \"\"\"\n",
    "    Compute the time-reversed Markov chain.\n",
    "    \n",
    "    The reversed chain has transition matrix P_tilde where:\n",
    "    P_tilde[i,j] = (pi[j] * P[j,i]) / pi[i]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    pi : array-like, optional\n",
    "        Stationary distribution. If None, computes it automatically.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    P_reversed : array\n",
    "        Transition matrix of reversed chain\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    # Get stationary distribution\n",
    "    if pi is None:\n",
    "        has_stat, reason = has_stationary_distribution(P)\n",
    "        if not has_stat:\n",
    "            raise ValueError(f\"Cannot compute reversed chain: {reason}\")\n",
    "        pi = stationary_distribution(P)\n",
    "    \n",
    "    pi = np.asarray(pi, float)\n",
    "    \n",
    "    # Compute reversed transition matrix\n",
    "    P_reversed = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if pi[i] > 1e-10:\n",
    "                P_reversed[i, j] = (pi[j] * P[j, i]) / pi[i]\n",
    "            else:\n",
    "                P_reversed[i, j] = 0.0\n",
    "    \n",
    "    return P_reversed\n",
    "\n",
    "def test_lumpability(P, partition, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Test if a partition of states is lumpable.\n",
    "    \n",
    "    A partition is lumpable if for each group A in the partition,\n",
    "    all states in A have the same transition probability to each other group B.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    partition : list of lists\n",
    "        Each element is a list of states that form a group\n",
    "    tol : float, default=1e-8\n",
    "        Numerical tolerance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    is_lumpable : bool\n",
    "        True if partition is lumpable\n",
    "    lumped_P : array or None\n",
    "        Transition matrix on lumped state space (if lumpable)\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    k = len(partition)  # Number of groups\n",
    "    \n",
    "    # Check lumpability condition\n",
    "    lumped_P = np.zeros((k, k))\n",
    "    \n",
    "    for a, group_a in enumerate(partition):\n",
    "        for b, group_b in enumerate(partition):\n",
    "            # Compute transition probability from group_a to group_b\n",
    "            # Should be same for all states in group_a\n",
    "            probs = []\n",
    "            for i in group_a:\n",
    "                prob_i_to_b = sum(P[i, j] for j in group_b)\n",
    "                probs.append(prob_i_to_b)\n",
    "            \n",
    "            # Check if all states in group_a have same transition prob to group_b\n",
    "            if len(probs) > 0:\n",
    "                avg_prob = np.mean(probs)\n",
    "                if not all(abs(p - avg_prob) < tol for p in probs):\n",
    "                    return False, None\n",
    "                lumped_P[a, b] = avg_prob\n",
    "    \n",
    "    return True, lumped_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4fb4b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonical form:\n",
      "[[0.5 0.5 0. ]\n",
      " [0.  0.6 0.4]\n",
      " [0.  0.  1. ]]\n",
      "State ordering: [0 1 2]\n",
      "Partition: {'transient': [0, 1], 'recurrent': [2], 'n_transient': 2, 'n_recurrent': 1}\n",
      "Original P:\n",
      "[[0.7 0.3]\n",
      " [0.4 0.6]]\n",
      "Reversed P:\n",
      "[[0.7 0.3]\n",
      " [0.4 0.6]]\n",
      "Partition is lumpable: True\n",
      "Lumped transition matrix:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for chain transformations\n",
    "# Canonical form\n",
    "P_canonical_test = np.array([[0.5, 0.5, 0],\n",
    "                             [0, 0.6, 0.4],\n",
    "                             [0, 0, 1.0]])\n",
    "P_can, order, partition = canonical_form(P_canonical_test, transient_states=[0, 1])\n",
    "print(f\"Canonical form:\\n{P_can}\")\n",
    "print(f\"State ordering: {order}\")\n",
    "print(f\"Partition: {partition}\")\n",
    "\n",
    "# Reverse chain\n",
    "P_to_reverse = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "P_reversed = reverse_chain(P_to_reverse)\n",
    "print(f\"Original P:\\n{P_to_reverse}\")\n",
    "print(f\"Reversed P:\\n{P_reversed}\")\n",
    "\n",
    "# Test lumpability\n",
    "P_lump = np.array([[0.5, 0.5, 0, 0],\n",
    "                   [0.5, 0.5, 0, 0],\n",
    "                   [0, 0, 0.6, 0.4],\n",
    "                   [0, 0, 0.3, 0.7]])\n",
    "partition_test = [[0, 1], [2, 3]]\n",
    "is_lump, lumped = test_lumpability(P_lump, partition_test)\n",
    "print(f\"Partition is lumpable: {is_lump}\")\n",
    "if is_lump:\n",
    "    print(f\"Lumped transition matrix:\\n{lumped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556dcf1",
   "metadata": {},
   "source": [
    "### Expected Time to Hit a Target Set\n",
    "\n",
    "**What is this?**\n",
    "Calculates the expected number of steps to reach a target state (or set of states) from each starting state.\n",
    "\n",
    "**Mathematical formulation:**\n",
    "For state $i$ not in target: $h_i = 1 + \\sum_j P_{ij} h_j$\n",
    "For state $i$ in target: $h_i = 0$\n",
    "\n",
    "**How it works:**\n",
    "Solves a system of linear equations to find expected hitting times.\n",
    "\n",
    "**Interpretation:**\n",
    "- $h_i$ = Average number of steps to reach target starting from state $i$\n",
    "- If target is never reachable from $i$, the value will be infinite\n",
    "\n",
    "**Example use case:**\n",
    "- Customer journey: Expected time until purchase\n",
    "- Game: Expected moves until winning/losing\n",
    "- Network: Expected hops to reach destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8a108c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_hitting_times(P, target_states):\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    target = set(target_states)\n",
    "\n",
    "    A = np.zeros((n, n), float)\n",
    "    b = np.zeros(n, float)\n",
    "\n",
    "    for i in range(n):\n",
    "        if i in target:\n",
    "            A[i, i] = 1.0\n",
    "            b[i] = 0.0\n",
    "        else:\n",
    "            A[i, i] = 1.0\n",
    "            A[i, :] -= P[i, :]\n",
    "            b[i] = 1.0\n",
    "\n",
    "    return np.linalg.solve(A, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a2e5137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected time to reach state 2 from each state: [7.77777778 6.66666667 0.        ]\n",
      "From state 0: 7.7778 steps\n",
      "From state 1: 6.6667 steps\n"
     ]
    }
   ],
   "source": [
    "# Sample call for expected_hitting_times (already has one after, but adding more complete example)\n",
    "P_hit = np.array([[0.7, 0.2, 0.1],\n",
    "                  [0.3, 0.5, 0.2],\n",
    "                  [0.1, 0.3, 0.6]])\n",
    "target_set = [2]\n",
    "hitting_times = expected_hitting_times(P_hit, target_set)\n",
    "print(f\"Expected time to reach state 2 from each state: {hitting_times}\")\n",
    "print(f\"From state 0: {hitting_times[0]:.4f} steps\")\n",
    "print(f\"From state 1: {hitting_times[1]:.4f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "92fd4af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25., 20.,  0.,  0.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_hitting_times(Mat_A, target_states=[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcefce",
   "metadata": {},
   "source": [
    "### Convergence By Powering Matrix (K Steps)\n",
    "\n",
    "**What is this?**\n",
    "Computes the k-step transition probabilities by raising the transition matrix to the k-th power.\n",
    "\n",
    "**Formula:** $P^{(k)} = P^k$\n",
    "\n",
    "**Interpretation:**\n",
    "- $P^k_{ij}$ = Probability of being in state $j$ after exactly $k$ steps starting from state $i$\n",
    "- As $k \\to \\infty$, rows converge to the stationary distribution (for irreducible, aperiodic chains)\n",
    "\n",
    "**Use cases:**\n",
    "- Predict state distribution after k time steps\n",
    "- Check convergence to stationary distribution\n",
    "- Analyze mixing time\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "P2 = n_step_transition(P, 2)  # 2-step transitions\n",
    "P100 = n_step_transition(P, 100)  # Should be close to stationary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6ce229ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_transition(P, k):\n",
    "    return np.linalg.matrix_power(np.asarray(P, float), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2aceaac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-step transition matrix:\n",
      "[[0.60039063 0.39960938]\n",
      " [0.59941406 0.40058594]]\n",
      "Compare to stationary: [0.6 0.4]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for n_step_transition\n",
    "P_nst = np.array([[0.8, 0.2], [0.3, 0.7]])\n",
    "P_10 = n_step_transition(P_nst, k=10)\n",
    "print(f\"10-step transition matrix:\\n{P_10}\")\n",
    "print(f\"Compare to stationary: {stationary_distribution(P_nst)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90099cbe",
   "metadata": {},
   "source": [
    "### Mixing Time\n",
    "\n",
    "**What is this?**\n",
    "The time it takes for a Markov chain to \"forget\" its initial state and get close to the stationary distribution.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "The mixing time $\\tau(\\epsilon)$ is the smallest time $t$ such that:\n",
    "$$\\max_i \\|P^t(i, \\cdot) - \\pi\\|_{TV} \\leq \\epsilon$$\n",
    "\n",
    "Where $\\|\\cdot\\|_{TV}$ is the total variation distance.\n",
    "\n",
    "**Total Variation Distance:**\n",
    "$$\\|p - q\\|_{TV} = \\frac{1}{2}\\sum_j |p_j - q_j|$$\n",
    "\n",
    "**Interpretation:**\n",
    "- After $\\tau(\\epsilon)$ steps, distribution is within $\\epsilon$ of stationary\n",
    "- Common choice: $\\epsilon = 0.25$ (within 25% of stationary)\n",
    "- Smaller mixing time = Faster convergence\n",
    "\n",
    "**Why it matters:**\n",
    "- **MCMC**: How many samples to discard as \"burn-in\"\n",
    "- **Randomized algorithms**: How long until output is \"random enough\"\n",
    "- **Performance**: Fast mixing = Efficient sampling\n",
    "\n",
    "**Factors affecting mixing time:**\n",
    "- Chain connectivity (better connected = faster mixing)\n",
    "- Second largest eigenvalue (closer to 1 = slower mixing)\n",
    "- Bottlenecks in state space\n",
    "\n",
    "**Example:**\n",
    "Well-connected graph: $\\tau \\approx \\log n$\n",
    "Line graph: $\\tau \\approx n^2$ (very slow!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4a5f726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_distance(p, q):\n",
    "    \"\"\"\n",
    "    Compute total variation distance between two probability distributions.\n",
    "    \n",
    "    TV(p, q) = 0.5 * sum_i |p_i - q_i|\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    p, q : array-like\n",
    "        Probability distributions (should sum to 1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tv_dist : float\n",
    "        Total variation distance (between 0 and 1)\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, float)\n",
    "    q = np.asarray(q, float)\n",
    "    \n",
    "    return 0.5 * np.sum(np.abs(p - q))\n",
    "\n",
    "def estimate_mixing_time(P, epsilon=0.25, max_steps=10000, pi=None):\n",
    "    \"\"\"\n",
    "    Estimate the mixing time of a Markov chain.\n",
    "    \n",
    "    Mixing time is the smallest t such that ||P^t(i,·) - π||_TV ≤ ε for all i.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    epsilon : float, default=0.25\n",
    "        Tolerance for total variation distance\n",
    "    max_steps : int, default=10000\n",
    "        Maximum number of steps to check\n",
    "    pi : array-like, optional\n",
    "        Stationary distribution. If None, computes it automatically.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mixing_time : int\n",
    "        Estimated mixing time (number of steps)\n",
    "    convergence_info : dict\n",
    "        Information about convergence from each starting state\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n = P.shape[0]\n",
    "    \n",
    "    # Get stationary distribution\n",
    "    if pi is None:\n",
    "        has_stat, reason = has_stationary_distribution(P)\n",
    "        if not has_stat:\n",
    "            return None, {\"error\": f\"Cannot compute mixing time: {reason}\"}\n",
    "        pi = stationary_distribution(P)\n",
    "    \n",
    "    pi = np.asarray(pi, float)\n",
    "    \n",
    "    # For each starting state, find when it gets close to stationary\n",
    "    convergence_times = np.zeros(n, dtype=int)\n",
    "    \n",
    "    for start_state in range(n):\n",
    "        # Initial distribution (start from state i)\n",
    "        current_dist = np.zeros(n)\n",
    "        current_dist[start_state] = 1.0\n",
    "        \n",
    "        P_power = np.eye(n)\n",
    "        \n",
    "        for t in range(1, max_steps + 1):\n",
    "            P_power = P_power @ P\n",
    "            current_dist = P_power[start_state, :]\n",
    "            \n",
    "            tv_dist = total_variation_distance(current_dist, pi)\n",
    "            \n",
    "            if tv_dist <= epsilon:\n",
    "                convergence_times[start_state] = t\n",
    "                break\n",
    "        else:\n",
    "            # Didn't converge within max_steps\n",
    "            convergence_times[start_state] = max_steps\n",
    "    \n",
    "    mixing_time = int(np.max(convergence_times))\n",
    "    \n",
    "    return mixing_time, {\n",
    "        \"convergence_times\": convergence_times,\n",
    "        \"max_time\": mixing_time,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"all_converged\": (mixing_time < max_steps)\n",
    "    }\n",
    "\n",
    "def spectral_gap_mixing_bound(P, pi=None):\n",
    "    \"\"\"\n",
    "    Estimate mixing time using spectral gap (second largest eigenvalue).\n",
    "    \n",
    "    The spectral gap λ = 1 - |λ_2| where λ_2 is the second largest eigenvalue.\n",
    "    Mixing time is roughly O(1/λ).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    pi : array-like, optional\n",
    "        Stationary distribution\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    spectral_gap : float\n",
    "        Gap between largest and second largest eigenvalue\n",
    "    mixing_estimate : float\n",
    "        Rough estimate of mixing time based on spectral gap\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    \n",
    "    # Compute eigenvalues\n",
    "    eigenvalues = np.linalg.eigvals(P)\n",
    "    eigenvalues_sorted = np.sort(np.abs(eigenvalues))[::-1]\n",
    "    \n",
    "    # Spectral gap = 1 - |second largest eigenvalue|\n",
    "    if len(eigenvalues_sorted) > 1:\n",
    "        second_largest = eigenvalues_sorted[1]\n",
    "        spectral_gap = 1.0 - second_largest\n",
    "    else:\n",
    "        spectral_gap = 1.0\n",
    "    \n",
    "    # Mixing time estimate: O(log(n) / spectral_gap)\n",
    "    n = P.shape[0]\n",
    "    if spectral_gap > 1e-10:\n",
    "        mixing_estimate = np.log(n) / spectral_gap\n",
    "    else:\n",
    "        mixing_estimate = np.inf\n",
    "    \n",
    "    return float(spectral_gap), float(mixing_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a4d229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV distance from deterministic start to stationary: 0.4000\n",
      "Mixing time (ε=0.1): 3 steps\n",
      "Convergence times from each state: [3 3]\n",
      "Spectral gap: 0.5000\n",
      "Mixing time estimate (spectral): 1.39\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for mixing time\n",
    "P_mix = np.array([[0.8, 0.2], [0.3, 0.7]])\n",
    "\n",
    "# Total variation distance\n",
    "pi_mix = stationary_distribution(P_mix)\n",
    "p1 = np.array([1.0, 0.0])  # Start from state 0\n",
    "tv_dist = total_variation_distance(p1, pi_mix)\n",
    "print(f\"TV distance from deterministic start to stationary: {tv_dist:.4f}\")\n",
    "\n",
    "# Estimate mixing time\n",
    "mix_time, mix_info = estimate_mixing_time(P_mix, epsilon=0.1, max_steps=100)\n",
    "print(f\"Mixing time (ε=0.1): {mix_time} steps\")\n",
    "print(f\"Convergence times from each state: {mix_info['convergence_times']}\")\n",
    "\n",
    "# Spectral gap estimate\n",
    "gap, estimate = spectral_gap_mixing_bound(P_mix)\n",
    "print(f\"Spectral gap: {gap:.4f}\")\n",
    "print(f\"Mixing time estimate (spectral): {estimate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30657988",
   "metadata": {},
   "source": [
    "### Theoretical Foundations of Markov Chains\n",
    "\n",
    "This section covers the key mathematical theorems that underpin Markov chain theory.\n",
    "\n",
    "---\n",
    "\n",
    "#### Chapman-Kolmogorov Equation\n",
    "\n",
    "**What is this?**\n",
    "The fundamental equation describing how multi-step transition probabilities compose.\n",
    "\n",
    "**Statement:**\n",
    "$$P^{(n+m)}_{ij} = \\sum_{k} P^{(n)}_{ik} P^{(m)}_{kj}$$\n",
    "\n",
    "In matrix form: $P^{(n+m)} = P^{(n)} \\cdot P^{(m)}$\n",
    "\n",
    "**Interpretation:**\n",
    "- To go from $i$ to $j$ in $n+m$ steps, you must pass through some intermediate state $k$\n",
    "- Probability = sum over all possible intermediate states\n",
    "- This is essentially the Markov property in action\n",
    "\n",
    "**Consequence:**\n",
    "- $P^{(n)} = P^n$ (n-fold matrix multiplication)\n",
    "- Multi-step probabilities can be computed by matrix powers\n",
    "- Foundational for analyzing long-term behavior\n",
    "\n",
    "---\n",
    "\n",
    "#### Limiting Behavior Theorem\n",
    "\n",
    "**What is this?**\n",
    "Conditions under which a Markov chain converges to a stationary distribution.\n",
    "\n",
    "**Statement:**\n",
    "For a finite, irreducible, and aperiodic Markov chain:\n",
    "$$\\lim_{n \\to \\infty} P^n_{ij} = \\pi_j$$\n",
    "\n",
    "Where $\\pi$ is the unique stationary distribution.\n",
    "\n",
    "**What this means:**\n",
    "- No matter where you start (state $i$), after many steps the probability of being in state $j$ approaches $\\pi_j$\n",
    "- The chain \"forgets\" its initial state\n",
    "- All rows of $P^n$ converge to the same vector $\\pi$\n",
    "\n",
    "**Requirements:**\n",
    "1. **Finite**: Finite number of states\n",
    "2. **Irreducible**: Can reach any state from any other state\n",
    "3. **Aperiodic**: No cyclical return patterns\n",
    "\n",
    "**Visual interpretation:**\n",
    "```\n",
    "P^1: Initial transitions\n",
    "P^2: 2-step transitions  \n",
    "...\n",
    "P^100: All rows ≈ [π₀, π₁, π₂, ...]\n",
    "P^∞: Exact stationary distribution\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Ergodic Theorem\n",
    "\n",
    "**What is this?**\n",
    "Connects **time averages** (how often you visit states in one long run) with **space averages** (stationary distribution).\n",
    "\n",
    "**Statement:**\n",
    "For an irreducible, aperiodic Markov chain with stationary distribution $\\pi$:\n",
    "$$\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k=1}^{n} \\mathbb{1}_{X_k = j} = \\pi_j \\quad \\text{almost surely}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Left side: Fraction of time spent in state $j$ over a long run\n",
    "- Right side: Stationary probability of state $j$\n",
    "- They're equal! (with probability 1)\n",
    "\n",
    "**Practical meaning:**\n",
    "- Simulate one long chain → frequency of visits estimates $\\pi$\n",
    "- Don't need multiple independent runs\n",
    "- Justifies using simulation to estimate stationary distribution\n",
    "\n",
    "**Example:**\n",
    "Weather chain with $\\pi_{\\text{sunny}} = 0.7$:\n",
    "- Run chain for 1000 days\n",
    "- Count sunny days ≈ 700\n",
    "- As days → ∞, proportion → 0.7 exactly\n",
    "\n",
    "**Why it matters:**\n",
    "- **Monte Carlo methods**: Single long run is enough\n",
    "- **MCMC sampling**: Frequencies give you the target distribution\n",
    "- **Real-world validation**: Can estimate $\\pi$ from observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8aebf88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_chapman_kolmogorov(P, n, m, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Verify Chapman-Kolmogorov equation: P^(n+m) = P^n * P^m\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    n, m : int\n",
    "        Step counts to verify\n",
    "    tol : float, default=1e-10\n",
    "        Numerical tolerance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    verified : bool\n",
    "        True if equation holds within tolerance\n",
    "    max_error : float\n",
    "        Maximum absolute difference between left and right sides\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    \n",
    "    # Compute P^(n+m) directly\n",
    "    P_n_plus_m = np.linalg.matrix_power(P, n + m)\n",
    "    \n",
    "    # Compute P^n * P^m\n",
    "    P_n = np.linalg.matrix_power(P, n)\n",
    "    P_m = np.linalg.matrix_power(P, m)\n",
    "    P_n_times_P_m = P_n @ P_m\n",
    "    \n",
    "    # Check if they're equal\n",
    "    diff = np.abs(P_n_plus_m - P_n_times_P_m)\n",
    "    max_error = float(np.max(diff))\n",
    "    verified = (max_error < tol)\n",
    "    \n",
    "    return verified, max_error\n",
    "\n",
    "def demonstrate_limiting_behavior(P, max_steps=100, start_state=0):\n",
    "    \"\"\"\n",
    "    Demonstrate convergence to stationary distribution.\n",
    "    \n",
    "    Shows how P^n rows converge to π as n increases.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    max_steps : int, default=100\n",
    "        Maximum number of steps to compute\n",
    "    start_state : int, default=0\n",
    "        Starting state for distribution evolution\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    convergence_data : dict\n",
    "        Contains:\n",
    "        - 'steps': Array of step numbers\n",
    "        - 'distributions': Distribution at each step\n",
    "        - 'stationary': Stationary distribution\n",
    "        - 'distances': Distance from stationary at each step\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n_states = P.shape[0]\n",
    "    \n",
    "    # Get stationary distribution\n",
    "    try:\n",
    "        pi = stationary_distribution(P)\n",
    "    except:\n",
    "        pi = None\n",
    "    \n",
    "    # Track distribution starting from start_state\n",
    "    steps = []\n",
    "    distributions = []\n",
    "    distances = []\n",
    "    \n",
    "    # Initial distribution (deterministic start)\n",
    "    current_dist = np.zeros(n_states)\n",
    "    current_dist[start_state] = 1.0\n",
    "    \n",
    "    P_power = np.eye(n_states)\n",
    "    \n",
    "    for step in range(max_steps + 1):\n",
    "        steps.append(step)\n",
    "        current_dist = P_power[start_state, :]\n",
    "        distributions.append(current_dist.copy())\n",
    "        \n",
    "        if pi is not None:\n",
    "            tv_dist = total_variation_distance(current_dist, pi)\n",
    "            distances.append(tv_dist)\n",
    "        \n",
    "        if step < max_steps:\n",
    "            P_power = P_power @ P\n",
    "    \n",
    "    return {\n",
    "        'steps': np.array(steps),\n",
    "        'distributions': np.array(distributions),\n",
    "        'stationary': pi,\n",
    "        'distances': np.array(distances) if pi is not None else None\n",
    "    }\n",
    "\n",
    "def demonstrate_ergodic_theorem(P, start_state=0, n_steps=10000, target_state=None):\n",
    "    \"\"\"\n",
    "    Demonstrate the Ergodic Theorem by simulation.\n",
    "    \n",
    "    Shows that time average (frequency of visits) converges to stationary probability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : array-like\n",
    "        Transition matrix\n",
    "    start_state : int, default=0\n",
    "        Starting state\n",
    "    n_steps : int, default=10000\n",
    "        Number of simulation steps\n",
    "    target_state : int, optional\n",
    "        State to track. If None, tracks all states.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result : dict\n",
    "        Contains:\n",
    "        - 'path': Simulated path\n",
    "        - 'time_averages': Frequency of each state over time\n",
    "        - 'stationary': Stationary distribution\n",
    "        - 'final_frequencies': Final empirical frequencies\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, float)\n",
    "    n_states = P.shape[0]\n",
    "    \n",
    "    # Simulate path\n",
    "    path = simulate_chain(P, start_state, n_steps)\n",
    "    \n",
    "    # Compute cumulative frequencies\n",
    "    visit_counts = np.zeros((n_steps + 1, n_states))\n",
    "    \n",
    "    for step in range(n_steps + 1):\n",
    "        if step == 0:\n",
    "            visit_counts[0, path[0]] = 1\n",
    "        else:\n",
    "            visit_counts[step] = visit_counts[step - 1]\n",
    "            visit_counts[step, path[step]] += 1\n",
    "    \n",
    "    # Compute time averages (frequencies)\n",
    "    time_averages = np.zeros((n_steps + 1, n_states))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        time_averages[step] = visit_counts[step] / (step + 1)\n",
    "    \n",
    "    # Get stationary distribution\n",
    "    try:\n",
    "        pi = stationary_distribution(P)\n",
    "    except:\n",
    "        pi = None\n",
    "    \n",
    "    return {\n",
    "        'path': path,\n",
    "        'time_averages': time_averages,\n",
    "        'stationary': pi,\n",
    "        'final_frequencies': time_averages[-1],\n",
    "        'steps': np.arange(n_steps + 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9c4489f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapman-Kolmogorov verified: True\n",
      "Maximum error: 0.0000000000\n",
      "Distribution at step 0: [1. 0.]\n",
      "Distribution at step 20: [0.57142857 0.42857143]\n",
      "Stationary distribution: [0.57142857 0.42857143]\n",
      "Distance from stationary at step 20: 0.000000\n",
      "Final empirical frequencies: [0.59340659 0.40659341]\n",
      "Stationary distribution: [0.57142857 0.42857143]\n",
      "Difference: [0.02197802 0.02197802]\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for theoretical foundations\n",
    "P_theory = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "\n",
    "# Verify Chapman-Kolmogorov equation\n",
    "verified, max_err = verify_chapman_kolmogorov(P_theory, n=3, m=2)\n",
    "print(f\"Chapman-Kolmogorov verified: {verified}\")\n",
    "print(f\"Maximum error: {max_err:.10f}\")\n",
    "\n",
    "# Demonstrate limiting behavior\n",
    "limit_data = demonstrate_limiting_behavior(P_theory, max_steps=20, start_state=0)\n",
    "print(f\"Distribution at step 0: {limit_data['distributions'][0]}\")\n",
    "print(f\"Distribution at step 20: {limit_data['distributions'][20]}\")\n",
    "print(f\"Stationary distribution: {limit_data['stationary']}\")\n",
    "print(f\"Distance from stationary at step 20: {limit_data['distances'][20]:.6f}\")\n",
    "\n",
    "# Demonstrate ergodic theorem\n",
    "# Note: This might take a moment with 10000 steps\n",
    "ergodic_data = demonstrate_ergodic_theorem(P_theory, start_state=0, n_steps=1000)\n",
    "print(f\"Final empirical frequencies: {ergodic_data['final_frequencies']}\")\n",
    "print(f\"Stationary distribution: {ergodic_data['stationary']}\")\n",
    "print(f\"Difference: {np.abs(ergodic_data['final_frequencies'] - ergodic_data['stationary'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46962877",
   "metadata": {},
   "source": [
    "# 5) Pattern Recognition & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a056ac5",
   "metadata": {},
   "source": [
    "### 5.1 Confusion Matrix and Metrics\n",
    "\n",
    "**What is this?**\n",
    "A table showing the performance of a binary classifier by comparing predictions to true labels.\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "                Predicted\n",
    "              0 (Neg)  1 (Pos)\n",
    "Actual  0     TN       FP\n",
    "        1     FN       TP\n",
    "```\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Precision** = $\\frac{TP}{TP+FP}$ = Of all positive predictions, how many were correct?\n",
    "  - High precision = Few false alarms\n",
    "- **Recall (Sensitivity)** = $\\frac{TP}{TP+FN}$ = Of all actual positives, how many did we catch?\n",
    "  - High recall = Few missed positives\n",
    "- **Accuracy** = $\\frac{TP+TN}{TP+TN+FP+FN}$ = Overall correctness\n",
    "\n",
    "![title](img/confusion_matrix.png)\n",
    "\n",
    "**Trade-offs:**\n",
    "- Precision ↑ Recall ↓ (and vice versa)\n",
    "- Spam filter: High precision = Less good email marked as spam\n",
    "- Disease screening: High recall = Catch more sick patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f542aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_counts(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    y_pred = np.asarray(y_pred, int)\n",
    "    TP = int(np.sum((y_true==1) & (y_pred==1)))\n",
    "    TN = int(np.sum((y_true==0) & (y_pred==0)))\n",
    "    FP = int(np.sum((y_true==0) & (y_pred==1)))\n",
    "    FN = int(np.sum((y_true==1) & (y_pred==0)))\n",
    "    return {\"TP\":TP, \"TN\":TN, \"FP\":FP, \"FN\":FN}\n",
    "\n",
    "def precision_recall_accuracy(counts):\n",
    "    TP, TN, FP, FN = counts[\"TP\"], counts[\"TN\"], counts[\"FP\"], counts[\"FN\"]\n",
    "    precision = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "    recall = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN) if (TP+TN+FP+FN)>0 else 0.0\n",
    "    return precision, recall, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0aff005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix counts: {'TP': 3, 'TN': 3, 'FP': 1, 'FN': 1}\n",
      "Precision: 0.7500, Recall: 0.7500, Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for confusion matrix and metrics\n",
    "y_true_cf = np.array([1, 0, 1, 1, 0, 1, 0, 0])\n",
    "y_pred_cf = np.array([1, 0, 1, 0, 0, 1, 1, 0])\n",
    "\n",
    "counts = confusion_counts(y_true_cf, y_pred_cf)\n",
    "print(f\"Confusion matrix counts: {counts}\")\n",
    "\n",
    "prec, rec, acc = precision_recall_accuracy(counts)\n",
    "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5ce7f",
   "metadata": {},
   "source": [
    "### 5.2 F1 Score\n",
    "\n",
    "**What is this?**\n",
    "The harmonic mean of precision and recall, providing a balanced measure of classifier performance.\n",
    "\n",
    "**Formula:** $F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "**Alternative form:** $F_1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$\n",
    "\n",
    "**Why harmonic mean?**\n",
    "- Penalizes extreme values (unlike arithmetic mean)\n",
    "- If either precision or recall is low, F1 is low\n",
    "- Example: Precision=1.0, Recall=0.1 → F1=0.18 (not 0.55)\n",
    "\n",
    "**When to use:**\n",
    "- Imbalanced datasets\n",
    "- Need balance between precision and recall\n",
    "- Single metric for model comparison\n",
    "\n",
    "**Variants:**\n",
    "- **F-beta score**: $F_\\beta = (1+\\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\beta^2 \\cdot \\text{Precision} + \\text{Recall}}$\n",
    "- β > 1: Favor recall\n",
    "- β < 1: Favor precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bb90935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute F1 score for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels\n",
    "    y_pred : array-like\n",
    "        Predicted binary labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    f1 : float\n",
    "        F1 score\n",
    "    \"\"\"\n",
    "    counts = confusion_counts(y_true, y_pred)\n",
    "    precision, recall, _ = precision_recall_accuracy(counts)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return float(f1)\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1.0):\n",
    "    \"\"\"\n",
    "    Compute F-beta score.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels\n",
    "    y_pred : array-like\n",
    "        Predicted binary labels\n",
    "    beta : float, default=1.0\n",
    "        Weight of recall vs precision\n",
    "        beta > 1: favor recall\n",
    "        beta < 1: favor precision\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fbeta : float\n",
    "        F-beta score\n",
    "    \"\"\"\n",
    "    counts = confusion_counts(y_true, y_pred)\n",
    "    precision, recall, _ = precision_recall_accuracy(counts)\n",
    "    \n",
    "    if (beta**2 * precision + recall) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    fbeta = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n",
    "    return float(fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "025f24e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7273\n",
      "F2 score (favors recall): 0.6897\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for F1 score\n",
    "y_true_f1 = np.array([1, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n",
    "y_pred_f1 = np.array([1, 0, 0, 1, 0, 1, 1, 0, 1, 0])\n",
    "\n",
    "f1 = f1_score(y_true_f1, y_pred_f1)\n",
    "f2 = fbeta_score(y_true_f1, y_pred_f1, beta=2.0)\n",
    "print(f\"F1 score: {f1:.4f}\")\n",
    "print(f\"F2 score (favors recall): {f2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad8059",
   "metadata": {},
   "source": [
    "### 5.3 ROC Curve and AUC\n",
    "\n",
    "**What is this?**\n",
    "ROC (Receiver Operating Characteristic) curve visualizes classifier performance across all threshold values.\n",
    "\n",
    "**How it works:**\n",
    "1. For each threshold, compute TPR and FPR:\n",
    "   - **TPR (True Positive Rate / Recall)**: $\\frac{TP}{TP+FN}$ (y-axis)\n",
    "   - **FPR (False Positive Rate)**: $\\frac{FP}{FP+TN}$ (x-axis)\n",
    "2. Plot (FPR, TPR) points for all thresholds\n",
    "3. Connect points to form ROC curve\n",
    "\n",
    "**AUC (Area Under Curve):**\n",
    "- AUC = 1.0: Perfect classifier\n",
    "- AUC = 0.5: Random classifier (diagonal line)\n",
    "- AUC < 0.5: Worse than random (flip predictions!)\n",
    "\n",
    "**Interpretation:**\n",
    "- Closer to top-left corner = Better classifier\n",
    "- AUC summarizes performance in single number\n",
    "- Threshold-independent metric\n",
    "\n",
    "**When to use:** Comparing classifiers, handling imbalanced data, when costs unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "626ec5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_curve(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Compute ROC curve points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels (0 or 1)\n",
    "    y_scores : array-like\n",
    "        Predicted scores/probabilities\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fpr : array\n",
    "        False positive rates\n",
    "    tpr : array\n",
    "        True positive rates\n",
    "    thresholds : array\n",
    "        Thresholds used\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    y_scores = np.asarray(y_scores, float)\n",
    "    \n",
    "    # Get unique thresholds (sorted descending)\n",
    "    thresholds = np.unique(y_scores)[::-1]\n",
    "    \n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        counts = confusion_counts(y_true, y_pred)\n",
    "        \n",
    "        # TPR = TP / (TP + FN)\n",
    "        tpr = counts[\"TP\"] / (counts[\"TP\"] + counts[\"FN\"]) if (counts[\"TP\"] + counts[\"FN\"]) > 0 else 0\n",
    "        # FPR = FP / (FP + TN)\n",
    "        fpr = counts[\"FP\"] / (counts[\"FP\"] + counts[\"TN\"]) if (counts[\"FP\"] + counts[\"TN\"]) > 0 else 0\n",
    "        \n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "    \n",
    "    return np.array(fpr_list), np.array(tpr_list), thresholds\n",
    "\n",
    "def compute_auc(fpr, tpr):\n",
    "    \"\"\"Compute Area Under ROC Curve using trapezoidal rule.\"\"\"\n",
    "    # Sort by fpr\n",
    "    indices = np.argsort(fpr)\n",
    "    fpr_sorted = fpr[indices]\n",
    "    tpr_sorted = tpr[indices]\n",
    "    \n",
    "    # Trapezoidal integration\n",
    "    auc = float(np.trapz(tpr_sorted, fpr_sorted))\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0bb9c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for ROC curve and AUC\n",
    "y_true_roc = np.array([1, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n",
    "y_scores_roc = np.array([0.9, 0.8, 0.3, 0.7, 0.2, 0.85, 0.4, 0.1, 0.75, 0.95])\n",
    "\n",
    "fpr, tpr, thresholds = compute_roc_curve(y_true_roc, y_scores_roc)\n",
    "auc_score = compute_auc(fpr, tpr)\n",
    "print(f\"AUC score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fb977",
   "metadata": {},
   "source": [
    "### 5.4 Thresholding Score into Labels\n",
    "\n",
    "**What is this?**\n",
    "Converts continuous scores (e.g., probabilities, confidence values) into binary class labels.\n",
    "\n",
    "**Algorithm:** \n",
    "- If score ≥ threshold → Predict class 1 (positive)\n",
    "- If score < threshold → Predict class 0 (negative)\n",
    "\n",
    "**Threshold selection:**\n",
    "- **threshold = 0.5**: Default for balanced classes\n",
    "- **Lower threshold**: More positive predictions (↑ recall, ↓ precision)\n",
    "- **Higher threshold**: Fewer positive predictions (↓ recall, ↑ precision)\n",
    "\n",
    "**Use cases:**\n",
    "- After logistic regression (threshold probability)\n",
    "- After SVM (threshold decision function)\n",
    "- Tune threshold based on business costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a63affe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_label(scores, threshold=0.0):\n",
    "    scores = np.asarray(scores, float)\n",
    "    return (scores >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c917052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [-1.5  0.5  2.1 -0.3  1.2]\n",
      "Labels (threshold=0.0): [0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for score_to_label\n",
    "scores_thresh = np.array([-1.5, 0.5, 2.1, -0.3, 1.2])\n",
    "labels_thresh = score_to_label(scores_thresh, threshold=0.0)\n",
    "print(f\"Scores: {scores_thresh}\")\n",
    "print(f\"Labels (threshold=0.0): {labels_thresh}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07f317",
   "metadata": {},
   "source": [
    "### 5.5 Cost-Sensitive Decisions\n",
    "\n",
    "**What is this?**\n",
    "Making classification decisions when different types of errors have different costs.\n",
    "\n",
    "**Cost Components:**\n",
    "- `c_fp`: Cost of False Positive (e.g., unnecessary treatment)\n",
    "- `c_fn`: Cost of False Negative (e.g., missed disease)\n",
    "- `c_tp`: Cost of True Positive (usually 0)\n",
    "- `c_tn`: Cost of True Negative (usually 0)\n",
    "\n",
    "**Total Cost:** $\\text{Cost} = c_{FP} \\cdot FP + c_{FN} \\cdot FN + c_{TP} \\cdot TP + c_{TN} \\cdot TN$\n",
    "\n",
    "**Threshold Sweeping:**\n",
    "Try many threshold values and choose the one that minimizes total cost.\n",
    "\n",
    "**Example:**\n",
    "- Medical test: FN (missed disease) might cost 100x more than FP (unnecessary follow-up)\n",
    "- Lower threshold to catch more cases (↑ recall) even if more false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7b8a2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost_from_counts(counts, c_fp=1.0, c_fn=1.0, c_tp=0.0, c_tn=0.0):\n",
    "    return (counts[\"FP\"]*c_fp + counts[\"FN\"]*c_fn + counts[\"TP\"]*c_tp + counts[\"TN\"]*c_tn)\n",
    "\n",
    "def sweep_thresholds(y_true, scores, thresholds, c_fp=1.0, c_fn=1.0):\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    scores = np.asarray(scores, float)\n",
    "    best = None\n",
    "    for t in thresholds:\n",
    "        y_pred = (scores >= t).astype(int)\n",
    "        counts = confusion_counts(y_true, y_pred)\n",
    "        cost = total_cost_from_counts(counts, c_fp=c_fp, c_fn=c_fn)\n",
    "        row = {\"threshold\": float(t), \"cost\": float(cost), **counts}\n",
    "        if best is None or row[\"cost\"] < best[\"cost\"]:\n",
    "            best = row\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "190b5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.5000\n",
      "Minimum cost: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Sample call for cost-sensitive decisions\n",
    "y_true_cost = np.array([1, 0, 1, 0, 1, 1, 0, 0])\n",
    "scores_cost = np.array([0.8, 0.3, 0.9, 0.2, 0.7, 0.85, 0.4, 0.15])\n",
    "thresholds_to_try = np.linspace(0, 1, 11)\n",
    "\n",
    "# FN costs 10x more than FP\n",
    "best_result = sweep_thresholds(y_true_cost, scores_cost, thresholds_to_try, c_fp=1.0, c_fn=10.0)\n",
    "print(f\"Best threshold: {best_result['threshold']:.4f}\")\n",
    "print(f\"Minimum cost: {best_result['cost']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc427845",
   "metadata": {},
   "source": [
    "### 5.6 Logistic Regression (from scratch)\n",
    "\n",
    "**What is this?**\n",
    "A linear model for binary classification that predicts probabilities using the sigmoid function.\n",
    "\n",
    "**Model:** $P(y=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$\n",
    "\n",
    "**How it works:**\n",
    "1. **Predict probability:** Apply sigmoid to linear combination of features\n",
    "2. **Loss function:** Negative log-likelihood (cross-entropy)\n",
    "   - $L = -\\sum [y \\log(p) + (1-y)\\log(1-p)]$\n",
    "3. **Training:** Minimize loss using optimization (e.g., gradient descent, CG)\n",
    "\n",
    "**Output:**\n",
    "- `w`: Feature weights (coefficients)\n",
    "- `b`: Bias (intercept)\n",
    "- Higher probability → More confident in class 1\n",
    "\n",
    "**Advantages:**\n",
    "- Outputs calibrated probabilities\n",
    "- Interpretable coefficients\n",
    "- Works well for linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "775cffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_predict_proba(X, w, b):\n",
    "    X = np.asarray(X, float)\n",
    "    z = X @ w + b\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_neg_loglik(params, X, y, eps=1e-12):\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "    w = params[:-1]\n",
    "    b = params[-1]\n",
    "    p = logistic_predict_proba(X, w, b)\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return float(np.sum(-(y*np.log(p) + (1-y)*np.log(1-p))))\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "def fit_logistic(X, y):\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "    init = np.zeros(X.shape[1] + 1)\n",
    "    res = optimize.minimize(lambda p: logistic_neg_loglik(p, X, y), init, method=\"CG\")\n",
    "    w = res.x[:-1]\n",
    "    b = res.x[-1]\n",
    "    return w, b, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9b82914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression weights: [2.22986989 2.28435107 0.41216203]\n",
      "Logistic regression bias: -1.9951\n",
      "Sample probabilities: [0.03942043 0.07558844 0.93060092 0.14749589 0.89923138]\n"
     ]
    }
   ],
   "source": [
    "# Sample call for logistic regression\n",
    "X_log, y_log = make_classification(n_samples=50, n_features=3, n_informative=3, n_redundant=0, random_state=42)\n",
    "w_log, b_log, res_log = fit_logistic(X_log, y_log)\n",
    "print(f\"Logistic regression weights: {w_log}\")\n",
    "print(f\"Logistic regression bias: {b_log:.4f}\")\n",
    "\n",
    "# Predict probabilities\n",
    "probs_log = logistic_predict_proba(X_log[:5], w_log, b_log)\n",
    "print(f\"Sample probabilities: {probs_log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1db9e",
   "metadata": {},
   "source": [
    "### 5.7 Vapnik–Chervonenkis (VC) Dimension\n",
    "\n",
    "**What is this?**\n",
    "A measure of the complexity/capacity of a hypothesis class - how flexible or expressive it is.\n",
    "\n",
    "**Definition:** The VC dimension is the largest number of points that can be **shattered** by the hypothesis class.\n",
    "\n",
    "**Shattering:** A set of $n$ points is shattered if the hypothesis class can perfectly separate them for ALL possible binary labelings (all $2^n$ ways).\n",
    "\n",
    "**VC Dimensions for Common Classifiers:**\n",
    "- **Linear classifier in $d$ dimensions**: VC dimension = $d + 1$\n",
    "  - In 2D: Can shatter 3 points, but not all configurations of 4 points\n",
    "- **Perceptron in $\\mathbb{R}^d$**: VC dimension = $d + 1$\n",
    "- **Decision tree of depth $h$**: VC dimension ≈ $2^h$\n",
    "- **Neural network with $W$ weights**: VC dimension ≈ $O(W \\log W)$\n",
    "\n",
    "**Example: Linear classifier in 2D**\n",
    "- Can shatter 3 points (VC dimension = 3)\n",
    "- Cannot shatter 4 points in general position (e.g., XOR problem)\n",
    "\n",
    "**Intuition:**\n",
    "- Higher VC dimension = More expressive = Can fit more complex patterns\n",
    "- But also = Higher risk of overfitting\n",
    "- VC dimension tells us how many \"effective parameters\" a model has\n",
    "\n",
    "**Why it matters:**\n",
    "- Foundation for statistical learning theory\n",
    "- Used to derive generalization bounds\n",
    "- Helps understand model complexity without training\n",
    "\n",
    "**Trade-off:**\n",
    "- Low VC dimension: Risk underfitting (too simple)\n",
    "- High VC dimension: Risk overfitting (too complex)\n",
    "- Need enough data relative to VC dimension for good generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6a0839d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vc_dimension_linear(d):\n",
    "    \"\"\"\n",
    "    VC dimension of linear classifier in d-dimensional space.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : int\n",
    "        Number of dimensions/features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : VC dimension (d + 1)\n",
    "    \"\"\"\n",
    "    return d + 1\n",
    "\n",
    "def can_shatter_check(n_points, vc_dim):\n",
    "    \"\"\"\n",
    "    Check if n points can potentially be shattered given VC dimension.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_points : int\n",
    "        Number of points to check\n",
    "    vc_dim : int\n",
    "        VC dimension of hypothesis class\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool : True if n_points <= vc_dim (can be shattered)\n",
    "    \"\"\"\n",
    "    return n_points <= vc_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "36083150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VC dimension of linear classifier in 10D: 11\n",
      "Can shatter 5 points: True\n",
      "Can shatter 15 points: False\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for VC dimension\n",
    "d_features = 10\n",
    "vc_dim = vc_dimension_linear(d_features)\n",
    "print(f\"VC dimension of linear classifier in {d_features}D: {vc_dim}\")\n",
    "print(f\"Can shatter 5 points: {can_shatter_check(5, vc_dim)}\")\n",
    "print(f\"Can shatter 15 points: {can_shatter_check(15, vc_dim)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc3f1a",
   "metadata": {},
   "source": [
    "### 5.8 Generalization Bounds\n",
    "\n",
    "**What is this?**\n",
    "Theoretical guarantees on how well a model trained on finite data will perform on unseen data.\n",
    "\n",
    "**Main Idea:** With high probability, test error is close to training error, but depends on:\n",
    "- Sample size $n$\n",
    "- Model complexity (VC dimension $h$)\n",
    "- Confidence level $\\delta$\n",
    "\n",
    "**VC Generalization Bound:**\n",
    "With probability at least $1 - \\delta$:\n",
    "$$R_{\\text{test}} \\leq R_{\\text{train}} + \\sqrt{\\frac{h(\\log(2n/h) + 1) - \\log(\\delta/4)}{n}}$$\n",
    "\n",
    "where:\n",
    "- $R_{\\text{test}}$ = True risk (test error)\n",
    "- $R_{\\text{train}}$ = Empirical risk (training error)\n",
    "- $h$ = VC dimension\n",
    "- $n$ = Number of training samples\n",
    "- $\\delta$ = Failure probability\n",
    "\n",
    "**Key Insights:**\n",
    "1. **Gap decreases with more data**: Bound is $O(1/\\sqrt{n})$\n",
    "2. **Gap increases with model complexity**: Higher VC dimension → Worse bound\n",
    "3. **Trade-off**: Complex models may have lower training error but larger gap\n",
    "\n",
    "**Practical Implications:**\n",
    "- **Sample size requirements**: Need $n \\gg h$ for good generalization\n",
    "- **Model selection**: Simpler models generalize better with limited data\n",
    "- **Overfitting detection**: If training error ≪ test error, you're overfitting\n",
    "\n",
    "**Rademacher Complexity Bound (Alternative):**\n",
    "$$R_{\\text{test}} \\leq R_{\\text{train}} + 2\\mathfrak{R}_n(\\mathcal{H}) + \\sqrt{\\frac{\\log(1/\\delta)}{2n}}$$\n",
    "\n",
    "where $\\mathfrak{R}_n(\\mathcal{H})$ measures how well the hypothesis class fits random noise.\n",
    "\n",
    "**PAC Learning (Probably Approximately Correct):**\n",
    "For $\\epsilon$ accuracy and $\\delta$ confidence, need sample size:\n",
    "$$n = O\\left(\\frac{h + \\log(1/\\delta)}{\\epsilon^2}\\right)$$\n",
    "\n",
    "**When to use these bounds:**\n",
    "- Theoretical analysis of learning algorithms\n",
    "- Determining how much data you need\n",
    "- Understanding model selection trade-offs\n",
    "- Note: Bounds are often loose in practice (pessimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fe92b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vc_generalization_bound(train_error, n, h, delta=0.05):\n",
    "    \"\"\"\n",
    "    Compute VC generalization bound on test error.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_error : float\n",
    "        Empirical risk (training error rate)\n",
    "    n : int\n",
    "        Number of training samples\n",
    "    h : int\n",
    "        VC dimension of hypothesis class\n",
    "    delta : float, default=0.05\n",
    "        Failure probability (e.g., 0.05 for 95% confidence)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    upper_bound : float\n",
    "        Upper bound on test error with probability 1-delta\n",
    "    gap : float\n",
    "        Generalization gap (difference from training error)\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    # VC bound formula\n",
    "    term1 = h * (math.log(2 * n / h) + 1)\n",
    "    term2 = -math.log(delta / 4)\n",
    "    gap = math.sqrt((term1 + term2) / n)\n",
    "    \n",
    "    upper_bound = train_error + gap\n",
    "    return float(upper_bound), float(gap)\n",
    "\n",
    "def pac_sample_size(h, epsilon, delta=0.05):\n",
    "    \"\"\"\n",
    "    Compute required sample size for PAC learning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    h : int\n",
    "        VC dimension\n",
    "    epsilon : float\n",
    "        Desired accuracy (error tolerance)\n",
    "    delta : float, default=0.05\n",
    "        Failure probability\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : Minimum number of samples needed\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    # PAC formula: n = O((h + log(1/delta)) / epsilon^2)\n",
    "    numerator = h + math.log(1 / delta)\n",
    "    n = numerator / (epsilon ** 2)\n",
    "    \n",
    "    return int(np.ceil(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7888b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.0500\n",
      "Generalization bound (test error <= ): 0.3095\n",
      "Generalization gap: 0.2595\n",
      "Required samples for PAC learning: 1300\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for generalization bounds\n",
    "train_err = 0.05\n",
    "n_samples = 1000\n",
    "vc_dim = 10\n",
    "\n",
    "bound, gap = vc_generalization_bound(train_err, n_samples, vc_dim, delta=0.05)\n",
    "print(f\"Training error: {train_err:.4f}\")\n",
    "print(f\"Generalization bound (test error <= ): {bound:.4f}\")\n",
    "print(f\"Generalization gap: {gap:.4f}\")\n",
    "\n",
    "# PAC sample size\n",
    "required_n = pac_sample_size(h=10, epsilon=0.1, delta=0.05)\n",
    "print(f\"Required samples for PAC learning: {required_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b30469",
   "metadata": {},
   "source": [
    "**Comparing Bounds Across Different VC Dimensions:**\n",
    "\n",
    "**Question:** If we use a classifier with smaller VC dimension, do we get a tighter bound?\n",
    "\n",
    "**Answer:** YES! Lower VC dimension → Smaller generalization gap → Tighter bound on accuracy\n",
    "\n",
    "**Formula reminder:**\n",
    "$$\\text{Test Error} \\leq \\text{Train Error} + \\underbrace{\\sqrt{\\frac{h(\\log(2n/h) + 1) - \\log(\\delta/4)}{n}}}_{\\text{Gap depends on } h}$$\n",
    "\n",
    "**Key insight:** The gap grows with $\\sqrt{h}$, so:\n",
    "- Classifier with VC-dim 3 has smaller gap than VC-dim 10\n",
    "- More data (larger $n$) reduces the gap (grows as $1/\\sqrt{n}$)\n",
    "- Simpler models → Better guarantees (but may underfit!)\n",
    "\n",
    "**Example comparison:**\n",
    "Given same training error and sample size, which classifier gives tighter bound?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ebd63dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vc_bounds(train_error, n, vc_dims, delta=0.05):\n",
    "    \"\"\"\n",
    "    Compare generalization bounds for different VC dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_error : float\n",
    "        Training error (same for all classifiers)\n",
    "    n : int\n",
    "        Sample size\n",
    "    vc_dims : list of int\n",
    "        List of VC dimensions to compare\n",
    "    delta : float\n",
    "        Confidence level\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results for each VC dimension\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for h in vc_dims:\n",
    "        bound, gap = vc_generalization_bound(train_error, n, h, delta)\n",
    "        results[h] = {\n",
    "            'bound': bound,\n",
    "            'gap': gap,\n",
    "            'interval_width': 2 * gap  # ±gap around point estimate\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def answer_vc_comparison_question(train_error, n, h_current, h_alternative, delta=0.05):\n",
    "    \"\"\"\n",
    "    Answer: \"Would using VC-dim h_alternative give smaller interval than h_current?\"\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_error : float\n",
    "        Training error\n",
    "    n : int\n",
    "        Number of samples (using \"all data\")\n",
    "    h_current : int\n",
    "        Current classifier's VC dimension\n",
    "    h_alternative : int\n",
    "        Alternative classifier's VC dimension\n",
    "    delta : float\n",
    "        Confidence level\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comparison results with answer\n",
    "    \"\"\"\n",
    "    bound_curr, gap_curr = vc_generalization_bound(train_error, n, h_current, delta)\n",
    "    bound_alt, gap_alt = vc_generalization_bound(train_error, n, h_alternative, delta)\n",
    "    \n",
    "    # Interval width is 2*gap (±gap)\n",
    "    interval_curr = 2 * gap_curr\n",
    "    interval_alt = 2 * gap_alt\n",
    "    \n",
    "    answer = interval_alt < interval_curr\n",
    "    \n",
    "    return {\n",
    "        'h_current': h_current,\n",
    "        'h_alternative': h_alternative,\n",
    "        'current_bound': bound_curr,\n",
    "        'current_gap': gap_curr,\n",
    "        'current_interval_width': interval_curr,\n",
    "        'alternative_bound': bound_alt,\n",
    "        'alternative_gap': gap_alt,\n",
    "        'alternative_interval_width': interval_alt,\n",
    "        'smaller_interval': answer,\n",
    "        'improvement': interval_curr - interval_alt if answer else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "656b3ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Comparison: Train Error = 0.050, n = 1000, δ = 0.05\n",
      "======================================================================\n",
      "\n",
      "VC Dimension = 3:\n",
      "  Upper Bound:      0.2140\n",
      "  Generalization Gap: 0.1640\n",
      "  Interval Width:   ±0.1640 (total: 0.3280)\n",
      "\n",
      "VC Dimension = 10:\n",
      "  Upper Bound:      0.3095\n",
      "  Generalization Gap: 0.2595\n",
      "  Interval Width:   ±0.2595 (total: 0.5191)\n",
      "\n",
      "VC Dimension = 50:\n",
      "  Upper Bound:      0.5387\n",
      "  Generalization Gap: 0.4887\n",
      "  Interval Width:   ±0.4887 (total: 0.9774)\n",
      "\n",
      "======================================================================\n",
      "QUESTION: Would VC-dim 3 give smaller interval than VC-dim 10?\n",
      "======================================================================\n",
      "\n",
      "Current (VC-dim 10):\n",
      "  Bound: 0.3095\n",
      "  Interval width: 0.5191\n",
      "\n",
      "Alternative (VC-dim 3):\n",
      "  Bound: 0.2140\n",
      "  Interval width: 0.3280\n",
      "\n",
      "ANSWER: YES\n",
      "  → VC-dim 3 gives SMALLER interval\n",
      "  → Improvement: 0.1911 narrower interval\n"
     ]
    }
   ],
   "source": [
    "# Example: Compare classifiers with different VC dimensions\n",
    "train_error = 0.05  # 5% training error\n",
    "n_samples = 1000    # Using all 1000 data points\n",
    "delta = 0.05        # 95% confidence\n",
    "\n",
    "# Compare VC dimensions: 3 vs 10 vs 50\n",
    "vc_dimensions = [3, 10, 50]\n",
    "comparison = compare_vc_bounds(train_error, n_samples, vc_dimensions, delta)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Comparison: Train Error = {train_error:.3f}, n = {n_samples}, δ = {delta}\")\n",
    "print(\"=\" * 70)\n",
    "for h, result in comparison.items():\n",
    "    print(f\"\\nVC Dimension = {h}:\")\n",
    "    print(f\"  Upper Bound:      {result['bound']:.4f}\")\n",
    "    print(f\"  Generalization Gap: {result['gap']:.4f}\")\n",
    "    print(f\"  Interval Width:   ±{result['gap']:.4f} (total: {result['interval_width']:.4f})\")\n",
    "\n",
    "# Answer specific question: Would VC-dim 3 give smaller interval than VC-dim 10?\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"QUESTION: Would VC-dim 3 give smaller interval than VC-dim 10?\")\n",
    "print(\"=\" * 70)\n",
    "answer = answer_vc_comparison_question(train_error, n_samples, h_current=10, h_alternative=3, delta=delta)\n",
    "\n",
    "print(f\"\\nCurrent (VC-dim {answer['h_current']}):\")\n",
    "print(f\"  Bound: {answer['current_bound']:.4f}\")\n",
    "print(f\"  Interval width: {answer['current_interval_width']:.4f}\")\n",
    "\n",
    "print(f\"\\nAlternative (VC-dim {answer['h_alternative']}):\")\n",
    "print(f\"  Bound: {answer['alternative_bound']:.4f}\")\n",
    "print(f\"  Interval width: {answer['alternative_interval_width']:.4f}\")\n",
    "\n",
    "print(f\"\\nANSWER: {'YES' if answer['smaller_interval'] else 'NO'}\")\n",
    "if answer['smaller_interval']:\n",
    "    print(f\"  → VC-dim {answer['h_alternative']} gives SMALLER interval\")\n",
    "    print(f\"  → Improvement: {answer['improvement']:.4f} narrower interval\")\n",
    "else:\n",
    "    print(f\"  → VC-dim {answer['h_alternative']} gives LARGER interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773acec",
   "metadata": {},
   "source": [
    "# 6) High Dimensional Phenomena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b764382",
   "metadata": {},
   "source": [
    "### 6.1 Distance Concentration\n",
    "\n",
    "**What is this?**\n",
    "A surprising phenomenon: In high dimensions, distances between random points become very similar!\n",
    "\n",
    "**The Curse of Dimensionality:**\n",
    "As dimension $d$ increases:\n",
    "- All points appear equally far apart\n",
    "- Ratio $\\frac{\\text{max distance}}{\\text{min distance}} \\to 1$\n",
    "- Intuition: In high-D space, there's \"so much room\" that everything spreads out\n",
    "\n",
    "**Why it matters:**\n",
    "- **Nearest neighbor methods fail**: \"Nearest\" and \"farthest\" become meaningless\n",
    "- **Clustering becomes hard**: All points seem equally distant\n",
    "- **Similarity search breaks down**: Need dimension reduction!\n",
    "\n",
    "**Example:** \n",
    "In 2D: Some points close, some far (ratio ~5-10)\n",
    "In 1000D: All points ~same distance (ratio ~1.1)\n",
    "\n",
    "**Takeaway:** High-dimensional data needs special treatment (PCA, manifold learning, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "727bd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_concentration_demo(n=2000, d=2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(0, 1, size=(n, d))\n",
    "    # distances from first point\n",
    "    diffs = X - X[0]\n",
    "    dist = np.linalg.norm(diffs, axis=1)[1:]\n",
    "    return {\n",
    "        \"mean_dist\": float(np.mean(dist)),\n",
    "        \"min_dist\": float(np.min(dist)),\n",
    "        \"max_dist\": float(np.max(dist)),\n",
    "        \"ratio_max_min\": float(np.max(dist)/np.min(dist))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fc5631d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D - ratio max/min: 104.3617\n",
      "100D - ratio max/min: 1.4944\n"
     ]
    }
   ],
   "source": [
    "# Sample call for distance_concentration_demo\n",
    "# Compare 2D vs 100D\n",
    "result_2d = distance_concentration_demo(n=1000, d=2, seed=42)\n",
    "result_100d = distance_concentration_demo(n=1000, d=100, seed=42)\n",
    "print(f\"2D - ratio max/min: {result_2d['ratio_max_min']:.4f}\")\n",
    "print(f\"100D - ratio max/min: {result_100d['ratio_max_min']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9821e67",
   "metadata": {},
   "source": [
    "### 6.2 Geometry of Hyperspheres\n",
    "\n",
    "**What is this?**\n",
    "Counter-intuitive properties of spheres in high-dimensional spaces.\n",
    "\n",
    "**Volume Concentration:** In high dimensions, almost all the volume of a hypersphere is concentrated near its surface!\n",
    "\n",
    "**Key Facts:**\n",
    "1. **Volume of $d$-dimensional unit sphere**: $V_d = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)}$\n",
    "   - Initially grows with dimension\n",
    "   - Peaks around $d \\approx 5$\n",
    "   - Then decays exponentially to 0!\n",
    "\n",
    "2. **Volume in a shell**: Consider shell from radius $r$ to $1$ (with $r$ close to 1, e.g., $r=0.99$)\n",
    "   - Fraction of volume in shell: $1 - r^d$\n",
    "   - For $r=0.99$, $d=100$: Shell contains $>63\\%$ of total volume\n",
    "   - As $d \\to \\infty$, nearly all volume is near the surface\n",
    "\n",
    "3. **Distance from center**: Random point in unit sphere has distance from center ≈ 1 for large $d$\n",
    "\n",
    "**Implications:**\n",
    "- **Curse of dimensionality**: Hard to sample interior of high-D sphere\n",
    "- **Distance metrics**: Most points are far from center\n",
    "- **Data distribution**: Real data often lies near surface of hypersphere when normalized\n",
    "\n",
    "**Formula for shell volume:** \n",
    "$$\\frac{V_{\\text{shell}}}{V_{\\text{sphere}}} = 1 - r^d$$\n",
    "\n",
    "**Example:** In 100D, 99% of volume is in outer 10% radius shell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43b20c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "\n",
    "def hypersphere_volume(d, r=1.0):\n",
    "    \"\"\"\n",
    "    Compute volume of d-dimensional hypersphere of radius r.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : int\n",
    "        Dimension\n",
    "    r : float, default=1.0\n",
    "        Radius\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Volume of the hypersphere\n",
    "    \"\"\"\n",
    "    volume = (np.pi ** (d/2)) / gamma(d/2 + 1) * (r ** d)\n",
    "    return float(volume)\n",
    "\n",
    "def hypersphere_shell_fraction(d, r_inner):\n",
    "    \"\"\"\n",
    "    Fraction of hypersphere volume in shell from r_inner to 1.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : int\n",
    "        Dimension\n",
    "    r_inner : float\n",
    "        Inner radius (0 < r_inner < 1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Fraction of volume in shell\n",
    "    \"\"\"\n",
    "    fraction = 1 - (r_inner ** d)\n",
    "    return float(fraction)\n",
    "\n",
    "def hypersphere_sample_radius_stats(n_samples, d, seed=0):\n",
    "    \"\"\"\n",
    "    Statistics of distances from origin for random points in unit hypersphere.\n",
    "    \n",
    "    Returns mean, std, min, max of radii.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Sample from unit sphere using normal distribution + normalization\n",
    "    X = rng.normal(0, 1, size=(n_samples, d))\n",
    "    radii = np.linalg.norm(X, axis=1)\n",
    "    \n",
    "    # Scale to unit sphere\n",
    "    X = X / radii[:, np.newaxis]\n",
    "    \n",
    "    # For uniform distribution in sphere, scale by r^(1/d)\n",
    "    r = rng.uniform(0, 1, size=n_samples) ** (1/d)\n",
    "    X = X * r[:, np.newaxis]\n",
    "    \n",
    "    radii = np.linalg.norm(X, axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"mean\": float(np.mean(radii)),\n",
    "        \"std\": float(np.std(radii)),\n",
    "        \"min\": float(np.min(radii)),\n",
    "        \"max\": float(np.max(radii))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3e66fc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit hypersphere volume in 3D: 4.1888\n",
      "Unit hypersphere volume in 10D: 2.550164\n",
      "Fraction of volume in outer 1% shell (100D): 0.6340\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for hypersphere geometry\n",
    "vol_3d = hypersphere_volume(d=3, r=1.0)\n",
    "vol_10d = hypersphere_volume(d=10, r=1.0)\n",
    "print(f\"Unit hypersphere volume in 3D: {vol_3d:.4f}\")\n",
    "print(f\"Unit hypersphere volume in 10D: {vol_10d:.6f}\")\n",
    "\n",
    "# Shell fraction\n",
    "shell_frac_100d = hypersphere_shell_fraction(d=100, r_inner=0.99)\n",
    "print(f\"Fraction of volume in outer 1% shell (100D): {shell_frac_100d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be4cf6",
   "metadata": {},
   "source": [
    "### 6.3 Geometry of Hypercubes\n",
    "\n",
    "**What is this?**\n",
    "Surprising properties of cubes (unit hypercubes $[0,1]^d$) in high dimensions.\n",
    "\n",
    "**Key Facts:**\n",
    "\n",
    "1. **Volume stays constant**: Unit hypercube always has volume = 1\n",
    "   - Unlike hypersphere which shrinks!\n",
    "\n",
    "2. **Diagonal length grows**: $\\sqrt{d}$\n",
    "   - In 2D: diagonal = $\\sqrt{2} \\approx 1.41$\n",
    "   - In 100D: diagonal = $10$\n",
    "   - In 10,000D: diagonal = $100$\n",
    "   - Points at opposite corners get arbitrarily far apart!\n",
    "\n",
    "3. **Inscribed sphere shrinks relative to cube**:\n",
    "   - Largest sphere fitting in unit cube has radius $1/2$\n",
    "   - Volume ratio: $\\frac{V_{\\text{sphere}}}{V_{\\text{cube}}} = \\frac{\\pi^{d/2}}{2^d \\cdot \\Gamma(d/2+1)} \\to 0$ as $d \\to \\infty$\n",
    "   - In high dimensions, cube is \"mostly corners\"!\n",
    "\n",
    "4. **Most volume is in corners**:\n",
    "   - Consider inscribed sphere of radius $r=1/2$ (largest that fits)\n",
    "   - Volume outside sphere = $1 - \\frac{\\pi^{d/2}}{2^d \\cdot \\Gamma(d/2+1)}$\n",
    "   - For $d=10$: ~99.8% of cube volume is outside inscribed sphere\n",
    "   - Random point in cube is almost surely far from center!\n",
    "\n",
    "5. **Distance from center**: Random point in $[-1,1]^d$ cube has expected squared distance from origin = $d/3$\n",
    "   - Typical distance ≈ $\\sqrt{d/3}$\n",
    "\n",
    "**Implications:**\n",
    "- **Sampling**: Uniform sampling gives points mostly near corners\n",
    "- **Optimization**: Search space is dominated by corners\n",
    "- **Data analysis**: Center of cube is \"empty\" - most data near boundaries\n",
    "\n",
    "**Why it matters:**\n",
    "- Explains why high-D optimization is hard\n",
    "- Motivates dimension reduction\n",
    "- Shows limits of grid-based methods in high dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7bcdc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypercube_diagonal_length(d):\n",
    "    \"\"\"\n",
    "    Compute diagonal length of unit hypercube [0,1]^d.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : int\n",
    "        Dimension\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Diagonal length (sqrt(d))\n",
    "    \"\"\"\n",
    "    return float(np.sqrt(d))\n",
    "\n",
    "def hypercube_inscribed_sphere_ratio(d):\n",
    "    \"\"\"\n",
    "    Ratio of inscribed sphere volume to hypercube volume.\n",
    "    \n",
    "    The largest sphere that fits in unit cube [0,1]^d has radius 1/2.\n",
    "    This computes what fraction of the cube's volume it occupies.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : int\n",
    "        Dimension\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Volume ratio (goes to 0 as d increases!)\n",
    "    \"\"\"\n",
    "    from scipy.special import gamma\n",
    "    \n",
    "    # Sphere radius = 0.5 (to fit in unit cube)\n",
    "    # Sphere volume = pi^(d/2) / Gamma(d/2 + 1) * r^d\n",
    "    sphere_vol = (np.pi ** (d/2)) / gamma(d/2 + 1) * (0.5 ** d)\n",
    "    cube_vol = 1.0\n",
    "    \n",
    "    return float(sphere_vol / cube_vol)\n",
    "\n",
    "def hypercube_sample_distance_stats(n_samples, d, seed=0):\n",
    "    \"\"\"\n",
    "    Statistics of distances from origin for random points in unit cube [-1,1]^d.\n",
    "    \n",
    "    Returns mean, std of squared distances.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Sample uniformly from [-1, 1]^d\n",
    "    X = rng.uniform(-1, 1, size=(n_samples, d))\n",
    "    \n",
    "    # Distances from origin\n",
    "    squared_distances = np.sum(X**2, axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"mean_squared_dist\": float(np.mean(squared_distances)),\n",
    "        \"expected_squared_dist\": d / 3.0,  # Theoretical value\n",
    "        \"typical_dist\": float(np.sqrt(np.mean(squared_distances)))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9cd0293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal length of 10D unit cube: 3.1623\n",
      "Inscribed sphere / cube volume ratio (10D): 0.002490\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for hypercube geometry\n",
    "diag_10d = hypercube_diagonal_length(10)\n",
    "print(f\"Diagonal length of 10D unit cube: {diag_10d:.4f}\")\n",
    "\n",
    "sphere_ratio_10d = hypercube_inscribed_sphere_ratio(10)\n",
    "print(f\"Inscribed sphere / cube volume ratio (10D): {sphere_ratio_10d:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b545181",
   "metadata": {},
   "source": [
    "### 6.4 Random Projections (Johnson-Lindenstrauss Lemma)\n",
    "\n",
    "**What is this?**\n",
    "A powerful dimensionality reduction technique: projecting high-dimensional data onto random lower-dimensional subspace preserves distances approximately!\n",
    "\n",
    "**Johnson-Lindenstrauss Lemma:**\n",
    "For any set of $n$ points in $\\mathbb{R}^d$ and any $0 < \\epsilon < 1$, there exists a linear map $f: \\mathbb{R}^d \\to \\mathbb{R}^k$ where:\n",
    "$$k = O\\left(\\frac{\\log n}{\\epsilon^2}\\right)$$\n",
    "\n",
    "such that for all pairs of points $u, v$:\n",
    "$$(1-\\epsilon)\\|u-v\\|^2 \\leq \\|f(u)-f(v)\\|^2 \\leq (1+\\epsilon)\\|u-v\\|^2$$\n",
    "\n",
    "**Key Points:**\n",
    "1. **Target dimension depends on $n$, not $d$**: Can reduce from millions of dimensions to hundreds!\n",
    "2. **Random projection works**: Use random Gaussian matrix $R \\in \\mathbb{R}^{k \\times d}$\n",
    "3. **Distances preserved**: Pairwise distances change by at most factor $(1 \\pm \\epsilon)$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Generate random matrix $R$ with entries $\\sim N(0, 1/k)$\n",
    "2. Project: $X_{\\text{low}} = X \\cdot R^T / \\sqrt{k}$\n",
    "3. Result: Dimension reduced from $d$ to $k$ with distances approximately preserved\n",
    "\n",
    "**Minimum target dimension:** \n",
    "$$k \\geq \\frac{4 \\log n}{\\epsilon^2 - \\epsilon^3}$$\n",
    "\n",
    "**Example:**\n",
    "- Original: 10,000 dimensions\n",
    "- Data: 1,000 points\n",
    "- Target: $\\epsilon = 0.1$ (10% error)\n",
    "- Required: $k \\approx 920$ dimensions\n",
    "- Reduction: $>10\\times$ smaller!\n",
    "\n",
    "**When to use:**\n",
    "- **Approximate nearest neighbor search**: Speed up with minimal accuracy loss\n",
    "- **Fast distances**: Computing distances in lower dimension\n",
    "- **Preprocessing**: Before clustering, classification\n",
    "- **Privacy**: Random projection can obscure original data\n",
    "- **Streaming data**: Online dimension reduction\n",
    "\n",
    "**Advantages:**\n",
    "- Very fast (just matrix multiplication)\n",
    "- No training required (purely random)\n",
    "- Theory guarantees quality\n",
    "- Works for any data distribution\n",
    "\n",
    "**Limitations:**\n",
    "- Only preserves distances (not other structure)\n",
    "- Requires $k = O(\\log n / \\epsilon^2)$ which can still be large for huge $n$\n",
    "- Approximate method (not exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "da6288b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def johnson_lindenstrauss_min_dim(n_samples, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Compute minimum target dimension for Johnson-Lindenstrauss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of data points\n",
    "    epsilon : float, default=0.1\n",
    "        Maximum distortion (e.g., 0.1 for 10% error)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : Minimum dimension k\n",
    "    \"\"\"\n",
    "    k = 4 * np.log(n_samples) / (epsilon**2 - epsilon**3)\n",
    "    return int(np.ceil(k))\n",
    "\n",
    "def random_projection_matrix(d_original, k_target, seed=0):\n",
    "    \"\"\"\n",
    "    Generate random projection matrix for Johnson-Lindenstrauss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d_original : int\n",
    "        Original dimension\n",
    "    k_target : int\n",
    "        Target dimension\n",
    "    seed : int, default=0\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    R : array, shape (k_target, d_original)\n",
    "        Random projection matrix\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Gaussian random matrix\n",
    "    R = rng.normal(0, 1, size=(k_target, d_original))\n",
    "    \n",
    "    # Normalize by sqrt(k) for variance preservation\n",
    "    R = R / np.sqrt(k_target)\n",
    "    \n",
    "    return R\n",
    "\n",
    "def random_projection(X, k_target, seed=0):\n",
    "    \"\"\"\n",
    "    Apply random projection to reduce dimensionality.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array, shape (n_samples, d_original)\n",
    "        Original high-dimensional data\n",
    "    k_target : int\n",
    "        Target dimension\n",
    "    seed : int, default=0\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_projected : array, shape (n_samples, k_target)\n",
    "        Projected lower-dimensional data\n",
    "    R : array, shape (k_target, d_original)\n",
    "        Projection matrix (for applying to new data)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    n_samples, d_original = X.shape\n",
    "    \n",
    "    # Generate random projection matrix\n",
    "    R = random_projection_matrix(d_original, k_target, seed)\n",
    "    \n",
    "    # Project data: X_new = X @ R.T\n",
    "    X_projected = X @ R.T\n",
    "    \n",
    "    return X_projected, R\n",
    "\n",
    "def verify_distance_preservation(X, X_projected, epsilon=0.1, n_pairs=100, seed=0):\n",
    "    \"\"\"\n",
    "    Verify that random projection preserves pairwise distances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array\n",
    "        Original data\n",
    "    X_projected : array\n",
    "        Projected data\n",
    "    epsilon : float\n",
    "        Expected distortion tolerance\n",
    "    n_pairs : int\n",
    "        Number of random pairs to check\n",
    "    seed : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Statistics on distance preservation\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Sample random pairs\n",
    "    pairs = rng.choice(n_samples, size=(n_pairs, 2), replace=True)\n",
    "    \n",
    "    ratios = []\n",
    "    for i, j in pairs:\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        # Original distance\n",
    "        dist_orig = np.linalg.norm(X[i] - X[j])\n",
    "        \n",
    "        # Projected distance\n",
    "        dist_proj = np.linalg.norm(X_projected[i] - X_projected[j])\n",
    "        \n",
    "        if dist_orig > 0:\n",
    "            ratio = dist_proj / dist_orig\n",
    "            ratios.append(ratio)\n",
    "    \n",
    "    ratios = np.array(ratios)\n",
    "    \n",
    "    # Check how many are within (1-epsilon, 1+epsilon)\n",
    "    within_bound = np.sum((ratios >= 1-epsilon) & (ratios <= 1+epsilon))\n",
    "    \n",
    "    return {\n",
    "        \"mean_ratio\": float(np.mean(ratios)),\n",
    "        \"std_ratio\": float(np.std(ratios)),\n",
    "        \"min_ratio\": float(np.min(ratios)),\n",
    "        \"max_ratio\": float(np.max(ratios)),\n",
    "        \"fraction_within_bound\": float(within_bound / len(ratios)),\n",
    "        \"expected_bound\": (1-epsilon, 1+epsilon)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "909d5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum dimension for 500 points (ε=0.1): 2763\n",
      "Original shape: (100, 200), Projected shape: (100, 50)\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for random projections (Johnson-Lindenstrauss)\n",
    "n_points = 500\n",
    "min_dim = johnson_lindenstrauss_min_dim(n_points, epsilon=0.1)\n",
    "print(f\"Minimum dimension for {n_points} points (ε=0.1): {min_dim}\")\n",
    "\n",
    "# Apply random projection\n",
    "X_high = np.random.randn(100, 200)  # 100 samples, 200 dimensions\n",
    "X_low, R = random_projection(X_high, k_target=50, seed=42)\n",
    "print(f\"Original shape: {X_high.shape}, Projected shape: {X_low.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813db43",
   "metadata": {},
   "source": [
    "# 7) Dimensionality Reduction & PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66230ad9",
   "metadata": {},
   "source": [
    "### 7.1 PCA using SVD\n",
    "\n",
    "**What is this?**\n",
    "Principal Component Analysis - finds the directions of maximum variance in your data.\n",
    "\n",
    "**How it works:**\n",
    "1. **Center the data**: Subtract mean from each feature\n",
    "2. **Apply SVD**: $X_c = U \\Sigma V^T$\n",
    "3. **Principal components**: Columns of $V$ (or rows of $V^T$)\n",
    "4. **Variance explained**: Proportional to squared singular values $\\sigma_i^2$\n",
    "\n",
    "**Key outputs:**\n",
    "- **mu**: Mean of original data (for centering)\n",
    "- **Vt**: Principal component directions (rows are PCs)\n",
    "- **s**: Singular values (related to variance)\n",
    "- **evr**: Explained variance ratio per component\n",
    "- **cum_evr**: Cumulative explained variance\n",
    "\n",
    "**Transform:** Project data onto first $k$ components: $Z = X_c V_k$\n",
    "**Inverse:** Reconstruct: $\\hat{X} = Z V_k^T + \\mu$\n",
    "\n",
    "**Use cases:** Dimensionality reduction, visualization, noise removal, feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ad4b6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_fit(X, center=True):\n",
    "    X = np.asarray(X, float)\n",
    "    mu = X.mean(axis=0) if center else np.zeros(X.shape[1])\n",
    "    Xc = X - mu\n",
    "    U, s, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    # explained variance ratio\n",
    "    var = s**2\n",
    "    evr = var / var.sum()\n",
    "    return {\"mu\": mu, \"U\": U, \"s\": s, \"Vt\": Vt, \"evr\": evr, \"cum_evr\": np.cumsum(evr)}\n",
    "\n",
    "def pca_transform(X, pca, k):\n",
    "    X = np.asarray(X, float)\n",
    "    Xc = X - pca[\"mu\"]\n",
    "    V = pca[\"Vt\"][:k].T\n",
    "    Z = Xc @ V\n",
    "    return Z\n",
    "\n",
    "def pca_inverse_transform(Z, pca, k):\n",
    "    Z = np.asarray(Z, float)\n",
    "    V = pca[\"Vt\"][:k].T\n",
    "    Xhat = Z @ V.T + pca[\"mu\"]\n",
    "    return Xhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "651da409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio (first 5 components): [0.10479217 0.08844768 0.08157502 0.07323637 0.06681447]\n",
      "Cumulative variance (first 5): [0.10479217 0.19323985 0.27481487 0.34805124 0.41486571]\n",
      "Transformed shape: (100, 3)\n",
      "Reconstructed shape: (100, 20)\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for PCA\n",
    "X_pca = np.random.randn(100, 20)\n",
    "pca_result = pca_fit(X_pca, center=True)\n",
    "print(f\"Explained variance ratio (first 5 components): {pca_result['evr'][:5]}\")\n",
    "print(f\"Cumulative variance (first 5): {pca_result['cum_evr'][:5]}\")\n",
    "\n",
    "# Transform to 3 components\n",
    "Z = pca_transform(X_pca, pca_result, k=3)\n",
    "print(f\"Transformed shape: {Z.shape}\")\n",
    "\n",
    "# Inverse transform (reconstruct)\n",
    "X_reconstructed = pca_inverse_transform(Z, pca_result, k=3)\n",
    "print(f\"Reconstructed shape: {X_reconstructed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0264b57",
   "metadata": {},
   "source": [
    "### 7.2 Choosing Number of Components (Keep X% Variance)\n",
    "\n",
    "**What is this?**\n",
    "Determines how many principal components to keep based on desired variance explained.\n",
    "\n",
    "**Algorithm:**\n",
    "Find the smallest $k$ such that cumulative variance ≥ target (e.g., 90%)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Components: [1,    2,    3,    4,    5]\n",
    "Cum. Var:   [0.5, 0.7, 0.85, 0.92, 0.95]\n",
    "Target = 0.90 → Choose k=4\n",
    "```\n",
    "\n",
    "**Common targets:**\n",
    "- **90%**: Good balance (retains most information, removes noise)\n",
    "- **95%**: More conservative (less information loss)\n",
    "- **99%**: Very conservative (mostly for compression)\n",
    "\n",
    "**Trade-off:** More components = More information but higher dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b64daca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_k(cum_evr, target=0.90):\n",
    "    cum_evr = np.asarray(cum_evr, float)\n",
    "    return int(np.searchsorted(cum_evr, target) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ea45d6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to keep 90% variance: 5\n"
     ]
    }
   ],
   "source": [
    "# Sample call for choose_k\n",
    "cum_var = np.array([0.4, 0.65, 0.80, 0.88, 0.93, 0.96, 0.98])\n",
    "k_90 = choose_k(cum_var, target=0.90)\n",
    "print(f\"Number of components to keep 90% variance: {k_90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad12e2",
   "metadata": {},
   "source": [
    "### 7.3 Scree Plot for Explained Variance\n",
    "\n",
    "**What is this?**\n",
    "A visualization showing variance explained by each principal component, helping decide how many components to keep.\n",
    "\n",
    "**How to read it:**\n",
    "- X-axis: Component number\n",
    "- Y-axis: Explained variance (or variance ratio)\n",
    "- Look for \"elbow\" where curve flattens\n",
    "\n",
    "**Elbow Method:**\n",
    "- Sharp drop initially = Components capture real structure\n",
    "- Flat tail = Components capture noise\n",
    "- Choose k at the elbow (where diminishing returns start)\n",
    "\n",
    "**Alternative: Cumulative plot:**\n",
    "Shows running total of variance explained\n",
    "- Find where curve reaches target (e.g., 90%, 95%)\n",
    "\n",
    "**Example interpretation:**\n",
    "```\n",
    "Components 1-3: Steep drop (keep these)\n",
    "Components 4+: Flat line (likely noise)\n",
    "→ Choose k=3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "abbe4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scree(pca_result, max_components=None):\n",
    "    \"\"\"\n",
    "    Create data for scree plot visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca_result : dict\n",
    "        Result from pca_fit function\n",
    "    max_components : int, optional\n",
    "        Maximum number of components to show\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    component_nums : array\n",
    "        Component numbers (1, 2, 3, ...)\n",
    "    variance_ratios : array\n",
    "        Explained variance ratio for each component\n",
    "    cumulative_variance : array\n",
    "        Cumulative explained variance\n",
    "    \"\"\"\n",
    "    evr = pca_result[\"evr\"]\n",
    "    cum_evr = pca_result[\"cum_evr\"]\n",
    "    \n",
    "    if max_components is not None:\n",
    "        evr = evr[:max_components]\n",
    "        cum_evr = cum_evr[:max_components]\n",
    "    \n",
    "    component_nums = np.arange(1, len(evr) + 1)\n",
    "    \n",
    "    return component_nums, evr, cum_evr\n",
    "\n",
    "def find_elbow_point(variance_ratios, method='max_curvature'):\n",
    "    \"\"\"\n",
    "    Automatically detect elbow point in scree plot.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    variance_ratios : array-like\n",
    "        Explained variance ratios\n",
    "    method : str, default='max_curvature'\n",
    "        Method to find elbow ('max_curvature' or 'threshold')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    elbow_idx : int\n",
    "        Index of elbow point (0-based)\n",
    "    \"\"\"\n",
    "    var = np.asarray(variance_ratios, float)\n",
    "    \n",
    "    if method == 'max_curvature':\n",
    "        # Find point with maximum curvature\n",
    "        # Approximate second derivative\n",
    "        if len(var) < 3:\n",
    "            return 0\n",
    "        \n",
    "        curvature = np.abs(np.diff(var, n=2))\n",
    "        elbow_idx = int(np.argmax(curvature))\n",
    "        return elbow_idx\n",
    "    \n",
    "    elif method == 'threshold':\n",
    "        # Find where variance drops below threshold\n",
    "        threshold = 0.05  # 5% variance\n",
    "        below_threshold = np.where(var < threshold)[0]\n",
    "        if len(below_threshold) > 0:\n",
    "            return int(below_threshold[0])\n",
    "        return len(var) - 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "715e64cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component numbers: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "Variance ratios: [0.16576588 0.15556487 0.13372965 0.10286273 0.09680176 0.08615833\n",
      " 0.07701844 0.07238031 0.06274974 0.0469683 ]\n",
      "Cumulative variance: [0.16576588 0.32133075 0.4550604  0.55792313 0.65472488 0.74088321\n",
      " 0.81790165 0.89028197 0.9530317  1.        ]\n",
      "Elbow point at component: 3\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for scree plot functions\n",
    "X_scree = np.random.randn(100, 10)\n",
    "pca_scree = pca_fit(X_scree)\n",
    "\n",
    "# Get scree plot data\n",
    "comp_nums, var_ratios, cum_var = plot_scree(pca_scree, max_components=10)\n",
    "print(f\"Component numbers: {comp_nums}\")\n",
    "print(f\"Variance ratios: {var_ratios}\")\n",
    "print(f\"Cumulative variance: {cum_var}\")\n",
    "\n",
    "# Find elbow point\n",
    "elbow_idx = find_elbow_point(var_ratios, method='max_curvature')\n",
    "print(f\"Elbow point at component: {elbow_idx + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74248b9",
   "metadata": {},
   "source": [
    "### 7.4 Reconstruction Error / Anomaly Detection\n",
    "\n",
    "**What is this?**\n",
    "Uses PCA reconstruction error to detect outliers/anomalies in data.\n",
    "\n",
    "**How it works:**\n",
    "1. **Fit PCA** with $k$ components (keeping most variance)\n",
    "2. **Project & reconstruct**: $\\hat{X} = \\text{PCA}^{-1}(\\text{PCA}(X))$\n",
    "3. **Compute error**: $\\text{error}_i = \\|X_i - \\hat{X}_i\\|$\n",
    "4. **Find anomalies**: Largest errors are outliers\n",
    "\n",
    "**Why it works:**\n",
    "- Normal data follows principal patterns (low reconstruction error)\n",
    "- Anomalies don't fit main patterns (high reconstruction error)\n",
    "- PCA captures \"normal\" structure, anomalies deviate\n",
    "\n",
    "**Use cases:**\n",
    "- Fraud detection\n",
    "- Manufacturing defect detection\n",
    "- Network intrusion detection\n",
    "- Data quality checks\n",
    "\n",
    "**Tip:** Use fewer components (smaller k) for more sensitive anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "91cff792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error(X, Xhat):\n",
    "    X = np.asarray(X, float)\n",
    "    Xhat = np.asarray(Xhat, float)\n",
    "    return np.linalg.norm(X - Xhat, axis=1)\n",
    "\n",
    "def top_anomalies(errors, k=10):\n",
    "    idx = np.argsort(errors)[::-1][:k]\n",
    "    return idx, errors[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0b02ae56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 anomalies (indices): [27  6 12  2 19]\n",
      "Their reconstruction errors: [3.57732622 3.53665016 3.47471192 3.14327573 3.11653902]\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for reconstruction error / anomaly detection\n",
    "X_anom = np.random.randn(50, 10)\n",
    "pca_anom = pca_fit(X_anom)\n",
    "Z_anom = pca_transform(X_anom, pca_anom, k=3)\n",
    "X_recon = pca_inverse_transform(Z_anom, pca_anom, k=3)\n",
    "\n",
    "errors = reconstruction_error(X_anom, X_recon)\n",
    "top_idx, top_errors = top_anomalies(errors, k=5)\n",
    "print(f\"Top 5 anomalies (indices): {top_idx}\")\n",
    "print(f\"Their reconstruction errors: {top_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7efa86e",
   "metadata": {},
   "source": [
    "### 7.5 PCA Standardization\n",
    "\n",
    "**What is this?**\n",
    "Scales features to have mean=0 and standard deviation=1 before PCA.\n",
    "\n",
    "**Why standardize?**\n",
    "- Features with large scales dominate PCA\n",
    "- Example: Feature A in [0,1000], Feature B in [0,1]\n",
    "  - Without standardization: PC1 ≈ Feature A direction\n",
    "  - With standardization: Equal importance to both\n",
    "\n",
    "**When to standardize:**\n",
    "- ✓ Features have different units (meters, dollars, age)\n",
    "- ✓ Features have very different scales\n",
    "- ✗ Features already on same scale (e.g., all pixel values 0-255)\n",
    "- ✗ Scale is meaningful for your problem\n",
    "\n",
    "**Formula:** $X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "**Best practice:** Fit standardization on training data, apply same transform to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "61b0d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_fit(X, eps=1e-12):\n",
    "    X = np.asarray(X, float)\n",
    "    mu = X.mean(axis=0)\n",
    "    sd = X.std(axis=0, ddof=0)\n",
    "    sd = np.maximum(sd, eps)\n",
    "    return mu, sd\n",
    "\n",
    "def standardize_apply(X, mu, sd):\n",
    "    X = np.asarray(X, float)\n",
    "    return (X - mu) / sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d1fd4a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[  1 100]\n",
      " [  2 200]\n",
      " [  3 300]\n",
      " [  4 400]]\n",
      "Standardized data:\n",
      "[[-1.34164079 -1.34164079]\n",
      " [-0.4472136  -0.4472136 ]\n",
      " [ 0.4472136   0.4472136 ]\n",
      " [ 1.34164079  1.34164079]]\n",
      "Standardized mean: [0. 0.]\n",
      "Standardized std: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for standardization\n",
    "X_std = np.array([[1, 100], [2, 200], [3, 300], [4, 400]])\n",
    "mu_std, sd_std = standardize_fit(X_std)\n",
    "X_standardized = standardize_apply(X_std, mu_std, sd_std)\n",
    "print(f\"Original data:\\n{X_std}\")\n",
    "print(f\"Standardized data:\\n{X_standardized}\")\n",
    "print(f\"Standardized mean: {np.mean(X_standardized, axis=0)}\")\n",
    "print(f\"Standardized std: {np.std(X_standardized, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1299d0",
   "metadata": {},
   "source": [
    "### 7.6 Whitening Transformation\n",
    "\n",
    "**What is this?**\n",
    "Transforms data so features are uncorrelated AND have unit variance (stricter than standardization).\n",
    "\n",
    "**Formula:** $X_{\\text{whitened}} = X_c \\Sigma^{-1/2} V^T$\n",
    "\n",
    "Where:\n",
    "- $X_c$ = Centered data\n",
    "- $\\Sigma$ = Diagonal matrix of singular values\n",
    "- $V$ = PCA component directions\n",
    "\n",
    "**Effect:**\n",
    "- Removes correlation between features\n",
    "- Makes all features have variance = 1\n",
    "- Spheres the data distribution\n",
    "\n",
    "**When to use:**\n",
    "- Before ICA (Independent Component Analysis)\n",
    "- Some neural networks benefit\n",
    "- When you want truly independent features\n",
    "\n",
    "**Difference from standardization:**\n",
    "- Standardization: Per-feature scaling (doesn't remove correlation)\n",
    "- Whitening: Decorrelates AND scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "24de0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_whiten(X, pca=None, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Apply whitening transformation using PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Data to whiten\n",
    "    pca : dict, optional\n",
    "        Pre-fitted PCA result. If None, fits PCA on X\n",
    "    eps : float, default=1e-5\n",
    "        Small constant to prevent division by zero\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_whitened : array\n",
    "        Whitened data\n",
    "    pca_result : dict\n",
    "        PCA parameters (for inverse transform)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    \n",
    "    # Fit PCA if not provided\n",
    "    if pca is None:\n",
    "        pca = pca_fit(X, center=True)\n",
    "    \n",
    "    # Center data\n",
    "    X_centered = X - pca[\"mu\"]\n",
    "    \n",
    "    # Whiten: X_white = X_c @ V @ Sigma^(-1)\n",
    "    # This decorrelates and scales to unit variance\n",
    "    V = pca[\"Vt\"].T  # Principal components as columns\n",
    "    s = pca[\"s\"]\n",
    "    \n",
    "    # Prevent division by very small singular values\n",
    "    s_inv = 1.0 / np.maximum(s, eps)\n",
    "    \n",
    "    X_whitened = X_centered @ V @ np.diag(s_inv)\n",
    "    \n",
    "    return X_whitened, pca\n",
    "\n",
    "def pca_unwhiten(X_whitened, pca):\n",
    "    \"\"\"\n",
    "    Inverse whitening transformation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_whitened : array-like\n",
    "        Whitened data\n",
    "    pca : dict\n",
    "        PCA parameters from pca_whiten\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : array\n",
    "        Original-scale data\n",
    "    \"\"\"\n",
    "    X_whitened = np.asarray(X_whitened, float)\n",
    "    \n",
    "    V = pca[\"Vt\"].T\n",
    "    s = pca[\"s\"]\n",
    "    \n",
    "    # Reverse transformation\n",
    "    X_centered = X_whitened @ np.diag(s) @ V.T\n",
    "    X = X_centered + pca[\"mu\"]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c2d5fb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (100, 2)\n",
      "Whitened shape: (100, 2)\n",
      "Whitened covariance (should be ~identity):\n",
      "[[1.01010101e-02 4.25892897e-19]\n",
      " [4.25892897e-19 1.01010101e-02]]\n",
      "Reconstruction error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Sample calls for whitening\n",
    "X_whiten = np.random.randn(100, 5) @ np.array([[2, 0.5], [0.5, 1], [0, 0], [0, 0], [0, 0]])\n",
    "X_whitened, pca_white = pca_whiten(X_whiten)\n",
    "print(f\"Original shape: {X_whiten.shape}\")\n",
    "print(f\"Whitened shape: {X_whitened.shape}\")\n",
    "print(f\"Whitened covariance (should be ~identity):\\n{np.cov(X_whitened.T)}\")\n",
    "\n",
    "# Unwhiten\n",
    "X_unwhitened = pca_unwhiten(X_whitened, pca_white)\n",
    "print(f\"Reconstruction error: {np.max(np.abs(X_whiten - X_unwhitened)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4586890",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b13b1d41",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
