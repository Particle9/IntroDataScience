{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c41a18",
   "metadata": {},
   "source": [
    "## 1) Imports\n",
    "\n",
    "**What is this section for?**\n",
    "\n",
    "This section imports all the essential libraries and modules from scikit-learn (sklearn) that you'll need for machine learning tasks. Think of it as gathering all your tools before starting a project.\n",
    "\n",
    "**Key Components:**\n",
    "- **numpy & pandas**: For data manipulation and numerical operations\n",
    "- **model_selection**: Tools for splitting data and evaluating models\n",
    "- **Pipeline & ColumnTransformer**: For creating reproducible preprocessing workflows\n",
    "- **preprocessing**: For transforming features (scaling, encoding, etc.)\n",
    "- **metrics**: For evaluating model performance\n",
    "\n",
    "**Why import everything at the start?**\n",
    "By importing all modules upfront, you ensure all dependencies are available and can quickly identify any missing packages before running your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1524ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, KFold, StratifiedKFold, GridSearchCV, cross_val_score, cross_val_predict\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, OneHotEncoder, PolynomialFeatures\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e27f732",
   "metadata": {},
   "source": [
    "## 2) Universal Split\n",
    "\n",
    "**Purpose:** Splitting your data into training and test sets is crucial for evaluating how well your model generalizes to unseen data.\n",
    "\n",
    "**Why split data?**\n",
    "- **Training set**: Used to train/fit the model\n",
    "- **Test set**: Used to evaluate model performance on unseen data\n",
    "- Prevents overfitting and gives a realistic estimate of model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6cd07",
   "metadata": {},
   "source": [
    "### 2.1 Basic Split\n",
    "\n",
    "**What it does:** Randomly splits your data into training (80%) and test (20%) sets.\n",
    "\n",
    "**Parameters explained:**\n",
    "- `test_size=0.2`: 20% of data goes to test set, 80% to training\n",
    "- `random_state=0`: Sets a seed for reproducibility (same split every time)\n",
    "- `shuffle=True`: Randomly shuffles data before splitting (recommended)\n",
    "\n",
    "**When to use:** Most general-purpose machine learning tasks where class balance isn't a major concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b072652",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab0ec4",
   "metadata": {},
   "source": [
    "### 2.2 Stratified Split\n",
    "\n",
    "**For Imbalanced Classes**\n",
    "\n",
    "**What it does:** Ensures that the proportion of each class is maintained in both training and test sets.\n",
    "\n",
    "**Parameters explained:**\n",
    "- `stratify=y`: Maintains the same class distribution in train and test sets\n",
    "\n",
    "**Example:** If your dataset has 70% class 0 and 30% class 1, stratified split ensures both train and test sets have approximately the same 70-30 ratio.\n",
    "\n",
    "**When to use:** \n",
    "- Classification problems with imbalanced classes\n",
    "- When you want to ensure representative sampling of minority classes\n",
    "- Prevents situations where rare classes might be missing from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, shuffle=True, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d201d4",
   "metadata": {},
   "source": [
    "### 2.3 Train Test Calibration Split\n",
    "\n",
    "**What it does:** Creates THREE sets instead of two:\n",
    "- **Training set (60%)**: For training the model\n",
    "- **Calibration set (20%)**: For calibrating predicted probabilities\n",
    "- **Test set (20%)**: For final evaluation\n",
    "\n",
    "**Why calibration?** \n",
    "Some models (like SVM) produce scores that aren't well-calibrated probabilities. A calibration set helps adjust these scores to reflect true probabilities.\n",
    "\n",
    "**When to use:**\n",
    "- When you need reliable probability estimates (e.g., risk assessment)\n",
    "- When working with models that output uncalibrated scores\n",
    "- When threshold selection is important for decision-making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "616333b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bn/w4ms4n5n7l3czmht6p6ntpjm0000gn/T/ipykernel_10945/2850786260.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_cal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_tr, X_tmp, y_tr, y_tmp = train_test_split(X, y, test_size=0.4, random_state=0, stratify=y)\n",
    "X_cal, X_te, y_cal, y_te = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=0, stratify=y_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c24aa",
   "metadata": {},
   "source": [
    "## 3) Data Handling & Encoding\n",
    "\n",
    "**What is this section for?**\n",
    "\n",
    "This section covers essential data preprocessing techniques for handling different types of features in your dataset. Before feeding data into machine learning models, you often need to transform categorical variables, cyclical features, and handle various data types appropriately.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Categorical Encoding**: Converting text/category labels into numbers\n",
    "- **One-Hot Encoding**: Creating binary columns for each category\n",
    "- **Ordinal Encoding**: Assigning numeric order to categories\n",
    "- **Cyclical Encoding**: Handling circular features like time/angles\n",
    "- **Target Encoding**: Using target statistics for encoding\n",
    "\n",
    "**Why is encoding important?**\n",
    "Most machine learning algorithms work only with numerical data. Proper encoding ensures that categorical information is preserved while making it compatible with ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854e6c6",
   "metadata": {},
   "source": [
    "### 3.1 One-Hot Encoding\n",
    "\n",
    "**What it does:** Creates binary (0/1) columns for each unique category in a categorical variable. Each row gets a 1 in the column corresponding to its category, and 0 in all others.\n",
    "\n",
    "**Example:** \n",
    "- Color: ['red', 'blue', 'green'] becomes:\n",
    "  - `Color_red`: [1, 0, 0]\n",
    "  - `Color_blue`: [0, 1, 0]\n",
    "  - `Color_green`: [0, 0, 1]\n",
    "\n",
    "**Parameters:**\n",
    "- `drop='first'`: Removes one category to avoid multicollinearity (recommended for linear models)\n",
    "- `handle_unknown='ignore'`: Ignores unseen categories in test data\n",
    "- `sparse_output=False`: Returns dense array instead of sparse matrix\n",
    "\n",
    "**When to use:**\n",
    "- Nominal categorical features (no inherent order)\n",
    "- When you have relatively few unique categories (< 10-15)\n",
    "- Tree-based models (don't need to drop first)\n",
    "\n",
    "**Advantages:** No ordinal relationship assumed\n",
    "**Disadvantages:** Creates many columns with high cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-Hot Encoding in a pipeline\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Full preprocessing with OneHotEncoder\n",
    "pre = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
    "])\n",
    "\n",
    "# Usage\n",
    "pre.fit(X_train)\n",
    "X_train_processed = pre.transform(X_train)\n",
    "X_test_processed = pre.transform(X_test)\n",
    "\n",
    "# Or directly without pipeline\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X_train[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4f5ed",
   "metadata": {},
   "source": [
    "### 3.2 Ordinal Encoding\n",
    "\n",
    "**What it does:** Maps categories to integers while preserving order/rank. Useful when categories have a natural ordering.\n",
    "\n",
    "**Example:**\n",
    "- Education: ['High School', 'Bachelor', 'Master', 'PhD'] → [0, 1, 2, 3]\n",
    "- Size: ['Small', 'Medium', 'Large'] → [0, 1, 2]\n",
    "\n",
    "**Parameters:**\n",
    "- `categories`: List of lists defining the order for each feature\n",
    "- `handle_unknown='use_encoded_value'`: Handle unseen categories\n",
    "- `unknown_value`: Value to use for unknown categories (e.g., -1)\n",
    "\n",
    "**When to use:**\n",
    "- Ordinal categorical features (clear ordering)\n",
    "- Education level, satisfaction ratings, size categories\n",
    "- When the numeric order matters\n",
    "\n",
    "**Advantages:** \n",
    "- Single column output (memory efficient)\n",
    "- Preserves ordinal relationships\n",
    "\n",
    "**Disadvantages:** \n",
    "- Assumes equal spacing between categories\n",
    "- Can mislead models into thinking numeric distance is meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9334fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define category order\n",
    "education_order = [['High School', 'Bachelor', 'Master', 'PhD']]\n",
    "size_order = [['Small', 'Medium', 'Large']]\n",
    "\n",
    "# Ordinal encoding in pipeline\n",
    "ordinal_pipe = Pipeline(steps=[\n",
    "    (\"ordinal\", OrdinalEncoder(\n",
    "        categories=education_order,\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Multiple ordinal features with ColumnTransformer\n",
    "pre = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), numeric_cols),\n",
    "    (\"ordinal_edu\", OrdinalEncoder(categories=education_order), ['education']),\n",
    "    (\"ordinal_size\", OrdinalEncoder(categories=size_order), ['size']),\n",
    "    (\"cat\", OneHotEncoder(drop='first'), other_categorical_cols)\n",
    "])\n",
    "\n",
    "# Direct usage\n",
    "encoder = OrdinalEncoder(categories=education_order)\n",
    "X_encoded = encoder.fit_transform(X_train[['education']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cf7aa",
   "metadata": {},
   "source": [
    "### 3.3 Label Encoding\n",
    "\n",
    "**What it does:** Simple integer encoding for categorical variables. Maps each unique category to an integer (0, 1, 2, ...).\n",
    "\n",
    "**Example:**\n",
    "- Color: ['red', 'blue', 'green', 'red'] → [0, 1, 2, 0]\n",
    "\n",
    "**When to use:**\n",
    "- **Primary use**: Encoding the target variable (y) for classification\n",
    "- Tree-based models can handle label-encoded features\n",
    "- When you need simple integer mapping without creating multiple columns\n",
    "\n",
    "**Important Note:** \n",
    "- For features, prefer OneHotEncoder or OrdinalEncoder depending on whether there's an order\n",
    "- LabelEncoder doesn't work with pipelines (no transform method for new data)\n",
    "- LabelEncoder is mainly for target variables\n",
    "\n",
    "**Advantages:** Simple, memory efficient, good for targets\n",
    "**Disadvantages:** Implies ordinal relationship, not pipeline-compatible for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686289f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Primarily for target variable encoding\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)  # ['cat', 'dog', 'cat'] → [0, 1, 0]\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Get original labels back\n",
    "y_original = le.inverse_transform(y_train_encoded)\n",
    "\n",
    "# See the mapping\n",
    "print(le.classes_)  # ['cat', 'dog']\n",
    "\n",
    "# For features (not recommended - use OrdinalEncoder instead)\n",
    "# LabelEncoder can be used but doesn't work well in pipelines\n",
    "le_feature = LabelEncoder()\n",
    "X_train['color_encoded'] = le_feature.fit_transform(X_train['color'])\n",
    "X_test['color_encoded'] = le_feature.transform(X_test['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc71ad",
   "metadata": {},
   "source": [
    "### 3.4 Cyclical Encoding (Sine-Cosine Transform)\n",
    "\n",
    "**What it does:** Transforms cyclical/circular features (like time, angles, days) into two continuous features using sine and cosine functions. This preserves the circular nature where the maximum value is \"close\" to the minimum.\n",
    "\n",
    "**Why is it needed?**\n",
    "- Regular encoding treats 23:59 and 00:01 as far apart, but they're actually 2 minutes apart\n",
    "- Hour 0 and Hour 23 should be close, not distant\n",
    "- Months: December (12) and January (1) are consecutive\n",
    "\n",
    "**Example - Hours (0-23):**\n",
    "- Hour 0 → sin=0, cos=1\n",
    "- Hour 6 → sin=1, cos=0\n",
    "- Hour 12 → sin=0, cos=-1\n",
    "- Hour 18 → sin=-1, cos=0\n",
    "\n",
    "**When to use:**\n",
    "- Time features: hours, minutes, seconds\n",
    "- Calendar features: day of week, day of month, month\n",
    "- Angular measurements: compass directions\n",
    "- Any feature that \"wraps around\"\n",
    "\n",
    "**Formula:**\n",
    "- `sin_feature = sin(2π × value / max_value)`\n",
    "- `cos_feature = cos(2π × value / max_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecad954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create cyclical encoding function\n",
    "def cyclical_encoding(X, period):\n",
    "    \"\"\"\n",
    "    Encode cyclical features using sin/cos transformation\n",
    "    X: array-like, shape (n_samples, 1) - the cyclical feature\n",
    "    period: int - the period of the cycle (e.g., 24 for hours, 7 for days, 12 for months)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X).reshape(-1, 1)\n",
    "    sin_encoded = np.sin(2 * np.pi * X / period)\n",
    "    cos_encoded = np.cos(2 * np.pi * X / period)\n",
    "    return np.concatenate([sin_encoded, cos_encoded], axis=1)\n",
    "\n",
    "# Example: Encode hours (0-23)\n",
    "X_train['hour_sin'] = np.sin(2 * np.pi * X_train['hour'] / 24)\n",
    "X_train['hour_cos'] = np.cos(2 * np.pi * X_train['hour'] / 24)\n",
    "\n",
    "# Example: Encode day of week (0-6)\n",
    "X_train['day_sin'] = np.sin(2 * np.pi * X_train['day_of_week'] / 7)\n",
    "X_train['day_cos'] = np.cos(2 * np.pi * X_train['day_of_week'] / 7)\n",
    "\n",
    "# Example: Encode month (1-12)\n",
    "X_train['month_sin'] = np.sin(2 * np.pi * X_train['month'] / 12)\n",
    "X_train['month_cos'] = np.cos(2 * np.pi * X_train['month'] / 12)\n",
    "\n",
    "# Using FunctionTransformer in a pipeline\n",
    "hour_transformer = FunctionTransformer(lambda x: cyclical_encoding(x, period=24))\n",
    "\n",
    "pre = ColumnTransformer(transformers=[\n",
    "    (\"hour_cyclic\", hour_transformer, ['hour']),\n",
    "    (\"num\", StandardScaler(), other_numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(drop='first'), categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ac865",
   "metadata": {},
   "source": [
    "### 3.5 Target Encoding (Mean Encoding)\n",
    "\n",
    "**What it does:** Replaces each category with the mean of the target variable for that category. This creates a numeric representation that directly correlates with the target.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "City        | Target | → City_encoded\n",
    "------------|--------|----------------\n",
    "New York    | 1      | 0.75\n",
    "Los Angeles | 0      | 0.75\n",
    "New York    | 1      | 0.75\n",
    "Chicago     | 0      | 0.33\n",
    "New York    | 0      | 0.75\n",
    "Chicago     | 1      | 0.33\n",
    "```\n",
    "New York average: (1+1+0)/3 = 0.67, etc.\n",
    "\n",
    "**Advantages:**\n",
    "- Handles high cardinality features well (many unique categories)\n",
    "- Captures relationship with target\n",
    "- Single column output\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Risk of overfitting/data leakage** - must use cross-validation\n",
    "- Requires target variable (can't use on test set directly)\n",
    "- Can overfit on rare categories\n",
    "\n",
    "**When to use:**\n",
    "- High cardinality categorical features (e.g., city names, zip codes)\n",
    "- When One-Hot would create too many columns\n",
    "- Tree-based models (gradient boosting, random forests)\n",
    "\n",
    "**Important:** Always use cross-validation or holdout approach to prevent data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44cf06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding using category_encoders library\n",
    "# Install: pip install category-encoders\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Target encoding in pipeline (handles data leakage with smoothing)\n",
    "target_enc = TargetEncoder(cols=['city', 'category'], smoothing=1.0)\n",
    "\n",
    "# Fit on training data with target\n",
    "target_enc.fit(X_train[['city', 'category']], y_train)\n",
    "\n",
    "# Transform both train and test\n",
    "X_train_encoded = target_enc.transform(X_train[['city', 'category']])\n",
    "X_test_encoded = target_enc.transform(X_test[['city', 'category']])\n",
    "\n",
    "# Manual target encoding with cross-validation to prevent overfitting\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def target_encode_cv(X_train, y_train, X_test, col, n_splits=5):\n",
    "    \"\"\"Target encode with cross-validation to prevent data leakage\"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    \n",
    "    X_train_encoded = np.zeros(len(X_train))\n",
    "    # Cross-validation encoding for train\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        target_mean = X_train.iloc[train_idx].groupby(col)[y_train.iloc[train_idx]].mean()\n",
    "        X_train_encoded[val_idx] = X_train.iloc[val_idx][col].map(target_mean)\n",
    "    \n",
    "    # Global mean for test set\n",
    "    target_mean_global = X_train.groupby(col)[y_train].mean()\n",
    "    X_test_encoded = X_test[col].map(target_mean_global)\n",
    "    \n",
    "    # Fill unknown categories with overall mean\n",
    "    X_test_encoded = X_test_encoded.fillna(y_train.mean())\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "# Usage\n",
    "X_train['city_encoded'], X_test['city_encoded'] = target_encode_cv(\n",
    "    X_train, y_train, X_test, col='city'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb19ea",
   "metadata": {},
   "source": [
    "### 3.6 Frequency Encoding\n",
    "\n",
    "**What it does:** Replaces each category with its frequency (count) or proportion in the dataset.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Color   | Count | → Frequency | → Proportion\n",
    "--------|-------|-------------|-------------\n",
    "Red     | 3     | 3           | 0.50\n",
    "Blue    | 2     | 2           | 0.33\n",
    "Red     | 3     | 3           | 0.50\n",
    "Green   | 1     | 1           | 0.17\n",
    "Red     | 3     | 3           | 0.50\n",
    "Blue    | 2     | 2           | 0.33\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- When the frequency of a category is informative\n",
    "- High cardinality categorical features\n",
    "- As a simple alternative to one-hot encoding\n",
    "- Complement to other encoding methods\n",
    "\n",
    "**Advantages:**\n",
    "- Simple and fast\n",
    "- Handles high cardinality\n",
    "- No data leakage risk\n",
    "- Single column output\n",
    "\n",
    "**Disadvantages:**\n",
    "- Different categories with same frequency get same encoding\n",
    "- Doesn't capture relationship with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158debec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding - manual implementation\n",
    "def frequency_encoding(X_train, X_test, col):\n",
    "    \"\"\"Encode categorical feature by its frequency\"\"\"\n",
    "    # Calculate frequencies on training data\n",
    "    freq_map = X_train[col].value_counts().to_dict()\n",
    "    \n",
    "    # Apply to train and test\n",
    "    X_train_encoded = X_train[col].map(freq_map)\n",
    "    X_test_encoded = X_test[col].map(freq_map)\n",
    "    \n",
    "    # Handle unseen categories in test (assign 0 or min frequency)\n",
    "    X_test_encoded = X_test_encoded.fillna(0)\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "# Usage\n",
    "X_train['color_freq'], X_test['color_freq'] = frequency_encoding(\n",
    "    X_train, X_test, col='color'\n",
    ")\n",
    "\n",
    "# Proportion encoding (frequency / total count)\n",
    "def proportion_encoding(X_train, X_test, col):\n",
    "    \"\"\"Encode categorical feature by its proportion\"\"\"\n",
    "    freq_map = X_train[col].value_counts(normalize=True).to_dict()\n",
    "    \n",
    "    X_train_encoded = X_train[col].map(freq_map)\n",
    "    X_test_encoded = X_test[col].map(freq_map).fillna(0)\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "# Usage\n",
    "X_train['color_prop'], X_test['color_prop'] = proportion_encoding(\n",
    "    X_train, X_test, col='color'\n",
    ")\n",
    "\n",
    "# Using category_encoders library\n",
    "# pip install category-encoders\n",
    "from category_encoders import CountEncoder\n",
    "\n",
    "count_enc = CountEncoder(cols=['color', 'brand'])\n",
    "count_enc.fit(X_train)\n",
    "X_train_encoded = count_enc.transform(X_train)\n",
    "X_test_encoded = count_enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f145331",
   "metadata": {},
   "source": [
    "### 3.7 Binary Encoding\n",
    "\n",
    "**What it does:** Converts categories to binary code (like computer binary). Each category gets a unique binary representation, then each bit becomes a separate feature.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Category | Integer | Binary  | → Bit_0 | Bit_1 | Bit_2\n",
    "---------|---------|---------|---------|-------|-------\n",
    "Red      | 0       | 000     | 0       | 0     | 0\n",
    "Blue     | 1       | 001     | 0       | 0     | 1\n",
    "Green    | 2       | 010     | 0       | 1     | 0\n",
    "Yellow   | 3       | 011     | 0       | 1     | 1\n",
    "Orange   | 4       | 100     | 1       | 0     | 0\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- High cardinality categorical features (100+ categories)\n",
    "- When one-hot encoding creates too many columns\n",
    "- Middle ground between one-hot and label encoding\n",
    "- Tree-based models\n",
    "\n",
    "**Advantages:**\n",
    "- Fewer columns than one-hot: log₂(n) columns instead of n\n",
    "- Handles high cardinality better than one-hot\n",
    "- Better than label encoding (doesn't imply order)\n",
    "\n",
    "**Disadvantages:**\n",
    "- Less interpretable than one-hot\n",
    "- Binary bits may not be meaningful to linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding using category_encoders\n",
    "# Install: pip install category-encoders\n",
    "\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "# Binary encoding\n",
    "binary_enc = BinaryEncoder(cols=['city', 'product_id'])\n",
    "binary_enc.fit(X_train)\n",
    "\n",
    "X_train_encoded = binary_enc.transform(X_train)\n",
    "X_test_encoded = binary_enc.transform(X_test)\n",
    "\n",
    "# Use in pipeline with ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "pre = make_column_transformer(\n",
    "    (StandardScaler(), numeric_cols),\n",
    "    (BinaryEncoder(), ['city', 'product_id']),\n",
    "    (OneHotEncoder(drop='first'), low_cardinality_cols)\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c0b31",
   "metadata": {},
   "source": [
    "### 3.8 Text Preprocessing with TF-IDF\n",
    "\n",
    "**What is TF-IDF?**\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) converts text into numerical features by weighing how important each word is:\n",
    "- **TF (Term Frequency)**: How often a word appears in a document\n",
    "- **IDF (Inverse Document Frequency)**: How unique/rare a word is across all documents\n",
    "- Words common everywhere get lower scores; rare discriminative words get higher scores\n",
    "\n",
    "**Why use TF-IDF?**\n",
    "- Transforms text into numbers that machine learning models can use\n",
    "- Automatically identifies important words\n",
    "- Reduces impact of very common words\n",
    "- Works well for text classification tasks\n",
    "\n",
    "**Parameters:**\n",
    "- `lowercase=True`: Convert all text to lowercase\n",
    "- `max_features`: Limit number of features (e.g., 5000 most important words)\n",
    "- `ngram_range=(1,1)`: Use single words; (1,2) includes word pairs\n",
    "- `min_df`: Minimum document frequency (ignore rare words)\n",
    "- `max_df`: Maximum document frequency (ignore very common words)\n",
    "- `stop_words='english'`: Remove common words like \"the\", \"is\", \"and\"\n",
    "\n",
    "**When to use:**\n",
    "- Text classification (spam detection, sentiment analysis)\n",
    "- Document categorization\n",
    "- Any NLP task requiring text vectorization\n",
    "\n",
    "**Common use cases:**\n",
    "- Email/message classification\n",
    "- Product review sentiment analysis\n",
    "- News article categorization\n",
    "- Search relevance ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Basic TF-IDF for a single text column\n",
    "tfidf = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_features=5000,        # Keep top 5000 features\n",
    "    ngram_range=(1, 2),       # Use unigrams and bigrams\n",
    "    min_df=2,                 # Ignore terms appearing in < 2 documents\n",
    "    max_df=0.95,              # Ignore terms appearing in > 95% of documents\n",
    "    stop_words='english'      # Remove common English words\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['text'].fillna(\"\"))\n",
    "X_test_tfidf = tfidf.transform(X_test['text'].fillna(\"\"))\n",
    "\n",
    "# Use in a pipeline with a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )),\n",
    "    ('clf', LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "# Fit and predict (fillna important for missing text)\n",
    "text_clf.fit(X_train['text'].fillna(\"\"), y_train)\n",
    "predictions = text_clf.predict(X_test['text'].fillna(\"\"))\n",
    "probabilities = text_clf.predict_proba(X_test['text'].fillna(\"\"))[:, 1]\n",
    "\n",
    "# Multiple text columns with ColumnTransformer\n",
    "text_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('title', TfidfVectorizer(max_features=3000), 'title'),\n",
    "        ('body', TfidfVectorizer(max_features=7000), 'body')\n",
    "    ]\n",
    ")\n",
    "\n",
    "multi_text_clf = Pipeline([\n",
    "    ('text_features', text_transformer),\n",
    "    ('clf', LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "# Prepare data (fill NaN for each text column)\n",
    "X_train_clean = X_train.copy()\n",
    "X_test_clean = X_test.copy()\n",
    "for col in ['title', 'body']:\n",
    "    X_train_clean[col] = X_train_clean[col].fillna(\"\")\n",
    "    X_test_clean[col] = X_test_clean[col].fillna(\"\")\n",
    "\n",
    "multi_text_clf.fit(X_train_clean, y_train)\n",
    "predictions = multi_text_clf.predict(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d98d3",
   "metadata": {},
   "source": [
    "## 4) Pipelining\n",
    "\n",
    "**What is a Pipeline?**\n",
    "\n",
    "A Pipeline is a powerful tool that chains together multiple preprocessing steps and a final model into a single object. This ensures:\n",
    "- **Reproducibility**: Same preprocessing applied consistently\n",
    "- **No data leakage**: Test data is never seen during fit\n",
    "- **Clean code**: All transformations in one place\n",
    "- **Easy deployment**: One object to save and load\n",
    "\n",
    "**Key concepts:**\n",
    "- **ColumnTransformer**: Applies different transformations to different columns\n",
    "- **Pipeline**: Chains multiple steps sequentially\n",
    "- Each step transforms the data before passing to the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cad57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a preprocessing pipeline for mixed data types\n",
    "def make_preprocess(numeric_cols, categorical_cols):\n",
    "    # Numeric pipeline: handle missing values → scale features\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),  # Fill missing with median\n",
    "        (\"scaler\", StandardScaler())                    # Standardize (mean=0, std=1)\n",
    "    ])\n",
    "\n",
    "    # Categorical pipeline: handle missing values → one-hot encode\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Fill with mode\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))     # Convert to binary columns\n",
    "    ])\n",
    "\n",
    "    # Combine both pipelines\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, numeric_cols),   # Apply num_pipe to numeric columns\n",
    "            (\"cat\", cat_pipe, categorical_cols),  # Apply cat_pipe to categorical columns\n",
    "        ],\n",
    "        remainder=\"drop\"  # Drop any columns not specified\n",
    "    )\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a990eb",
   "metadata": {},
   "source": [
    "### 4.1 Single text column → Classifier (Using TF-IDF)\n",
    "\n",
    "**What is TF-IDF?**\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) converts text into numerical features by:\n",
    "- **TF**: How often a word appears in a document\n",
    "- **IDF**: How unique/rare a word is across all documents\n",
    "- Words that are common everywhere get lower scores\n",
    "\n",
    "**Parameters explained:**\n",
    "- `lowercase=True`: Convert all text to lowercase\n",
    "- `stop_words=None`: Keep all words (or use \"english\" to remove common words like \"the\", \"is\")\n",
    "- `ngram_range=(1,2)`: Use individual words (unigrams) and word pairs (bigrams)\n",
    "- `min_df=2`: Ignore words that appear in fewer than 2 documents\n",
    "- `max_df=0.95`: Ignore words that appear in more than 95% of documents\n",
    "- `max_features=200000`: Keep only top 200,000 most important features\n",
    "\n",
    "**Use case:** Text classification (spam detection, sentiment analysis, document categorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6465120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "TEXT_COL = \"text\"  # change\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=None,          # or \"english\" if allowed/desired\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        max_features=200000\n",
    "    )),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "# If X is a DataFrame:\n",
    "clf.fit(X_train[TEXT_COL].fillna(\"\"), y_train)\n",
    "\n",
    "proba = clf.predict_proba(X_test[TEXT_COL].fillna(\"\"))[:, 1]\n",
    "pred  = (proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab1712",
   "metadata": {},
   "source": [
    "### 4.2 Multiple Text Fields\n",
    "\n",
    "**When to use:** Your dataset has multiple text columns (e.g., email subject + body, product title + description).\n",
    "\n",
    "**How it works:**\n",
    "- Each text field gets its own TF-IDF vectorizer\n",
    "- Features from all fields are concatenated\n",
    "- This allows the model to learn from different text sources independently\n",
    "\n",
    "**Important:** Always fill NaN values with empty strings before processing text, as TF-IDF cannot handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "TEXT_COLS = [\"subject\", \"body\"]  # change\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"subj\", TfidfVectorizer(ngram_range=(1,2), min_df=2), \"subject\"),\n",
    "        (\"body\", TfidfVectorizer(ngram_range=(1,2), min_df=2), \"body\"),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "# Important: fill NaNs per column\n",
    "Xtr = X_train.copy()\n",
    "Xte = X_test.copy()\n",
    "for c in TEXT_COLS:\n",
    "    Xtr[c] = Xtr[c].fillna(\"\")\n",
    "    Xte[c] = Xte[c].fillna(\"\")\n",
    "\n",
    "model.fit(Xtr, y_train)\n",
    "proba = model.predict_proba(Xte)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4917a8",
   "metadata": {},
   "source": [
    "### 4.3 Mixed Text, Numeric, Categorical\n",
    "\n",
    "**Most realistic scenario:** Your dataset contains different types of features:\n",
    "- **Text features**: Free-form text (reviews, descriptions, comments)\n",
    "- **Numeric features**: Numbers (age, price, counts)\n",
    "- **Categorical features**: Categories (country, device type, status)\n",
    "\n",
    "**Why different processing?**\n",
    "- Text needs TF-IDF vectorization\n",
    "- Numbers need imputation and scaling\n",
    "- Categories need one-hot encoding\n",
    "\n",
    "**The pipeline handles all three types simultaneously**, ensuring proper preprocessing for each feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "NUM_COLS = [\"len\", \"num_links\"]      # change / can be []\n",
    "CAT_COLS = [\"country\", \"device\"]     # change / can be []\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"text\", TfidfVectorizer(ngram_range=(1,2), min_df=2), TEXT_COL),\n",
    "    (\"num\", num_pipe, NUM_COLS),\n",
    "    (\"cat\", cat_pipe, CAT_COLS),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "Xtr = X_train.copy()\n",
    "Xte = X_test.copy()\n",
    "Xtr[TEXT_COL] = Xtr[TEXT_COL].fillna(\"\")\n",
    "Xte[TEXT_COL] = Xte[TEXT_COL].fillna(\"\")\n",
    "\n",
    "pipe.fit(Xtr, y_train)\n",
    "proba = pipe.predict_proba(Xte)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f688762",
   "metadata": {},
   "source": [
    "### 3.4 Grid Search Template for Pipeline\n",
    "\n",
    "**What is Grid Search?**\n",
    "Grid Search automatically tests multiple combinations of hyperparameters to find the best model configuration.\n",
    "\n",
    "**How to specify parameters:**\n",
    "Use double underscore notation: `\"step_name__parameter_name\"`\n",
    "- Example: `\"tfidf__ngram_range\"` refers to the `ngram_range` parameter of the `tfidf` step\n",
    "\n",
    "**How it works:**\n",
    "1. Tests all combinations of parameters\n",
    "2. Uses cross-validation to evaluate each combination\n",
    "3. Returns the best configuration\n",
    "\n",
    "**Tip:** Use `n_jobs=-1` to utilize all CPU cores for faster computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5993024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__min_df\": [1, 2, 5],\n",
    "    \"model__C\": [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "grid.fit(X_train[TEXT_COL].fillna(\"\"), y_train)\n",
    "\n",
    "best = grid.best_estimator_\n",
    "best_params = grid.best_params_\n",
    "best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9397f",
   "metadata": {},
   "source": [
    "## 4) Classification Models\n",
    "\n",
    "**What is classification?**\n",
    "Predicting a categorical label (class) from input features. Examples:\n",
    "- Spam vs. not spam\n",
    "- Disease present vs. absent\n",
    "- Customer will churn vs. won't churn\n",
    "\n",
    "**Key concepts:**\n",
    "- **proba**: Predicted probability (value between 0 and 1)\n",
    "- **pred**: Predicted class (0 or 1) after applying a threshold\n",
    "- Default threshold is usually 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ff5e4",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression\n",
    "\n",
    "**What is it?**\n",
    "Despite its name, Logistic Regression is a **classification** algorithm that predicts probabilities using a linear decision boundary.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Simple, fast, interpretable\n",
    "- ✅ Works well with high-dimensional data (many features)\n",
    "- ✅ Provides probability estimates\n",
    "- ❌ Assumes linear relationship between features and log-odds\n",
    "\n",
    "**Parameters:**\n",
    "- `max_iter=5000`: Maximum iterations for optimization (increase if convergence warning appears)\n",
    "\n",
    "**When to use:** Baseline model, text classification, when interpretability is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5dcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"pre\", pre),  # from make_preprocess(...)\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "pred  = (proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39d593",
   "metadata": {},
   "source": [
    "### 4.2 Linear SVM\n",
    "\n",
    "**What is SVM?**\n",
    "Support Vector Machine finds the optimal hyperplane that maximally separates classes.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Effective in high-dimensional spaces\n",
    "- ✅ Memory efficient\n",
    "- ✅ Robust to outliers\n",
    "- ❌ Doesn't provide probability estimates by default (only decision scores)\n",
    "\n",
    "**Important notes:**\n",
    "- `decision_function()` returns raw scores (not probabilities)\n",
    "- Default threshold is 0 (not 0.5 like in probabilities)\n",
    "- `C`: Regularization parameter (smaller = more regularization)\n",
    "\n",
    "**When to use:** High-dimensional data, text classification, when you need a strong linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", LinearSVC(C=1.0, max_iter=20000))\n",
    "])\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "scores = svm.decision_function(X_test)  # not probabilities\n",
    "pred = (scores >= 0.0).astype(int)      # threshold 0 by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7a499",
   "metadata": {},
   "source": [
    "### 4.3 RBF SVM\n",
    "\n",
    "**What is RBF kernel?**\n",
    "RBF (Radial Basis Function) kernel allows SVM to learn **non-linear** decision boundaries by mapping data to higher-dimensional space.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Can capture complex, non-linear patterns\n",
    "- ✅ Works well with small to medium datasets\n",
    "- ❌ Slower to train than linear models\n",
    "- ❌ More prone to overfitting\n",
    "- ❌ Requires careful hyperparameter tuning\n",
    "\n",
    "**Parameters:**\n",
    "- `kernel=\"rbf\"`: Use radial basis function kernel\n",
    "- `gamma=\"scale\"`: Controls influence of single training example\n",
    "- `probability=True`: Enable probability estimates (slower but useful)\n",
    "\n",
    "**When to use:** Non-linear relationships, small/medium datasets, when accuracy is more important than speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_rbf = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", SVC(C=1.0, kernel=\"rbf\", gamma=\"scale\", probability=True))\n",
    "])\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "proba = svm_rbf.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28132403",
   "metadata": {},
   "source": [
    "### 4.4 Random Forest\n",
    "\n",
    "**What is Random Forest?**\n",
    "An ensemble of decision trees that:\n",
    "1. Creates multiple decision trees on random subsets of data\n",
    "2. Each tree makes a prediction\n",
    "3. Final prediction is by majority vote (classification) or average (regression)\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Handles non-linear relationships naturally\n",
    "- ✅ Robust to outliers and noise\n",
    "- ✅ Can handle mixed feature types\n",
    "- ✅ Provides feature importance\n",
    "- ✅ Less prone to overfitting than single decision tree\n",
    "- ❌ Slower to train and predict than linear models\n",
    "- ❌ Less interpretable than single trees\n",
    "\n",
    "**Parameters:**\n",
    "- `n_estimators=300`: Number of trees (more = better performance but slower)\n",
    "- `random_state=0`: For reproducibility\n",
    "\n",
    "**When to use:** General-purpose strong classifier, when you need feature importance, when you have mixed data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06143834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", RandomForestClassifier(n_estimators=300, random_state=0))\n",
    "])\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "proba = rf.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f816a01",
   "metadata": {},
   "source": [
    "### 4.9 Decision Tree\n",
    "\n",
    "**What is Decision Tree?**\n",
    "A tree structure where internal nodes represent feature tests, branches represent outcomes, and leaf nodes represent class labels.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Highly interpretable (can visualize the tree)\n",
    "- ✅ No feature scaling required\n",
    "- ✅ Handles both numerical and categorical features\n",
    "- ✅ Automatically performs feature selection\n",
    "- ✅ Can capture non-linear relationships\n",
    "- ❌ Prone to overfitting (especially deep trees)\n",
    "- ❌ Unstable (small data changes can drastically change tree)\n",
    "- ❌ Biased toward features with many levels\n",
    "\n",
    "**Parameters:**\n",
    "- `max_depth=5`: Maximum tree depth (controls overfitting)\n",
    "- `min_samples_split=20`: Minimum samples to split a node\n",
    "- `min_samples_leaf=10`: Minimum samples in a leaf node\n",
    "- `criterion='gini'`: Splitting criterion ('gini' or 'entropy')\n",
    "\n",
    "**When to use:** When interpretability is crucial, as a baseline, or within ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb615dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", DecisionTreeClassifier(max_depth=5, min_samples_split=20, \n",
    "                                     min_samples_leaf=10, random_state=0))\n",
    "])\n",
    "dt.fit(X_train, y_train)\n",
    "proba = dt.predict_proba(X_test)[:, 1]\n",
    "pred = dt.predict(X_test)\n",
    "\n",
    "# Optional: Visualize the tree\n",
    "# from sklearn.tree import plot_tree\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plot_tree(dt.named_steps[\"model\"], filled=True, feature_names=feature_names)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff66a42",
   "metadata": {},
   "source": [
    "### 4.8 K-Nearest Neighbors (KNN)\n",
    "\n",
    "**What is KNN?**\n",
    "A non-parametric method that classifies based on the majority class of k nearest neighbors.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Simple to understand\n",
    "- ✅ No training phase (lazy learning)\n",
    "- ✅ Naturally handles multi-class problems\n",
    "- ✅ Can capture complex decision boundaries\n",
    "- ❌ Very slow prediction (searches all training data)\n",
    "- ❌ Sensitive to feature scaling (must scale features!)\n",
    "- ❌ Poor with high-dimensional data (\"curse of dimensionality\")\n",
    "- ❌ Requires large memory (stores all training data)\n",
    "\n",
    "**Parameters:**\n",
    "- `n_neighbors=5`: Number of neighbors to consider (odd numbers for binary classification)\n",
    "- `weights='uniform'`: All neighbors weighted equally (or 'distance' for distance-weighted)\n",
    "- `metric='minkowski'`: Distance metric\n",
    "\n",
    "**When to use:** Small datasets, as a baseline, when decision boundary is complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a45ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = Pipeline(steps=[\n",
    "    (\"pre\", pre),  # IMPORTANT: KNN requires scaled features!\n",
    "    (\"model\", KNeighborsClassifier(n_neighbors=5, weights='uniform'))\n",
    "])\n",
    "knn.fit(X_train, y_train)\n",
    "proba = knn.predict_proba(X_test)[:, 1]\n",
    "pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b74da",
   "metadata": {},
   "source": [
    "### 4.7 Naive Bayes\n",
    "\n",
    "**What is Naive Bayes?**\n",
    "A probabilistic classifier based on Bayes' theorem with \"naive\" assumption of feature independence.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Very fast training and prediction\n",
    "- ✅ Works well with high-dimensional data\n",
    "- ✅ Good for text classification\n",
    "- ✅ Requires little training data\n",
    "- ❌ Assumes feature independence (rarely true in practice)\n",
    "- ❌ Can be outperformed by more sophisticated models\n",
    "\n",
    "**Variants:**\n",
    "- **GaussianNB**: For continuous features (assumes Gaussian distribution)\n",
    "- **MultinomialNB**: For discrete counts (word counts, frequencies)\n",
    "- **BernoulliNB**: For binary features\n",
    "\n",
    "**When to use:** Text classification, as a fast baseline, when training data is limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "# For continuous features\n",
    "nb_gauss = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", GaussianNB())\n",
    "])\n",
    "\n",
    "# For text/count data (use with TfidfVectorizer or CountVectorizer)\n",
    "nb_multi = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=2)),\n",
    "    (\"model\", MultinomialNB(alpha=1.0))  # alpha: Laplace smoothing parameter\n",
    "])\n",
    "\n",
    "nb_gauss.fit(X_train, y_train)\n",
    "proba = nb_gauss.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a79cc6",
   "metadata": {},
   "source": [
    "### 4.6 XGBoost Classifier\n",
    "\n",
    "**What is XGBoost?**\n",
    "An optimized implementation of gradient boosting with advanced features and better performance.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ State-of-the-art performance on many datasets\n",
    "- ✅ Very fast training (optimized C++ backend)\n",
    "- ✅ Built-in regularization prevents overfitting\n",
    "- ✅ Handles missing values automatically\n",
    "- ✅ Parallel processing support\n",
    "- ❌ Requires separate installation (`pip install xgboost`)\n",
    "- ❌ More hyperparameters to tune\n",
    "\n",
    "**Parameters:**\n",
    "- `n_estimators=100`: Number of boosting rounds\n",
    "- `max_depth=6`: Maximum tree depth\n",
    "- `learning_rate=0.3`: Step size shrinkage\n",
    "- `use_label_encoder=False`: Suppress warning for newer versions\n",
    "- `eval_metric='logloss'`: Evaluation metric\n",
    "\n",
    "**When to use:** Kaggle competitions, production systems, when you need top performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.3, \n",
    "                           use_label_encoder=False, eval_metric='logloss', random_state=0))\n",
    "])\n",
    "xgb.fit(X_train, y_train)\n",
    "proba = xgb.predict_proba(X_test)[:, 1]\n",
    "pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34c77c",
   "metadata": {},
   "source": [
    "### 4.5 Gradient Boosting Classifier\n",
    "\n",
    "**What is Gradient Boosting?**\n",
    "Builds trees sequentially, where each new tree tries to correct the errors of previous trees.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Usually highest accuracy among tree-based methods\n",
    "- ✅ Handles non-linear relationships\n",
    "- ✅ Provides feature importance\n",
    "- ✅ Less prone to overfitting than single trees\n",
    "- ❌ Slower to train (sequential process)\n",
    "- ❌ Requires careful hyperparameter tuning\n",
    "- ❌ Can overfit if not properly regularized\n",
    "\n",
    "**Parameters:**\n",
    "- `n_estimators=100`: Number of boosting stages\n",
    "- `learning_rate=0.1`: Shrinks contribution of each tree (lower = more conservative)\n",
    "- `max_depth=3`: Maximum depth of trees (lower = simpler models)\n",
    "\n",
    "**When to use:** Competitions, when accuracy is crucial, tabular data with complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26afbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0))\n",
    "])\n",
    "gb.fit(X_train, y_train)\n",
    "proba = gb.predict_proba(X_test)[:, 1]\n",
    "pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", RandomForestClassifier(n_estimators=300, random_state=0))\n",
    "])\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "proba = rf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7224f00",
   "metadata": {},
   "source": [
    "## 5) Classification Evaluation Metrics\n",
    "\n",
    "**Why evaluate?**\n",
    "Accuracy alone is often misleading, especially with imbalanced classes. You need multiple metrics to understand model performance from different angles.\n",
    "\n",
    "**Key metrics overview:**\n",
    "- **Accuracy**: Overall correctness (use with caution on imbalanced data)\n",
    "- **Precision**: Of predicted positives, how many are truly positive? (Focus on false positives)\n",
    "- **Recall**: Of actual positives, how many did we catch? (Focus on false negatives)\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced metric)\n",
    "- **Confusion Matrix**: Shows all combinations of predicted vs actual\n",
    "- **AUC**: Overall ability to distinguish between classes across all thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828187c2",
   "metadata": {},
   "source": [
    "### 5.1 Classification Reports\n",
    "\n",
    "**Confusion Matrix structure:**\n",
    "```\n",
    "              Predicted\n",
    "             Neg    Pos\n",
    "Actual Neg | TN  |  FP |\n",
    "Actual Pos | FN  |  TP |\n",
    "```\n",
    "\n",
    "**Metric formulas:**\n",
    "- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)\n",
    "- **Precision** = TP / (TP + FP) - \"When I predict positive, how often am I right?\"\n",
    "- **Recall** = TP / (TP + FN) - \"Of all actual positives, how many did I find?\"\n",
    "- **F1** = 2 × (Precision × Recall) / (Precision + Recall)\n",
    "\n",
    "**`zero_division=0`**: Handles edge cases where division by zero occurs (e.g., no predicted positives)\n",
    "\n",
    "**Classification Report**: Shows precision, recall, and F1 for each class, plus weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fae053",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"Precision:\", precision_score(y_test, pred, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, pred, zero_division=0))\n",
    "print(\"F1:\", f1_score(y_test, pred, zero_division=0))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83d715",
   "metadata": {},
   "source": [
    "### 5.2 AUC (Area Under Curve)\n",
    "\n",
    "**What is AUC-ROC?**\n",
    "Area Under the Receiver Operating Characteristic curve measures the model's ability to distinguish between classes across all possible thresholds.\n",
    "\n",
    "**AUC interpretation:**\n",
    "- **1.0**: Perfect classifier\n",
    "- **0.9-1.0**: Excellent\n",
    "- **0.8-0.9**: Good\n",
    "- **0.7-0.8**: Fair\n",
    "- **0.5-0.7**: Poor\n",
    "- **0.5**: Random guessing (no better than coin flip)\n",
    "\n",
    "**Why AUC?**\n",
    "- Threshold-independent metric\n",
    "- Works well with imbalanced datasets\n",
    "- Single number summary of model performance\n",
    "\n",
    "**Note:** Requires probability scores or decision function values, not just binary predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3866e6",
   "metadata": {},
   "source": [
    "# proba or score for class 1\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, proba)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488868b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions\n",
    "proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, proba)\n",
    "auc = roc_auc_score(y_test, proba)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Model (AUC = {auc:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bf1ba",
   "metadata": {},
   "source": [
    "### 5.3 ROC Curve Visualization\n",
    "\n",
    "**What is ROC Curve?**\n",
    "ROC (Receiver Operating Characteristic) curve plots True Positive Rate vs False Positive Rate at various threshold settings.\n",
    "\n",
    "**How to interpret:**\n",
    "- **X-axis (FPR)**: False Positive Rate = FP / (FP + TN) - \"False alarm rate\"\n",
    "- **Y-axis (TPR)**: True Positive Rate = TP / (TP + FN) - \"Recall\"\n",
    "- Diagonal line = random classifier\n",
    "- Closer to top-left corner = better performance\n",
    "- Area under curve (AUC) summarizes overall performance\n",
    "\n",
    "**Why plot ROC?**\n",
    "- Visualize trade-off between TPR and FPR\n",
    "- Compare multiple models on same plot\n",
    "- Choose optimal threshold based on business requirements\n",
    "- Works well with imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2db68",
   "metadata": {},
   "source": [
    "## 6) Cost-Sensitive Threshold Optimization\n",
    "\n",
    "**Why adjust thresholds?**\n",
    "Default threshold (0.5) assumes equal cost for false positives (FP) and false negatives (FN). In real-world scenarios, these costs are often different.\n",
    "\n",
    "**Examples:**\n",
    "- **Medical diagnosis**: Missing a disease (FN) is much worse than a false alarm (FP)\n",
    "- **Spam filtering**: False positive (blocking real email) is worse than false negative (letting spam through)\n",
    "- **Fraud detection**: Missing fraud (FN) costs money, but investigating false alarms (FP) costs time\n",
    "\n",
    "**How it works:**\n",
    "1. Test many different thresholds\n",
    "2. Calculate cost for each threshold: `Cost = FP × c_fp + FN × c_fn`\n",
    "3. Choose threshold that minimizes total cost\n",
    "\n",
    "**Parameters:**\n",
    "- `c_fp`: Cost of one false positive\n",
    "- `c_fn`: Cost of one false negative\n",
    "\n",
    "**Practical use:** Adjust decision threshold based on business requirements and asymmetric costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_thresholds(y_true, scores, thresholds, c_fp=1.0, c_fn=1.0):\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    scores = np.asarray(scores, float)\n",
    "    best = None\n",
    "    for t in thresholds:\n",
    "        y_pred = (scores >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        cost = fp*c_fp + fn*c_fn\n",
    "        row = {\"threshold\": float(t), \"cost\": float(cost), \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn}\n",
    "        if best is None or row[\"cost\"] < best[\"cost\"]:\n",
    "            best = row\n",
    "    return best\n",
    "\n",
    "thresholds = np.linspace(np.min(scores), np.max(scores), 200)\n",
    "best = sweep_thresholds(y_test, scores, thresholds, c_fp=100, c_fn=500)\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc945060",
   "metadata": {},
   "source": [
    "## 7) Calibration Templates\n",
    "\n",
    "**What is probability calibration?**\n",
    "Some models output scores that don't represent true probabilities. Calibration adjusts these scores so that:\n",
    "- When model says \"70% probability\", about 70% of those predictions are actually positive\n",
    "- Predicted probabilities match observed frequencies\n",
    "\n",
    "**Why calibrate?**\n",
    "- For reliable risk assessment and decision-making\n",
    "- When you need to trust the probability values (not just rankings)\n",
    "- Required for threshold-based decisions where probability matters\n",
    "\n",
    "**Models that need calibration:**\n",
    "- SVM (outputs decision scores, not probabilities)\n",
    "- Naive Bayes (often overconfident)\n",
    "- Neural networks (can be miscalibrated)\n",
    "\n",
    "**Well-calibrated models:**\n",
    "- Logistic Regression (generally well-calibrated)\n",
    "- Random Forest (reasonably calibrated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e501c7",
   "metadata": {},
   "source": [
    "### 7.1 CalibratedClassifierCV\n",
    "\n",
    "**What it does:**\n",
    "Wraps any classifier and calibrates its output using cross-validation.\n",
    "\n",
    "**Calibration methods:**\n",
    "1. **Isotonic regression** (`method=\"isotonic\"`):\n",
    "   - Non-parametric, more flexible\n",
    "   - Can overfit on small datasets\n",
    "   - Requires more calibration data\n",
    "   - Better for non-monotonic relationships\n",
    "\n",
    "2. **Platt scaling** (`method=\"sigmoid\"`):\n",
    "   - Parametric (fits a sigmoid function)\n",
    "   - Works with less calibration data\n",
    "   - More stable on small datasets\n",
    "   - Better for monotonic relationships\n",
    "\n",
    "**Parameters:**\n",
    "- `cv=3`: Uses 3-fold cross-validation for calibration\n",
    "\n",
    "**When to use:** When working with SVMs or other models that don't output well-calibrated probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808bb50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearSVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bn/w4ms4n5n7l3czmht6p6ntpjm0000gn/T/ipykernel_10945/3535317464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalibration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCalibratedClassifierCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalibratedClassifierCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"isotonic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or method=\"sigmoid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pre\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearSVC' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "base = LinearSVC(C=1.0, max_iter=20000)\n",
    "cal = CalibratedClassifierCV(base, method=\"isotonic\", cv=3)  # or method=\"sigmoid\"\n",
    "pipe = Pipeline([(\"pre\", pre), (\"model\", cal)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "proba = pipe.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7ad60",
   "metadata": {},
   "source": [
    "### 7.2 Separate Calibration Model on Calibration Split\n",
    "\n",
    "**Advanced calibration approach:**\n",
    "1. Train base model on training set\n",
    "2. Get predictions on separate calibration set\n",
    "3. Train a calibrator (e.g., Decision Tree) to map raw probabilities to calibrated probabilities\n",
    "4. Apply both models in sequence at test time\n",
    "\n",
    "**Why use this?**\n",
    "- More control over calibration process\n",
    "- Can use different calibration models (trees, isotonic regression, etc.)\n",
    "- Useful when you need custom calibration approaches\n",
    "\n",
    "**Important:** \n",
    "- Always clip final probabilities to [0, 1] range\n",
    "- Requires a three-way split (train, calibration, test)\n",
    "\n",
    "**Trade-off:** Uses less data for training base model but provides better probability estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c392af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# base probability model\n",
    "base = Pipeline([(\"pre\", pre), (\"model\", LogisticRegression(max_iter=5000))])\n",
    "base.fit(X_tr, y_tr)\n",
    "\n",
    "p_cal = base.predict_proba(X_cal)[:, 1]\n",
    "calibrator = DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "calibrator.fit(p_cal.reshape(-1,1), y_cal)\n",
    "\n",
    "p_test_raw = base.predict_proba(X_te)[:, 1]\n",
    "p_test = np.clip(calibrator.predict(p_test_raw.reshape(-1,1)), 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0038a84",
   "metadata": {},
   "source": [
    "## 8) Regression Templates\n",
    "\n",
    "**What is regression?**\n",
    "Predicting a **continuous numerical value** (not a category). Examples:\n",
    "- House prices\n",
    "- Temperature\n",
    "- Stock prices\n",
    "- Customer lifetime value\n",
    "\n",
    "**Key difference from classification:**\n",
    "- Output is a number (not a class label)\n",
    "- Different evaluation metrics (MAE, MSE, RMSE, R²)\n",
    "- No probability predictions or thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be32fba",
   "metadata": {},
   "source": [
    "### 8.1 Linear Regression Baseline\n",
    "\n",
    "**What is Linear Regression?**\n",
    "Fits a linear equation to model the relationship between features and target:\n",
    "`y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ`\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Simple, fast, interpretable\n",
    "- ✅ Works well when relationships are linear\n",
    "- ✅ No hyperparameters to tune\n",
    "- ❌ Assumes linear relationships\n",
    "- ❌ Sensitive to outliers\n",
    "- ❌ Can't capture complex non-linear patterns\n",
    "\n",
    "**When to use:** \n",
    "- As a baseline model (always start here!)\n",
    "- When relationships appear linear\n",
    "- When interpretability is crucial (coefficients show feature impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d185198",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bn/w4ms4n5n7l3czmht6p6ntpjm0000gn/T/ipykernel_10945/3916499526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pre\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = Pipeline([(\"pre\", pre), (\"model\", LinearRegression())])\n",
    "reg.fit(X_train, y_train)\n",
    "pred = reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ba9e6",
   "metadata": {},
   "source": [
    "### 8.2 Ridge/Lasso\n",
    "\n",
    "**What are Ridge and Lasso?**\n",
    "Regularized linear regression models that prevent overfitting by penalizing large coefficients.\n",
    "\n",
    "**Ridge Regression (L2 regularization):**\n",
    "- Shrinks all coefficients toward zero (but never exactly zero)\n",
    "- Good when many features are useful\n",
    "- `alpha`: Controls regularization strength (higher = more regularization)\n",
    "\n",
    "**Lasso Regression (L1 regularization):**\n",
    "- Can shrink coefficients exactly to zero (feature selection)\n",
    "- Good for sparse models (many irrelevant features)\n",
    "- Creates simpler, more interpretable models\n",
    "- `alpha`: Regularization strength (start with 0.01-1.0)\n",
    "\n",
    "**When to use:**\n",
    "- **Ridge**: High multicollinearity, many correlated features, want to keep all features\n",
    "- **Lasso**: Feature selection, suspect many features are irrelevant, want sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33620af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "ridge = Pipeline([(\"pre\", pre), (\"model\", Ridge(alpha=1.0))])\n",
    "lasso = Pipeline([(\"pre\", pre), (\"model\", Lasso(alpha=0.01, max_iter=20000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c9857",
   "metadata": {},
   "source": [
    "### 8.2.1 ElasticNet\n",
    "\n",
    "**What is ElasticNet?**\n",
    "Combines L1 (Lasso) and L2 (Ridge) regularization, getting benefits of both.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Feature selection like Lasso\n",
    "- ✅ Stability of Ridge with correlated features\n",
    "- ✅ Better than Lasso when features are highly correlated\n",
    "- ❌ Two hyperparameters to tune (alpha and l1_ratio)\n",
    "\n",
    "**Parameters:**\n",
    "- `alpha`: Overall regularization strength\n",
    "- `l1_ratio`: Balance between L1 and L2 (0 = Ridge, 1 = Lasso, 0.5 = equal mix)\n",
    "\n",
    "**When to use:** Many correlated features, want both feature selection and stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41269a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic = Pipeline([(\"pre\", pre), (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=20000))])\n",
    "elastic.fit(X_train, y_train)\n",
    "pred = elastic.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96daed",
   "metadata": {},
   "source": [
    "### 8.3 Random Forest Regressor\n",
    "\n",
    "**What is it?**\n",
    "Ensemble of decision trees for regression (similar to Random Forest Classifier but predicts numbers).\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Handles non-linear relationships naturally\n",
    "- ✅ Robust to outliers\n",
    "- ✅ Can capture complex patterns\n",
    "- ✅ Provides feature importance\n",
    "- ✅ Works with mixed data types\n",
    "- ❌ Slower than linear models\n",
    "- ❌ Less interpretable\n",
    "- ❌ Can overfit on noisy data\n",
    "\n",
    "**Parameters:**\n",
    "- `n_estimators=500`: Number of trees (more = better but slower)\n",
    "- Higher is generally better for regression\n",
    "\n",
    "**When to use:** \n",
    "- Non-linear relationships\n",
    "- Complex patterns\n",
    "- When accuracy matters more than speed\n",
    "- When you need feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = Pipeline([(\"pre\", pre), (\"model\", RandomForestRegressor(n_estimators=500, random_state=0))])\n",
    "rf_reg.fit(X_train, y_train)\n",
    "pred = rf_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f865ed",
   "metadata": {},
   "source": [
    "### 8.6 Support Vector Regressor (SVR)\n",
    "\n",
    "**What is SVR?**\n",
    "SVM adapted for regression - tries to fit data within a margin (epsilon-tube).\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Effective in high-dimensional spaces\n",
    "- ✅ Robust to outliers (within epsilon tube)\n",
    "- ✅ Can use different kernels for non-linear relationships\n",
    "- ❌ Slow on large datasets\n",
    "- ❌ Memory intensive\n",
    "- ❌ Requires feature scaling\n",
    "\n",
    "**Parameters:**\n",
    "- `kernel='rbf'`: Kernel type ('linear', 'rbf', 'poly')\n",
    "- `C=1.0`: Regularization parameter\n",
    "- `epsilon=0.1`: Width of epsilon-tube (points inside don't contribute to loss)\n",
    "\n",
    "**When to use:** High-dimensional data, small to medium datasets, when outlier robustness is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = Pipeline([(\"pre\", pre), (\"model\", SVR(kernel='rbf', C=1.0, epsilon=0.1))])\n",
    "svr.fit(X_train, y_train)\n",
    "pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deae9a1",
   "metadata": {},
   "source": [
    "### 8.5 XGBoost Regressor\n",
    "\n",
    "**What is it?**\n",
    "Optimized gradient boosting implementation for regression tasks.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ State-of-the-art performance\n",
    "- ✅ Very fast training\n",
    "- ✅ Built-in regularization\n",
    "- ✅ Handles missing values automatically\n",
    "- ❌ Requires separate installation\n",
    "\n",
    "**When to use:** Production systems, competitions, when you need best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: pip install xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg = Pipeline([(\"pre\", pre), (\"model\", XGBRegressor(\n",
    "    n_estimators=100, max_depth=6, learning_rate=0.1, random_state=0\n",
    "))])\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "pred = xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca3620",
   "metadata": {},
   "source": [
    "### 8.4 Gradient Boosting Regressor\n",
    "\n",
    "**What is it?**\n",
    "Gradient Boosting for regression - builds trees sequentially to correct errors.\n",
    "\n",
    "**Characteristics:**\n",
    "- ✅ Usually best performance among tree-based methods\n",
    "- ✅ Handles non-linear relationships\n",
    "- ✅ Robust to outliers\n",
    "- ✅ Provides feature importance\n",
    "- ❌ Slower to train (sequential)\n",
    "- ❌ Requires hyperparameter tuning\n",
    "- ❌ Can overfit if not regularized\n",
    "\n",
    "**Parameters:**\n",
    "- `n_estimators=100`: Number of boosting stages\n",
    "- `learning_rate=0.1`: Shrinks contribution of each tree\n",
    "- `max_depth=3`: Maximum depth of individual trees\n",
    "- `subsample=1.0`: Fraction of samples for fitting trees (< 1.0 = stochastic gradient boosting)\n",
    "\n",
    "**When to use:** Kaggle competitions, when accuracy is most important, complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_reg = Pipeline([(\"pre\", pre), (\"model\", GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0\n",
    "))])\n",
    "gb_reg.fit(X_train, y_train)\n",
    "pred = gb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e80af",
   "metadata": {},
   "source": [
    "### 8.4 Regression Metrics\n",
    "\n",
    "**Key regression metrics:**\n",
    "\n",
    "1. **MAE (Mean Absolute Error)**:\n",
    "   - Average absolute difference between predicted and actual\n",
    "   - Easy to interpret (same units as target)\n",
    "   - Robust to outliers\n",
    "   - Formula: `(1/n) Σ|yᵢ - ŷᵢ|`\n",
    "\n",
    "2. **MSE (Mean Squared Error)**:\n",
    "   - Average squared difference\n",
    "   - Penalizes large errors more heavily\n",
    "   - Not in same units as target (squared units)\n",
    "   - Formula: `(1/n) Σ(yᵢ - ŷᵢ)²`\n",
    "\n",
    "3. **RMSE (Root Mean Squared Error)**:\n",
    "   - Square root of MSE\n",
    "   - Same units as target (easy to interpret)\n",
    "   - Penalizes large errors\n",
    "   - Most common metric\n",
    "   - Formula: `√MSE`\n",
    "\n",
    "4. **R² (R-squared / Coefficient of Determination)**:\n",
    "   - Proportion of variance explained by model\n",
    "   - Range: -∞ to 1 (1 = perfect, 0 = baseline, negative = worse than baseline)\n",
    "   - Scale-independent\n",
    "   - Formula: `1 - (SS_res / SS_tot)`\n",
    "\n",
    "**Which to use?**\n",
    "- **RMSE**: Most common, interpretable, penalizes large errors\n",
    "- **MAE**: When outliers shouldn't be penalized heavily\n",
    "- **R²**: To understand proportion of variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = reg.predict(X_test)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, pred))\n",
    "print(\"RMSE:\", root_mean_squared_error(y_test, pred))\n",
    "print(\"R2:\", r2_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af58e36",
   "metadata": {},
   "source": [
    "## 9) Cross-Validation Templates\n",
    "\n",
    "**What is cross-validation?**\n",
    "Instead of a single train-test split, cross-validation:\n",
    "1. Divides data into k \"folds\"\n",
    "2. Trains k times, each time using different fold as test set\n",
    "3. Averages results across all folds\n",
    "\n",
    "**Why use cross-validation?**\n",
    "- ✅ More reliable performance estimate\n",
    "- ✅ Uses all data for both training and testing\n",
    "- ✅ Reduces variance in performance estimates\n",
    "- ✅ Better for small datasets\n",
    "- ❌ Computationally expensive (trains k models instead of 1)\n",
    "\n",
    "**Common k values:**\n",
    "- k=5: Standard choice, good balance\n",
    "- k=10: More thorough, more expensive\n",
    "- k=n (Leave-One-Out): Maximum data usage, very expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a38a6",
   "metadata": {},
   "source": [
    "### 9.1 Simple cross_val_score\n",
    "\n",
    "**What it does:**\n",
    "Performs k-fold cross-validation and returns an array of scores (one per fold).\n",
    "\n",
    "**Parameters:**\n",
    "- `cv=5`: Number of folds\n",
    "- `scoring`: Metric to evaluate\n",
    "  - Classification: \"accuracy\", \"f1\", \"precision\", \"recall\", \"roc_auc\"\n",
    "  - Regression: \"neg_mean_squared_error\", \"neg_mean_absolute_error\", \"r2\"\n",
    "\n",
    "**Output interpretation:**\n",
    "- `scores.mean()`: Average performance across folds\n",
    "- `scores.std()`: Variability between folds (lower = more stable)\n",
    "\n",
    "**Note:** For regression metrics, sklearn returns negative values (so higher is always better). Use `np.abs()` or `-scores.mean()` to get positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3cbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")  # change scoring\n",
    "print(scores.mean(), scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f6cf3",
   "metadata": {},
   "source": [
    "### 9.2 StratifiedKFold (Classification)\n",
    "\n",
    "**What is StratifiedKFold?**\n",
    "Ensures each fold maintains the same class distribution as the original dataset.\n",
    "\n",
    "**Why use it?**\n",
    "- Critical for imbalanced datasets\n",
    "- Prevents folds with very few (or zero) examples of minority class\n",
    "- More reliable performance estimates\n",
    "\n",
    "**Parameters:**\n",
    "- `n_splits=5`: Number of folds\n",
    "- `shuffle=True`: Randomly shuffle data before splitting (recommended)\n",
    "- `random_state=0`: For reproducibility\n",
    "\n",
    "**When to use:** \n",
    "- Always for classification with imbalanced classes\n",
    "- Recommended for any classification cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec132d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=cv, scoring=\"f1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999cfce",
   "metadata": {},
   "source": [
    "### 9.3 KFold (Regression)\n",
    "\n",
    "**What is KFold?**\n",
    "Standard k-fold cross-validation for regression tasks (no stratification needed for continuous targets).\n",
    "\n",
    "**Parameters:**\n",
    "- `n_splits=5`: Number of folds\n",
    "- `shuffle=True`: Randomly shuffle before splitting (recommended)\n",
    "- `random_state=0`: For reproducibility\n",
    "\n",
    "**When to use:** Regression tasks, any time you don't need stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "scores = cross_val_score(reg, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "# Note: negative MSE, so use -scores.mean() or np.abs()\n",
    "print(f\"MSE: {-scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a508b",
   "metadata": {},
   "source": [
    "## 10) GridSearchCV Template\n",
    "\n",
    "**What is GridSearchCV?**\n",
    "Automates hyperparameter tuning by:\n",
    "1. Defining a grid of parameter combinations\n",
    "2. Training a model for each combination\n",
    "3. Evaluating using cross-validation\n",
    "4. Returning the best combination\n",
    "\n",
    "**How it works:**\n",
    "- Tests **all possible combinations** (Cartesian product)\n",
    "- Uses cross-validation for each combination\n",
    "- Prevents overfitting to validation set\n",
    "\n",
    "**Parameters explained:**\n",
    "- `estimator`: Your model/pipeline to optimize\n",
    "- `param_grid`: Dictionary of parameters to try\n",
    "- `scoring`: Metric to optimize\n",
    "- `cv`: Number of cross-validation folds\n",
    "- `n_jobs=-1`: Use all CPU cores (much faster)\n",
    "\n",
    "**Important notes:**\n",
    "- Accessing best model: `grid.best_estimator_`\n",
    "- Best parameters: `grid.best_params_`\n",
    "- Best CV score: `grid.best_score_`\n",
    "- All results: `grid.cv_results_`\n",
    "\n",
    "**Tip:** Grid grows exponentially! 3 parameters with 3 values each = 3³ = 27 combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50542a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10]\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    estimator=svm,              # pipeline\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",               # change\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "grid.best_params_, grid.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619cebd2",
   "metadata": {},
   "source": [
    "### 10.1 RandomizedSearchCV\n",
    "\n",
    "**What is RandomizedSearchCV?**\n",
    "Randomly samples hyperparameter combinations instead of testing all combinations.\n",
    "\n",
    "**Why use it?**\n",
    "- ✅ Much faster than GridSearchCV\n",
    "- ✅ Can cover wider parameter space\n",
    "- ✅ Good for exploratory tuning\n",
    "- ✅ Often finds near-optimal parameters with far fewer iterations\n",
    "\n",
    "**How it works:**\n",
    "- Randomly samples `n_iter` combinations from parameter distributions\n",
    "- Each combination is evaluated with cross-validation\n",
    "- Returns best combination found\n",
    "\n",
    "**Parameters:**\n",
    "- `param_distributions`: Dictionary of parameters (can use distributions or lists)\n",
    "- `n_iter=20`: Number of random combinations to try\n",
    "- `random_state=0`: For reproducibility\n",
    "\n",
    "**When to use:**\n",
    "- Large parameter space (would take too long with GridSearchCV)\n",
    "- Early-stage hyperparameter exploration\n",
    "- When you need results quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    \"model__n_estimators\": randint(50, 500),           # Random integers\n",
    "    \"model__max_depth\": randint(3, 20),\n",
    "    \"model__learning_rate\": uniform(0.01, 0.29),       # Continuous uniform\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,              # Number of random combinations\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "random_search.best_params_, random_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958d092",
   "metadata": {},
   "source": [
    "## 12) PCA Templates\n",
    "\n",
    "**What is PCA?**\n",
    "Principal Component Analysis is a dimensionality reduction technique that:\n",
    "1. Finds directions of maximum variance in data\n",
    "2. Projects data onto these directions (principal components)\n",
    "3. Reduces dimensions while preserving most information\n",
    "\n",
    "**Why use PCA?**\n",
    "- ✅ Reduce computational cost (fewer features)\n",
    "- ✅ Reduce overfitting (fewer features to learn)\n",
    "- ✅ Visualization (reduce to 2D or 3D)\n",
    "- ✅ Remove multicollinearity\n",
    "- ✅ Denoise data\n",
    "- ❌ Loses interpretability (PCs are combinations of original features)\n",
    "- ❌ Assumes linear relationships\n",
    "\n",
    "**Key concepts:**\n",
    "- **Explained variance**: How much information each PC captures\n",
    "- **Cumulative explained variance**: Total information captured by first k components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56653bd",
   "metadata": {},
   "source": [
    "### 11.1 PCA for Dimensionality Reduction + Explained Variance\n",
    "\n",
    "**What it does:**\n",
    "Reduces dimensions while keeping a specified amount of variance.\n",
    "\n",
    "**Parameters:**\n",
    "- `n_components=0.90`: Keep 90% of variance\n",
    "  - Alternative: `n_components=50` (keep exactly 50 components)\n",
    "- `svd_solver=\"full\"`: Algorithm for computing PCA\n",
    "\n",
    "**How to use:**\n",
    "1. Fit PCA on training data\n",
    "2. Transform both training and test data\n",
    "3. Check actual number of components: `pca.n_components_`\n",
    "\n",
    "**Interpretation:**\n",
    "- 0.90 means we keep 90% of the original information\n",
    "- Trade-off: Higher variance = more features = more computation\n",
    "- Typical values: 0.85-0.95\n",
    "\n",
    "**Important:** \n",
    "- Always fit PCA on training data only!\n",
    "- Apply same transformation to test data\n",
    "- PCA requires scaled features (use StandardScaler first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.90, svd_solver=\"full\")  # keep 90% variance\n",
    "pipe = Pipeline([(\"pre\", pre), (\"pca\", pca)])\n",
    "Z_train = pipe.fit_transform(X_train)\n",
    "Z_test  = pipe.transform(X_test)\n",
    "\n",
    "# if you need actual number of components:\n",
    "k = pipe.named_steps[\"pca\"].n_components_\n",
    "k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50390c",
   "metadata": {},
   "source": [
    "### 11.2 Reconstruction Error (Anomaly Detection)\n",
    "\n",
    "**What is reconstruction error?**\n",
    "The difference between original data and reconstructed data after dimensionality reduction.\n",
    "\n",
    "**How it works for anomaly detection:**\n",
    "1. Train PCA on normal data\n",
    "2. Project data to lower dimensions\n",
    "3. Reconstruct back to original dimensions\n",
    "4. Calculate error = ||original - reconstructed||\n",
    "5. High error = potential anomaly (data point is unusual)\n",
    "\n",
    "**Intuition:**\n",
    "- Normal data reconstructs well (low error)\n",
    "- Anomalies don't fit the patterns and reconstruct poorly (high error)\n",
    "- Works because PCA learns patterns from normal data\n",
    "\n",
    "**Use cases:**\n",
    "- Fraud detection\n",
    "- Manufacturing defect detection\n",
    "- Network intrusion detection\n",
    "- Quality control\n",
    "\n",
    "**Parameters:**\n",
    "- `n_components=k`: Use fewer components for more sensitive anomaly detection\n",
    "- `top_idx`: Indices of most anomalous samples (highest reconstruction error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68287e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=k)\n",
    "X_scaled = pre.fit_transform(X)   # if using ColumnTransformer; becomes numpy matrix\n",
    "Z = pca.fit_transform(X_scaled)\n",
    "X_hat = pca.inverse_transform(Z)\n",
    "err = np.linalg.norm(X_scaled - X_hat, axis=1)\n",
    "top_idx = np.argsort(err)[::-1][:10]\n",
    "top_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6023954",
   "metadata": {},
   "source": [
    "### 15.3 Random Under-sampling\n",
    "\n",
    "**What it does:**\n",
    "Randomly removes samples from majority class to balance classes.\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Fast and simple\n",
    "- ✅ Reduces training time\n",
    "- ❌ Loses potentially useful information\n",
    "- ❌ May remove important samples\n",
    "\n",
    "**When to use:**\n",
    "- Very large majority class (can afford to lose samples)\n",
    "- When training time is a concern\n",
    "- Combined with oversampling (combine both approaches)\n",
    "\n",
    "**Alternative:** Combine with SMOTE using `RandomUnderSampler` or use `SMOTEENN` / `SMOTETomek`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd04140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: pip install imbalanced-learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Random undersampling\n",
    "pipe_under = ImbPipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"undersampler\", RandomUnderSampler(sampling_strategy='auto', random_state=0)),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "pipe_under.fit(X_train, y_train)\n",
    "pred = pipe_under.predict(X_test)\n",
    "\n",
    "# Combined approach: SMOTE + Undersampling\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "pipe_combined = ImbPipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"resample\", SMOTEENN(random_state=0)),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "pipe_combined.fit(X_train, y_train)\n",
    "pred = pipe_combined.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97a671",
   "metadata": {},
   "source": [
    "### 15.2 SMOTE (Synthetic Minority Over-sampling)\n",
    "\n",
    "**What is SMOTE?**\n",
    "Creates synthetic samples of minority class by interpolating between existing minority samples.\n",
    "\n",
    "**How it works:**\n",
    "1. For each minority sample, find k nearest minority neighbors\n",
    "2. Create synthetic samples along the lines connecting neighbors\n",
    "3. Results in more diverse minority samples than simple duplication\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Better than random oversampling (no exact duplicates)\n",
    "- ✅ Increases minority class representation\n",
    "- ❌ Can create noisy samples in overlapping regions\n",
    "- ❌ Requires installation: `pip install imbalanced-learn`\n",
    "\n",
    "**When to use:** \n",
    "- Severe class imbalance (< 10% minority)\n",
    "- When class weights alone aren't enough\n",
    "- With careful validation (can cause overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941962b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install: pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn's Pipeline\n",
    "\n",
    "# Create pipeline with SMOTE\n",
    "pipe_smote = ImbPipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"smote\", SMOTE(sampling_strategy='auto', random_state=0)),  # auto balances classes\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "pipe_smote.fit(X_train, y_train)\n",
    "pred = pipe_smote.predict(X_test)\n",
    "proba = pipe_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Or apply SMOTE separately\n",
    "# smote = SMOTE(random_state=0)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c651311",
   "metadata": {},
   "source": [
    "### 15.1 Class Weights\n",
    "\n",
    "**What it does:**\n",
    "Gives higher penalty for misclassifying minority class during training.\n",
    "\n",
    "**How it works:**\n",
    "- Automatically adjusts loss function to account for class imbalance\n",
    "- No need to resample data\n",
    "- Most sklearn models support `class_weight` parameter\n",
    "\n",
    "**Options:**\n",
    "- `class_weight='balanced'`: Automatically adjusts weights inversely proportional to class frequencies\n",
    "- `class_weight={0: 1, 1: 10}`: Manual weight assignment\n",
    "\n",
    "**When to use:** \n",
    "- First approach to try (simple, no data modification)\n",
    "- Works with all sklearn models that support it\n",
    "- When you want to keep original data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with: LogisticRegression, SVC, RandomForest, DecisionTree, etc.\n",
    "\n",
    "# Automatic balanced weights\n",
    "clf_balanced = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Or with Random Forest\n",
    "rf_balanced = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=0))\n",
    "])\n",
    "\n",
    "# Manual weights (if class 1 is 10x more important)\n",
    "clf_manual = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, class_weight={0: 1, 1: 10}))\n",
    "])\n",
    "\n",
    "clf_balanced.fit(X_train, y_train)\n",
    "proba = clf_balanced.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a48202",
   "metadata": {},
   "source": [
    "## 14) Handling Class Imbalance\n",
    "\n",
    "**What is class imbalance?**\n",
    "When one class has significantly more samples than others (e.g., 95% negative, 5% positive).\n",
    "\n",
    "**Why it's a problem:**\n",
    "- Models tend to predict majority class\n",
    "- Accuracy becomes misleading metric\n",
    "- Minority class is often the most important (fraud, disease, churn)\n",
    "\n",
    "**Solutions:**\n",
    "1. **Resampling**: Oversample minority or undersample majority\n",
    "2. **Class weights**: Penalize mistakes on minority class more\n",
    "3. **Ensemble methods**: SMOTE, borderline-SMOTE\n",
    "4. **Evaluation**: Use appropriate metrics (F1, precision-recall, AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9152e744",
   "metadata": {},
   "source": [
    "### 14.3 Bagging Classifier\n",
    "\n",
    "**What is bagging?**\n",
    "Bootstrap Aggregating - trains multiple models on random subsets (with replacement) of data and averages predictions.\n",
    "\n",
    "**How it works:**\n",
    "1. Create multiple bootstrap samples (random sampling with replacement)\n",
    "2. Train a model on each sample\n",
    "3. Average predictions (regression) or vote (classification)\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ Reduces variance (overfitting)\n",
    "- ✅ Works well with unstable models (decision trees)\n",
    "- ✅ Can be parallelized (fast training)\n",
    "\n",
    "**Note:** Random Forest is a special case of bagging with decision trees and additional randomization\n",
    "\n",
    "**When to use:** With high-variance models (decision trees), when you want to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8208c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Bagging with decision trees\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=100,      # Number of base models\n",
    "    max_samples=0.8,       # % of samples to draw for each base model\n",
    "    max_features=0.8,      # % of features to draw for each base model\n",
    "    bootstrap=True,        # Sample with replacement\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"bagging\", bagging)])\n",
    "pipe.fit(X_train, y_train)\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca96f70",
   "metadata": {},
   "source": [
    "### 14.2 Stacking Classifier\n",
    "\n",
    "**What is stacking?**\n",
    "A two-level ensemble:\n",
    "1. **Level 0**: Multiple base models make predictions\n",
    "2. **Level 1**: A meta-model learns to combine base model predictions\n",
    "\n",
    "**How it works:**\n",
    "- Train multiple diverse base models on training data\n",
    "- Use their predictions as features for meta-model\n",
    "- Meta-model learns optimal way to combine base predictions\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Often achieves best performance\n",
    "- ✅ Learns optimal combination weights\n",
    "- ❌ More complex, slower to train\n",
    "\n",
    "**When to use:** Competitions, when maximum accuracy is needed, final production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Base models (level 0)\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=5000, random_state=0)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=0)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=50, random_state=0))\n",
    "]\n",
    "\n",
    "# Meta-model (level 1)\n",
    "meta_model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # Cross-validation to generate meta-features\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"stacking\", stacking)])\n",
    "pipe.fit(X_train, y_train)\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d78b9",
   "metadata": {},
   "source": [
    "### 14.1 Voting Classifier\n",
    "\n",
    "**What it does:**\n",
    "Combines predictions from multiple different classifiers.\n",
    "\n",
    "**Voting strategies:**\n",
    "- **'hard'**: Majority vote (most common predicted class)\n",
    "- **'soft'**: Averages predicted probabilities (requires `predict_proba`)\n",
    "\n",
    "**When to use:**\n",
    "- Combine diverse models (e.g., logistic regression + random forest + SVM)\n",
    "- Often improves over individual models\n",
    "- Simple and effective ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5830cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define individual models\n",
    "clf1 = LogisticRegression(max_iter=5000, random_state=0)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "clf3 = SVC(probability=True, random_state=0)  # probability=True for soft voting\n",
    "\n",
    "# Create voting classifier\n",
    "voting = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)],\n",
    "    voting='soft'  # or 'hard'\n",
    ")\n",
    "\n",
    "# Use in pipeline\n",
    "pipe = Pipeline([(\"pre\", pre), (\"voting\", voting)])\n",
    "pipe.fit(X_train, y_train)\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29999e",
   "metadata": {},
   "source": [
    "## 13) Ensemble Methods\n",
    "\n",
    "**What are ensemble methods?**\n",
    "Combine multiple models to create a stronger predictor. The key principle: \"wisdom of the crowd.\"\n",
    "\n",
    "**Main approaches:**\n",
    "1. **Voting**: Combine predictions from multiple different models\n",
    "2. **Bagging**: Train same model on different data subsets (e.g., Random Forest)\n",
    "3. **Boosting**: Train models sequentially, each correcting previous errors (e.g., Gradient Boosting)\n",
    "4. **Stacking**: Use predictions from multiple models as input to a meta-model\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ Often better performance than single models\n",
    "- ✅ Reduces overfitting (averaging reduces variance)\n",
    "- ✅ More robust predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9effc70",
   "metadata": {},
   "source": [
    "### 13.3 Feature Importance from Tree-Based Models\n",
    "\n",
    "**What it does:**\n",
    "Uses feature importance scores from trained tree-based models to select features.\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ Fast (uses already-trained model)\n",
    "- ✅ Captures feature interactions\n",
    "- ✅ Works well in practice\n",
    "\n",
    "**When to use:** With tree-based models (RF, GB, XGB), after training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Train Random Forest and select important features\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train_processed, y_train)  # X_train_processed = after preprocessing\n",
    "\n",
    "# Select features with importance > threshold\n",
    "selector = SelectFromModel(rf, threshold=\"median\", prefit=True)  # or threshold=0.01\n",
    "X_train_selected = selector.transform(X_train_processed)\n",
    "X_test_selected = selector.transform(X_test_processed)\n",
    "\n",
    "# Or use in pipeline\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=0), threshold=\"median\")\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"selector\", selector),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179b0b9",
   "metadata": {},
   "source": [
    "### 13.2 Recursive Feature Elimination (RFE)\n",
    "\n",
    "**What it does:**\n",
    "Recursively removes least important features and builds model until desired number remains.\n",
    "\n",
    "**How it works:**\n",
    "1. Train model on all features\n",
    "2. Rank features by importance\n",
    "3. Remove least important feature(s)\n",
    "4. Repeat until k features remain\n",
    "\n",
    "**Characteristics:**\n",
    "- ❌ Slow (trains multiple models)\n",
    "- ✅ Model-aware (uses actual model importance)\n",
    "- ✅ Often better than filter methods\n",
    "\n",
    "**When to use:** When you have time, when feature interactions matter, for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365adae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Select top 10 features using Random Forest\n",
    "estimator = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "rfe = RFE(estimator=estimator, n_features_to_select=10, step=1)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"rfe\", rfe),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "\n",
    "# See which features were selected\n",
    "# rfe.support_  # Boolean mask\n",
    "# rfe.ranking_  # Feature rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3d8be",
   "metadata": {},
   "source": [
    "### 13.1 SelectKBest (Filter Method)\n",
    "\n",
    "**What it does:**\n",
    "Selects top k features based on statistical tests.\n",
    "\n",
    "**Score functions:**\n",
    "- **Classification**: `f_classif` (ANOVA F-value), `chi2`, `mutual_info_classif`\n",
    "- **Regression**: `f_regression`, `mutual_info_regression`\n",
    "\n",
    "**When to use:** Fast feature selection, as preprocessing step, when you know how many features you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513bdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top 10 features\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"selector\", selector),\n",
    "    (\"model\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "\n",
    "# See selected feature scores\n",
    "# selector.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c479ae",
   "metadata": {},
   "source": [
    "## 12) Feature Selection Templates\n",
    "\n",
    "**What is feature selection?**\n",
    "The process of selecting a subset of relevant features to:\n",
    "- ✅ Reduce overfitting\n",
    "- ✅ Improve model performance\n",
    "- ✅ Reduce training time\n",
    "- ✅ Improve model interpretability\n",
    "\n",
    "**Three main approaches:**\n",
    "1. **Filter methods**: Score features independently (fast, model-agnostic)\n",
    "2. **Wrapper methods**: Use model performance to select features (slower, more accurate)\n",
    "3. **Embedded methods**: Feature selection during model training (e.g., Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530e26c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
