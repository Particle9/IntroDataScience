{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b503a67c",
   "metadata": {},
   "source": [
    "# Chapter 8 Toolkit â€” Pattern Recognition + VC Theory ğŸ§ ğŸ“Š\n",
    "\n",
    "## Welcome to Pattern Recognition and Statistical Learning Theory!\n",
    "\n",
    "This notebook is your comprehensive guide to understanding **how machines learn to classify data** and **why we can trust their predictions**. We'll take you step-by-step through the foundational concepts that power modern machine learning.\n",
    "\n",
    "### ğŸ“š What You'll Learn (in order):\n",
    "\n",
    "1. **Foundation: Risk and Loss** \n",
    "   - What does it mean for a classifier to make \"good\" predictions?\n",
    "   - How do we measure classification error?\n",
    "\n",
    "2. **Linear Classifiers (The Perceptron)** \n",
    "   - The simplest learning algorithm that works!\n",
    "   - When does it succeed? When does it fail?\n",
    "\n",
    "3. **Kernels: Escaping Linear Limitations** \n",
    "   - What if data isn't linearly separable?\n",
    "   - The \"kernel trick\" that transforms your data\n",
    "\n",
    "4. **Empirical Risk Minimization (ERM)** \n",
    "   - The fundamental principle: minimize training error\n",
    "   - Why training error â‰  true error (and how to fix it)\n",
    "\n",
    "5. **VC Theory: The Mathematics of Generalization** \n",
    "   - Why do machine learning models work on unseen data?\n",
    "   - Understanding model complexity and overfitting\n",
    "   - Rigorous probability bounds for generalization\n",
    "\n",
    "6. **Concentration Inequalities** \n",
    "   - Statistical guarantees for finite samples\n",
    "   - How much data do you need to trust your model?\n",
    "\n",
    "### ğŸ¯ Why This Matters:\n",
    "\n",
    "Pattern recognition is about **learning from examples**. Given data points with labels (e.g., spam/not spam, cat/dog), we want to:\n",
    "- Build a **classifier** that makes accurate predictions\n",
    "- Understand **when and why** it works\n",
    "- Get **mathematical guarantees** about future performance\n",
    "\n",
    "This isn't just theoryâ€”these concepts underpin all of supervised learning, from neural networks to support vector machines!\n",
    "\n",
    "### ğŸ’¡ How to Use This Notebook:\n",
    "\n",
    "1. **First time**: Run cells **top-to-bottom** to see concepts build upon each other\n",
    "2. **For review**: Jump to specific sections using the structure above\n",
    "3. **For exams**: Copy functions from the \"Function Index\" at the bottom\n",
    "4. **For understanding**: Read the explanations slowlyâ€”we've made them as clear as possible!\n",
    "\n",
    "Let's begin! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Tuple, Optional, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3062e",
   "metadata": {},
   "source": [
    "## 1. Foundation: Understanding Risk and Loss ğŸ¯\n",
    "\n",
    "### What is Classification?\n",
    "\n",
    "Imagine you have data points $X$ (features) and labels $Y$ (categories). For example:\n",
    "- Emails ($X$) labeled as spam or not spam ($Y$)\n",
    "- Images ($X$) labeled as cat or dog ($Y$)\n",
    "- Patients ($X$) labeled as healthy or sick ($Y$)\n",
    "\n",
    "**Goal**: Build a function $g(X)$ that predicts $Y$ as accurately as possible.\n",
    "\n",
    "### The 0-1 Loss: The Most Natural Error Measure\n",
    "\n",
    "The **0-1 loss** simply counts mistakes:\n",
    "$$\\ell(y, \\hat{y}) = \\begin{cases} 0 & \\text{if } y = \\hat{y} \\text{ (correct)} \\\\ 1 & \\text{if } y \\neq \\hat{y} \\text{ (wrong)} \\end{cases}$$\n",
    "\n",
    "**Why 0-1 loss?** It directly measures what we care about: did we get it right or wrong?\n",
    "\n",
    "### Two Types of Risk: True vs. Empirical\n",
    "\n",
    "#### 1. **True Risk** $R(g)$ â€” The Ideal (but unknown)\n",
    "\n",
    "$$R(g) = \\mathbb{E}[\\mathbf{1}\\{Y \\neq g(X)\\}] = \\mathbb{P}(Y \\neq g(X))$$\n",
    "\n",
    "This is the **probability** that classifier $g$ makes a mistake on a **random new example**. \n",
    "\n",
    "**Problem**: We can't compute this! We don't know the true distribution of $(X, Y)$.\n",
    "\n",
    "#### 2. **Empirical Risk** $\\hat{R}_n(g)$ â€” What We Can Actually Compute\n",
    "\n",
    "$$\\hat{R}_n(g) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{Y_i \\neq g(X_i)\\}$$\n",
    "\n",
    "This is just the **fraction of training examples** that $g$ gets wrong. We use $\\hat{R}_n(g)$ as an **estimate** of $R(g)$.\n",
    "\n",
    "### Key Insight: The Central Question of Machine Learning\n",
    "\n",
    "$$\\boxed{\\text{Training error } \\hat{R}_n(g) \\approx \\text{ True error } R(g) \\text{ ?}}$$\n",
    "\n",
    "**When is this approximation good?**\n",
    "- With **enough data** ($n$ large)\n",
    "- With **simple models** (not too many parameters)\n",
    "- With **independent test data** (never used for training!)\n",
    "\n",
    "This entire chapter is about making this approximation rigorous!\n",
    "\n",
    "### Why These Utilities Matter\n",
    "\n",
    "Below are reusable functions for:\n",
    "- Converting between label formats ({0,1} â†” {-1,+1})\n",
    "- Computing 0-1 loss and empirical risk\n",
    "- Calculating confusion matrices, precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea350f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pm1(y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert labels in {0,1} to {-1,+1}. If already {-1,+1}, keep.\"\"\"\n",
    "    y = np.asarray(y).astype(int)\n",
    "    vals = set(np.unique(y).tolist())\n",
    "    if vals.issubset({-1, 1}):\n",
    "        return y\n",
    "    if vals.issubset({0, 1}):\n",
    "        return 2*y - 1\n",
    "    raise ValueError(\"y must be in {0,1} or {-1,1}.\")\n",
    "\n",
    "def to_01(y_pm1: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert labels in {-1,+1} to {0,1}.\"\"\"\n",
    "    y = np.asarray(y_pm1).astype(int)\n",
    "    vals = set(np.unique(y).tolist())\n",
    "    if not vals.issubset({-1, 1}):\n",
    "        raise ValueError(\"y must be in {-1,1}.\")\n",
    "    return (y + 1)//2\n",
    "\n",
    "def zero_one_loss(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    return (y_true != y_pred).astype(float)\n",
    "\n",
    "def empirical_risk(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(zero_one_loss(y_true, y_pred)))\n",
    "\n",
    "@dataclass\n",
    "class Confusion:\n",
    "    tp: int; fp: int; tn: int; fn: int\n",
    "\n",
    "def confusion_matrix_01(y_true01: np.ndarray, y_pred01: np.ndarray) -> Confusion:\n",
    "    y_true01 = np.asarray(y_true01).astype(int)\n",
    "    y_pred01 = np.asarray(y_pred01).astype(int)\n",
    "    tp = int(np.sum((y_true01==1) & (y_pred01==1)))\n",
    "    fp = int(np.sum((y_true01==0) & (y_pred01==1)))\n",
    "    tn = int(np.sum((y_true01==0) & (y_pred01==0)))\n",
    "    fn = int(np.sum((y_true01==1) & (y_pred01==0)))\n",
    "    return Confusion(tp=tp, fp=fp, tn=tn, fn=fn)\n",
    "\n",
    "def precision_recall_f1(y_true01: np.ndarray, y_pred01: np.ndarray) -> Dict[str, float]:\n",
    "    C = confusion_matrix_01(y_true01, y_pred01)\n",
    "    precision = C.tp / (C.tp + C.fp) if (C.tp + C.fp) > 0 else float(\"nan\")\n",
    "    recall = C.tp / (C.tp + C.fn) if (C.tp + C.fn) > 0 else float(\"nan\")\n",
    "    f1 = 2*precision*recall/(precision+recall) if (precision==precision and recall==recall and (precision+recall)>0) else float(\"nan\")\n",
    "    acc = (C.tp + C.tn) / max(1, (C.tp + C.fp + C.tn + C.fn))\n",
    "    return {\"accuracy\": float(acc), \"precision\": float(precision), \"recall\": float(recall), \"f1\": float(f1), **C.__dict__}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77de065",
   "metadata": {},
   "source": [
    "### ğŸ“Š Test Set Guarantee: Hoeffding's Inequality\n",
    "\n",
    "#### The Problem with Training Error\n",
    "\n",
    "If you train a model on data and measure its error **on the same data**, you get an **optimistically biased** estimate. The model has \"seen\" this data, so it performs artificially well.\n",
    "\n",
    "#### The Solution: An Independent Test Set\n",
    "\n",
    "**Key Idea**: Split your data into:\n",
    "- **Training set** (size $n$): Used to learn the model $\\hat{g}$\n",
    "- **Test set** (size $m$): **Never** shown to the model during training\n",
    "\n",
    "Now measure $\\hat{R}_m(\\hat{g})$ on the test set. This is an unbiased estimate of $R(\\hat{g})$!\n",
    "\n",
    "#### Hoeffding's Bound: How Good is Our Test Estimate?\n",
    "\n",
    "For a **fixed** classifier $\\hat{g}$ and **independent** test set of size $m$:\n",
    "\n",
    "$$\\boxed{\\mathbb{P}\\Big(|\\hat{R}_m(\\hat{g}) - R(\\hat{g})| > \\varepsilon\\Big) \\leq 2e^{-2m\\varepsilon^2}}$$\n",
    "\n",
    "**What this means**:\n",
    "- With high probability (say 95%), $\\hat{R}_m$ is close to $R$\n",
    "- **More test data** ($m$ large) â†’ **tighter** guarantee\n",
    "- **Smaller tolerance** ($\\varepsilon$ small) â†’ need more data\n",
    "\n",
    "**Example**: For $\\varepsilon = 0.05$ and confidence $\\delta = 0.05$, you need $m \\approx 738$ test examples.\n",
    "\n",
    "#### Why This is Important\n",
    "\n",
    "This bound tells you:\n",
    "1. How much to trust your test error\n",
    "2. How many test examples you need for a reliable estimate\n",
    "3. The trade-off between data size and accuracy of estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hoehffding_test_bound(m: int, eps: float) -> float:\n",
    "    \"\"\"2 * exp(-2 m eps^2) for bounded loss in [0,1].\"\"\"\n",
    "    if m <= 0 or eps <= 0:\n",
    "        raise ValueError(\"m>0 and eps>0 required.\")\n",
    "    return float(2 * math.exp(-2 * m * eps * eps))\n",
    "\n",
    "def required_test_size_for_eps_delta(eps: float, delta: float) -> int:\n",
    "    \"\"\"Small helper: choose m so that 2 exp(-2 m eps^2) <= delta.\"\"\"\n",
    "    if not (0 < delta < 1) or eps <= 0:\n",
    "        raise ValueError(\"delta in (0,1), eps>0.\")\n",
    "    m = math.log(2/delta) / (2 * eps * eps)\n",
    "    return int(math.ceil(m))\n",
    "\n",
    "# Example:\n",
    "eps, delta = 0.03, 0.05\n",
    "print(\"m needed for eps=0.03, delta=0.05:\", required_test_size_for_eps_delta(eps, delta))\n",
    "print(\"bound check:\", hoehffding_test_bound(required_test_size_for_eps_delta(eps, delta), eps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d985dc",
   "metadata": {},
   "source": [
    "## 2. Linear Classifiers: The Perceptron Algorithm ğŸ§ \n",
    "\n",
    "### What is a Linear Classifier?\n",
    "\n",
    "Imagine drawing a **line** (in 2D) or **hyperplane** (in higher dimensions) that separates two classes:\n",
    "\n",
    "```\n",
    "    Class +1          |          Class -1\n",
    "      â—  â—            |            â—‹  â—‹\n",
    "        â—             |          â—‹\n",
    "      â—               |            â—‹\n",
    "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "           â†‘ This line is our classifier\n",
    "```\n",
    "\n",
    "**Mathematical form**: \n",
    "$$g(x) = \\text{sign}(w^\\top x + b)$$\n",
    "\n",
    "where:\n",
    "- $w$ is the **weight vector** (perpendicular to the line)\n",
    "- $b$ is the **bias** (shifts the line)\n",
    "- $\\text{sign}(z) = \\begin{cases} +1 & \\text{if } z \\geq 0 \\\\ -1 & \\text{if } z < 0 \\end{cases}$\n",
    "\n",
    "### The Perceptron: Learning the Line\n",
    "\n",
    "**Perceptron** is an algorithm that learns $w$ and $b$ from labeled examples.\n",
    "\n",
    "#### How It Works (Intuition):\n",
    "\n",
    "1. **Start** with random (or zero) weights\n",
    "2. **Loop** through training examples:\n",
    "   - If example $i$ is **correctly classified**: do nothing\n",
    "   - If example $i$ is **misclassified**: adjust $w$ and $b$ to move toward the correct side\n",
    "3. **Repeat** until no mistakes (or max iterations reached)\n",
    "\n",
    "#### The Update Rule:\n",
    "\n",
    "When point $(x_i, y_i)$ is misclassified (i.e., $y_i(w^\\top x_i + b) \\leq 0$):\n",
    "\n",
    "$$w \\leftarrow w + y_i x_i$$\n",
    "$$b \\leftarrow b + y_i$$\n",
    "\n",
    "**Intuition**: We \"push\" the decision boundary in the direction that would correctly classify $x_i$.\n",
    "\n",
    "### When Does Perceptron Work?\n",
    "\n",
    "**Theorem 8.2 (Perceptron Convergence)**: If data is **linearly separable**, perceptron will find a separating hyperplane in **finite time**.\n",
    "\n",
    "**Linearly separable** means there exists some line/hyperplane that perfectly separates the classes.\n",
    "\n",
    "#### Two Scenarios:\n",
    "\n",
    "1. **Separable data** â†’ Perceptron converges! âœ…\n",
    "2. **Non-separable data** â†’ Perceptron cycles forever âŒ\n",
    "\n",
    "For non-separable data, we need:\n",
    "- Fixed iteration limit (early stopping)\n",
    "- More sophisticated methods (kernels, neural networks, etc.)\n",
    "\n",
    "### Why Linear Classifiers?\n",
    "\n",
    "âœ… **Simple** and interpretable  \n",
    "âœ… **Fast** to train  \n",
    "âœ… **Foundation** for more complex methods  \n",
    "âœ… Work well when classes are roughly linearly separable  \n",
    "\n",
    "âŒ **Limited** to linear decision boundaries  \n",
    "âŒ Fails on **XOR-like** problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerceptronModel:\n",
    "    w: np.ndarray\n",
    "    b: float\n",
    "\n",
    "def perceptron_train(\n",
    "    X: np.ndarray,\n",
    "    y_pm1: np.ndarray,\n",
    "    max_epochs: int = 50,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[PerceptronModel, Dict[str, object]]:\n",
    "    \"\"\"Train a perceptron. y must be in {-1,+1}.\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = to_pm1(y_pm1).astype(int)\n",
    "    n, d = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    w = np.zeros(d, dtype=float)\n",
    "    b = 0.0\n",
    "    updates = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        idx = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng.shuffle(idx)\n",
    "        mistakes = 0\n",
    "        for i in idx:\n",
    "            margin = y[i] * (X[i] @ w + b)\n",
    "            if margin <= 0:\n",
    "                w = w + y[i] * X[i]\n",
    "                b = b + y[i]\n",
    "                updates += 1\n",
    "                mistakes += 1\n",
    "        if mistakes == 0:\n",
    "            return PerceptronModel(w=w, b=b), {\"converged\": True, \"epochs\": epoch+1, \"updates\": updates}\n",
    "    return PerceptronModel(w=w, b=b), {\"converged\": False, \"epochs\": max_epochs, \"updates\": updates}\n",
    "\n",
    "def perceptron_predict(model: PerceptronModel, X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    scores = X @ model.w + model.b\n",
    "    yhat = np.where(scores >= 0, 1, -1).astype(int)\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25cee26",
   "metadata": {},
   "source": [
    "### ğŸ“Š Demo: Separable vs. Non-Separable Data\n",
    "\n",
    "Let's see perceptron in action on two types of data:\n",
    "\n",
    "1. **Linearly separable**: A line can perfectly separate the classes\n",
    "   - Perceptron should converge (no mistakes after some iterations)\n",
    "\n",
    "2. **Non-separable (ring/circle)**: Inner circle vs. outer ring\n",
    "   - No single line can separate these\n",
    "   - Perceptron will struggle and not converge\n",
    "\n",
    "**What to observe**:\n",
    "- Training risk (error rate)\n",
    "- Number of epochs to convergence\n",
    "- Whether the algorithm converges at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29114d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linearly_separable(n: int = 200, seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n, 2))\n",
    "    true_w = np.array([1.2, -0.7])\n",
    "    true_b = 0.1\n",
    "    y = np.where(X @ true_w + true_b >= 0, 1, -1)\n",
    "    return X, y\n",
    "\n",
    "def make_ring_nonseparable(n: int = 300, seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    angles = rng.uniform(0, 2*np.pi, size=n)\n",
    "    radii = rng.uniform(0.0, 4.0, size=n)\n",
    "    X = np.column_stack([radii*np.cos(angles), radii*np.sin(angles)])\n",
    "    y = np.where(radii <= 1.0, 1, -1)  # inner disk vs outer region (not linearly separable)\n",
    "    return X, y\n",
    "\n",
    "def plot_2d_points(X: np.ndarray, y_pm1: np.ndarray, title: str = \"\"):\n",
    "    y = to_pm1(y_pm1)\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], s=18, label=\"+1\")\n",
    "    plt.scatter(X[y==-1, 0], X[y==-1, 1], s=18, label=\"-1\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "X_sep, y_sep = make_linearly_separable()\n",
    "plot_2d_points(X_sep, y_sep, \"Linearly separable data\")\n",
    "\n",
    "model, info = perceptron_train(X_sep, y_sep, max_epochs=30, seed=0)\n",
    "yhat = perceptron_predict(model, X_sep)\n",
    "print(info, \"train risk:\", empirical_risk(y_sep, yhat))\n",
    "\n",
    "X_ring, y_ring = make_ring_nonseparable()\n",
    "plot_2d_points(X_ring, y_ring, \"Non-separable (ring) data\")\n",
    "\n",
    "model2, info2 = perceptron_train(X_ring, y_ring, max_epochs=30, seed=0)\n",
    "yhat2 = perceptron_predict(model2, X_ring)\n",
    "print(info2, \"train risk:\", empirical_risk(y_ring, yhat2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667af8f2",
   "metadata": {},
   "source": [
    "## 3. Kernelization: Escaping Linear Limitations ğŸ”§\n",
    "\n",
    "### The Problem: What if Data Isn't Linearly Separable?\n",
    "\n",
    "Consider the \"ring\" problem we just saw:\n",
    "```\n",
    "          â—‹  â—‹  â—‹  â—‹\n",
    "        â—‹   â—  â—   â—‹\n",
    "       â—‹   â—    â—   â—‹\n",
    "        â—‹   â—  â—   â—‹\n",
    "          â—‹  â—‹  â—‹  â—‹\n",
    "```\n",
    "\n",
    "**No line** can separate the inner points (â—) from outer points (â—‹)!\n",
    "\n",
    "### The Solution: Feature Mapping\n",
    "\n",
    "**Key Idea**: Transform data to a **higher-dimensional space** where it becomes linearly separable!\n",
    "\n",
    "#### Example: Polynomial Features\n",
    "\n",
    "Original features: $(x_1, x_2)$  \n",
    "Mapped features: $\\phi(x) = (x_1, x_2, x_1^2, x_2^2, x_1x_2)$\n",
    "\n",
    "Now in 5D space, the data might be linearly separable!\n",
    "\n",
    "### The Kernel Trick ğŸ©âœ¨\n",
    "\n",
    "**Problem**: Computing $\\phi(x)$ explicitly can be expensive (or even impossible for infinite dimensions).\n",
    "\n",
    "**Kernel Trick**: We don't need $\\phi(x)$ itself! We only need **inner products** $\\phi(x) \\cdot \\phi(y)$.\n",
    "\n",
    "A **kernel function** computes this directly:\n",
    "$$k(x, y) = \\phi(x) \\cdot \\phi(y)$$\n",
    "\n",
    "**without** ever computing $\\phi(x)$ explicitly!\n",
    "\n",
    "### Common Kernels and Their Intuitions\n",
    "\n",
    "#### 1. **Linear Kernel** \n",
    "$$k(x, y) = x^\\top y$$\n",
    "- Just the original inner product\n",
    "- No transformation (baseline)\n",
    "\n",
    "#### 2. **Polynomial Kernel**\n",
    "$$k(x, y) = (\\gamma x^\\top y + r)^d$$\n",
    "- Implicitly maps to polynomial features of degree $d$\n",
    "- Captures interactions between features\n",
    "- Example: $d=2$ captures all pairwise products\n",
    "\n",
    "#### 3. **RBF (Gaussian) Kernel**\n",
    "$$k(x, y) = \\exp(-\\gamma \\|x - y\\|^2)$$\n",
    "- Measures \"similarity\" based on distance\n",
    "- Close points â†’ high similarity (â‰ˆ1)\n",
    "- Far points â†’ low similarity (â‰ˆ0)\n",
    "- Maps to **infinite-dimensional** space!\n",
    "- Most popular for non-linear data\n",
    "\n",
    "#### 4. **RBF with L1 Distance**\n",
    "$$k(x, y) = \\exp(-\\gamma \\|x - y\\|_1)$$\n",
    "- Like RBF but uses Manhattan distance\n",
    "- More robust to outliers in some cases\n",
    "\n",
    "### Valid Kernels: The PSD Property\n",
    "\n",
    "**Not every** function $k(x,y)$ is a valid kernel!\n",
    "\n",
    "**Required property**: The kernel matrix $K_{ij} = k(x_i, x_j)$ must be **positive semi-definite (PSD)**.\n",
    "\n",
    "**Why?** Because $K$ represents inner products, which always form PSD matrices.\n",
    "\n",
    "We provide utilities to:\n",
    "- Check if a matrix is PSD\n",
    "- Construct explicit feature maps from kernel matrices (Lemma 8.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Common kernels (use whichever you need)\n",
    "# -----------------------------\n",
    "\n",
    "def kernel_linear(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    return np.asarray(X) @ np.asarray(Y).T\n",
    "\n",
    "def kernel_poly(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0, r: float = 0.0, degree: int = 2) -> np.ndarray:\n",
    "    return (gamma * (np.asarray(X) @ np.asarray(Y).T) + r) ** degree\n",
    "\n",
    "def kernel_rbf_l2(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0) -> np.ndarray:\n",
    "    X = np.asarray(X); Y = np.asarray(Y)\n",
    "    X2 = np.sum(X*X, axis=1, keepdims=True)\n",
    "    Y2 = np.sum(Y*Y, axis=1, keepdims=True).T\n",
    "    d2 = X2 + Y2 - 2*(X @ Y.T)\n",
    "    return np.exp(-gamma * d2)\n",
    "\n",
    "def kernel_rbf_l1(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0) -> np.ndarray:\n",
    "    X = np.asarray(X); Y = np.asarray(Y)\n",
    "    K = np.empty((X.shape[0], Y.shape[0]), dtype=float)\n",
    "    for i in range(X.shape[0]):\n",
    "        K[i, :] = np.exp(-gamma * np.sum(np.abs(Y - X[i]), axis=1))\n",
    "    return K\n",
    "\n",
    "def is_psd(K: np.ndarray, tol: float = 1e-10) -> bool:\n",
    "    \"\"\"Check symmetric PSD: eigenvalues >= -tol.\"\"\"\n",
    "    K = np.asarray(K, dtype=float)\n",
    "    if not np.allclose(K, K.T, atol=1e-8):\n",
    "        return False\n",
    "    eigs = np.linalg.eigvalsh(K)\n",
    "    return bool(np.min(eigs) >= -tol)\n",
    "\n",
    "def feature_map_from_gram(K: np.ndarray, tol: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Lemma 8.3 constructive: if K is PSD, build B s.t. K = B B^T, rows are phi(x_i).\"\"\"\n",
    "    K = np.asarray(K, dtype=float)\n",
    "    if not np.allclose(K, K.T, atol=1e-8):\n",
    "        raise ValueError(\"K must be symmetric.\")\n",
    "    eigvals, eigvecs = np.linalg.eigh(K)\n",
    "    eigvals = np.maximum(eigvals, 0.0)\n",
    "    keep = eigvals > tol\n",
    "    B = eigvecs[:, keep] * np.sqrt(eigvals[keep])\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bc017",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KernelPerceptronModel:\n",
    "    X_train: np.ndarray\n",
    "    y_train: np.ndarray   # {-1,+1}\n",
    "    alpha: np.ndarray     # integer-like update counts\n",
    "    kernel: Callable[[np.ndarray, np.ndarray], np.ndarray]\n",
    "\n",
    "def kernel_perceptron_train(\n",
    "    X: np.ndarray,\n",
    "    y_pm1: np.ndarray,\n",
    "    kernel: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "    max_epochs: int = 20,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[KernelPerceptronModel, Dict[str, object]]:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = to_pm1(y_pm1).astype(int)\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    alpha = np.zeros(n, dtype=float)\n",
    "\n",
    "    K = kernel(X, X)\n",
    "    updates = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        idx = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng.shuffle(idx)\n",
    "        mistakes = 0\n",
    "        for i in idx:\n",
    "            # f(x_i) = sum_j alpha_j y_j k(x_j, x_i)\n",
    "            score = np.sum(alpha * y * K[:, i])\n",
    "            if y[i] * score <= 0:\n",
    "                alpha[i] += 1.0\n",
    "                updates += 1\n",
    "                mistakes += 1\n",
    "        if mistakes == 0:\n",
    "            return KernelPerceptronModel(X, y, alpha, kernel), {\"converged\": True, \"epochs\": epoch+1, \"updates\": updates}\n",
    "    return KernelPerceptronModel(X, y, alpha, kernel), {\"converged\": False, \"epochs\": max_epochs, \"updates\": updates}\n",
    "\n",
    "def kernel_perceptron_predict(model: KernelPerceptronModel, X_new: np.ndarray) -> np.ndarray:\n",
    "    X_new = np.asarray(X_new, dtype=float)\n",
    "    K = model.kernel(model.X_train, X_new)  # shape (n_train, n_new)\n",
    "    scores = (model.alpha * model.y_train)[:, None] * K\n",
    "    scores = np.sum(scores, axis=0)\n",
    "    return np.where(scores >= 0, 1, -1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfbf6f",
   "metadata": {},
   "source": [
    "### ğŸ¯ Demo: Kernels Fix the Ring Problem!\n",
    "\n",
    "Let's apply kernel perceptron to the ring data that linear perceptron failed on.\n",
    "\n",
    "**What to expect**:\n",
    "- **Linear kernel**: Should fail (equivalent to regular perceptron)\n",
    "- **RBF kernel**: Should succeed! The transformation makes data separable\n",
    "\n",
    "**Key Insight**: The RBF kernel measures **distances**, so it naturally separates inner (close to center) from outer (far from center) points.\n",
    "\n",
    "This demonstrates the **power of kernelization**: same algorithm (perceptron), but now works on non-linear problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce542ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernel perceptron on ring\n",
    "kp_lin, info_lin = kernel_perceptron_train(X_ring, y_ring, kernel=lambda A,B: kernel_linear(A,B), max_epochs=15, seed=0)\n",
    "pred_lin = kernel_perceptron_predict(kp_lin, X_ring)\n",
    "print(\"linear-kernel:\", info_lin, \"train risk:\", empirical_risk(y_ring, pred_lin))\n",
    "\n",
    "# RBF kernel perceptron on ring\n",
    "kp_rbf, info_rbf = kernel_perceptron_train(X_ring, y_ring, kernel=lambda A,B: kernel_rbf_l2(A,B,gamma=1.0), max_epochs=15, seed=0)\n",
    "pred_rbf = kernel_perceptron_predict(kp_rbf, X_ring)\n",
    "print(\"rbf-kernel:\", info_rbf, \"train risk:\", empirical_risk(y_ring, pred_rbf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7a5f6",
   "metadata": {},
   "source": [
    "## 4. Empirical Risk Minimization (ERM): The Fundamental Principle âš–ï¸\n",
    "\n",
    "### What is ERM?\n",
    "\n",
    "**Empirical Risk Minimization** is the foundational principle of supervised learning:\n",
    "\n",
    "$$\\boxed{\\hat{g}_n = \\arg\\min_{g \\in \\mathcal{M}} \\hat{R}_n(g)}$$\n",
    "\n",
    "**In words**: Among all classifiers in your model class $\\mathcal{M}$, pick the one that makes the **fewest mistakes on training data**.\n",
    "\n",
    "### Example Model Classes\n",
    "\n",
    "- $\\mathcal{M}_{\\text{linear}}$ = all linear classifiers\n",
    "- $\\mathcal{M}_{\\text{trees}}$ = all decision trees\n",
    "- $\\mathcal{M}_{\\text{neural}}$ = all neural networks with given architecture\n",
    "\n",
    "### The ERM Trap: Overfitting ğŸ£\n",
    "\n",
    "**Crucial observation**: $\\hat{R}_n(\\hat{g}_n)$ is **optimistically biased**!\n",
    "\n",
    "**Why?** Because we chose $\\hat{g}_n$ to minimize training error. It's like:\n",
    "- Training for an exam using **the exact test questions** \n",
    "- Of course you'll do well on those questions!\n",
    "- But will you do well on **new** questions?\n",
    "\n",
    "### The Bias-Complexity Trade-off\n",
    "\n",
    "Consider two extremes:\n",
    "\n",
    "1. **Very simple model** (e.g., constant predictor):\n",
    "   - High training error âŒ\n",
    "   - BUT: Training error â‰ˆ True error âœ…\n",
    "   \n",
    "2. **Very complex model** (e.g., memorize all training points):\n",
    "   - Zero training error âœ…\n",
    "   - BUT: Training error â‰ª True error âŒ (overfitting!)\n",
    "\n",
    "**Goal**: Find the sweet spot!\n",
    "\n",
    "### How to Get Honest Error Estimates\n",
    "\n",
    "#### Solution 1: Independent Test Set (Preferred!)\n",
    "\n",
    "1. **Split** data: Training (for ERM) + Test (never seen)\n",
    "2. **Train** on training set: $\\hat{g}_n$\n",
    "3. **Evaluate** on test set: $\\hat{R}_{\\text{test}}(\\hat{g}_n)$\n",
    "\n",
    "Now $\\hat{R}_{\\text{test}}$ is **unbiased** for $R(\\hat{g}_n)$!\n",
    "\n",
    "Plus, Hoeffding bound (from earlier) tells you how much to trust it:\n",
    "$$\\mathbb{P}(|\\hat{R}_{\\text{test}} - R| > \\varepsilon) \\leq 2e^{-2m\\varepsilon^2}$$\n",
    "\n",
    "#### Solution 2: VC Theory (Later!)\n",
    "\n",
    "If you don't have a test set, VC theory gives **worst-case bounds** on generalization error based on:\n",
    "- Training set size $n$\n",
    "- Model complexity (VC dimension)\n",
    "\n",
    "### Why Independent Test Data Matters\n",
    "\n",
    "**KEY**: Test set must be:\n",
    "- âœ… **Independently sampled** (same distribution as training)\n",
    "- âœ… **Never used for training** (no peeking!)\n",
    "- âœ… **Never used for model selection** (no tuning hyperparameters on it!)\n",
    "\n",
    "If you violate any of these, your test error becomes biased!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f94d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: np.ndarray, y: np.ndarray, test_size: float = 0.3, seed: int = 0):\n",
    "    X = np.asarray(X); y = np.asarray(y)\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n); rng.shuffle(idx)\n",
    "    m = int(round(n * test_size))\n",
    "    test_idx = idx[:m]; train_idx = idx[m:]\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# Example: evaluate perceptron with a test set + Hoeffding bound\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_sep, y_sep, test_size=0.3, seed=1)\n",
    "model, info = perceptron_train(Xtr, ytr, max_epochs=30, seed=0)\n",
    "pred_te = perceptron_predict(model, Xte)\n",
    "risk_te = empirical_risk(yte, pred_te)\n",
    "\n",
    "eps = 0.05\n",
    "print(\"test risk:\", risk_te)\n",
    "print(\"P(|Rhat - R| > eps) <=\", hoehffding_test_bound(len(yte), eps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f469575",
   "metadata": {},
   "source": [
    "## 4.2 Precision, Recall, and the Confusion Matrix ğŸ“Š\n",
    "\n",
    "### Beyond Accuracy: When Classes are Imbalanced\n",
    "\n",
    "**Accuracy** = (# correct) / (# total) is not always enough!\n",
    "\n",
    "**Example**: Cancer detection\n",
    "- 99% of patients are healthy\n",
    "- A classifier that always predicts \"healthy\" has 99% accuracy!\n",
    "- But it **never detects cancer** ğŸ˜±\n",
    "\n",
    "We need more nuanced metrics.\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "For binary classification (positive/negative):\n",
    "\n",
    "|                    | **Predicted +** | **Predicted -** |\n",
    "|--------------------|-----------------|-----------------|\n",
    "| **Actual +**       | TP (True Pos)   | FN (False Neg)  |\n",
    "| **Actual -**       | FP (False Pos)  | TN (True Neg)   |\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "#### 1. **Precision** (How many predicted positives are actually positive?)\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP} = P(Y=1 \\mid \\hat{Y}=1)$$\n",
    "\n",
    "**Intuition**: \"When I say positive, how often am I right?\"\n",
    "\n",
    "#### 2. **Recall** (How many actual positives did I find?)\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN} = P(\\hat{Y}=1 \\mid Y=1)$$\n",
    "\n",
    "**Intuition**: \"Of all positives, how many did I catch?\"  \n",
    "Also called **Sensitivity** or **True Positive Rate**.\n",
    "\n",
    "#### 3. **F1-Score** (Harmonic mean of precision and recall)\n",
    "$$F1 = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "**Intuition**: Balances precision and recall into a single number.\n",
    "\n",
    "### The Precision-Recall Trade-off\n",
    "\n",
    "Usually, there's a trade-off:\n",
    "- **High recall** (catch all positives) â†’ many false alarms â†’ low precision\n",
    "- **High precision** (only sure bets) â†’ miss many positives â†’ low recall\n",
    "\n",
    "Example:\n",
    "- Spam filter with high recall: catches all spam but some real emails too\n",
    "- Spam filter with high precision: never wrong but misses some spam\n",
    "\n",
    "### Edge Cases to Watch Out For âš ï¸\n",
    "\n",
    "1. **No predicted positives** ($TP + FP = 0$):\n",
    "   - Precision = undefined (NaN)\n",
    "   - Often happens with very imbalanced data\n",
    "\n",
    "2. **No actual positives** ($TP + FN = 0$):\n",
    "   - Recall = undefined (NaN)\n",
    "   - Rare unless data is severely biased\n",
    "\n",
    "Our utilities handle these cases gracefully by returning NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e55d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_on_testset(y_true, y_pred, label_space: str = \"pm1\") -> Dict[str, float]:\n",
    "    \"\"\"label_space: 'pm1' for {-1,1}, '01' for {0,1}.\"\"\"\n",
    "    if label_space == \"pm1\":\n",
    "        y_true01 = to_01(to_pm1(y_true))\n",
    "        y_pred01 = to_01(to_pm1(y_pred))\n",
    "    elif label_space == \"01\":\n",
    "        y_true01 = np.asarray(y_true).astype(int)\n",
    "        y_pred01 = np.asarray(y_pred).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"label_space must be 'pm1' or '01'\")\n",
    "    return precision_recall_f1(y_true01, y_pred01)\n",
    "\n",
    "# Edge-case demos\n",
    "y_true01 = np.array([1,0,0,0,0,0,0,0,0,0])\n",
    "always1 = np.ones_like(y_true01)\n",
    "always0 = np.zeros_like(y_true01)\n",
    "print(\"always 1:\", metrics_on_testset(y_true01, always1, label_space=\"01\"))\n",
    "print(\"always 0:\", metrics_on_testset(y_true01, always0, label_space=\"01\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46010e",
   "metadata": {},
   "source": [
    "## 4.3 Finite Hyperplane ERM (Theorem 8.16) ğŸ²\n",
    "\n",
    "### Motivation: Can We Get Generalization Guarantees?\n",
    "\n",
    "We've seen:\n",
    "- Training error $\\hat{R}_n$ can be optimistically biased\n",
    "- Test sets help, but require holding out data\n",
    "\n",
    "**Question**: Can we get **theoretical guarantees** about generalization?\n",
    "\n",
    "**Answer**: Yes! But only for **finite model classes**.\n",
    "\n",
    "### The Setup: Finite Set of Hyperplanes\n",
    "\n",
    "Consider a **restricted model class** $\\mathcal{M}_n$ containing only hyperplanes that pass through **$m$ training points** (where $m$ is the dimension).\n",
    "\n",
    "**Key property**: This class is **finite**!\n",
    "- There are $\\binom{n}{m}$ ways to choose $m$ points\n",
    "- Each defines 2 hyperplanes (one for each side)\n",
    "- Total: $|\\mathcal{M}_n| = 2\\binom{n}{m}$ classifiers\n",
    "\n",
    "### Theorem 8.16: Generalization Bound\n",
    "\n",
    "For this finite class, with high probability:\n",
    "\n",
    "$$R(\\hat{g}) \\leq \\inf_{g \\in \\mathcal{M}} R(g) + \\varepsilon$$\n",
    "\n",
    "with probability at least:\n",
    "\n",
    "$$1 - \\exp(2m\\varepsilon) \\cdot (2\\binom{n}{m} + 1) \\cdot \\exp(-n\\varepsilon^2/2)$$\n",
    "\n",
    "**What this means**:\n",
    "- Our ERM classifier $\\hat{g}$ is nearly as good as the **best in class**\n",
    "- The bound gets tighter with more data ($n$ large)\n",
    "- But it degrades with model complexity ($\\binom{n}{m}$ large)\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "In practice, $\\binom{n}{m}$ is huge! For $n=100, m=3$: $\\binom{100}{3} = 161,700$.\n",
    "\n",
    "**Solution**: **Random sampling approach**\n",
    "1. Randomly sample many $m$-tuples of points\n",
    "2. Build hyperplane through each\n",
    "3. Pick the one with lowest training error\n",
    "\n",
    "This approximates ERM over the finite class and is computationally feasible!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "This is our **first rigorous generalization guarantee**! It shows:\n",
    "- Finite model classes â†’ predictable generalization\n",
    "- But $\\binom{n}{m}$ grows fast â†’ loose bounds\n",
    "- **Foreshadowing**: Need better tools for infinite/large classes â†’ VC theory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperplane_from_d_points(points: np.ndarray, tol: float = 1e-12) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Build a hyperplane a^T x + b = 0 passing through d points in R^d.\n",
    "    Returns (a, b) up to scaling.\n",
    "    Uses nullspace of [points, 1].\n",
    "    \"\"\"\n",
    "    P = np.asarray(points, dtype=float)\n",
    "    d = P.shape[1]\n",
    "    if P.shape[0] != d:\n",
    "        raise ValueError(\"Need exactly d points in R^d to determine a hyperplane uniquely (up to scale).\")\n",
    "    A = np.hstack([P, np.ones((d, 1))])  # shape (d, d+1)\n",
    "    # Find vector v in nullspace of A (A v = 0). Nullspace dimension should be 1 in general position.\n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    v = Vt[-1, :]  # smallest singular value vector\n",
    "    if np.linalg.norm(A @ v) > 1e-8:\n",
    "        raise RuntimeError(\"Failed to find a stable nullspace vector; points may be degenerate.\")\n",
    "    a = v[:d]\n",
    "    b = v[d]\n",
    "    # normalize for stability\n",
    "    norm = np.linalg.norm(a)\n",
    "    if norm < tol:\n",
    "        raise RuntimeError(\"Degenerate hyperplane (normal ~ 0).\")\n",
    "    a = a / norm\n",
    "    b = b / norm\n",
    "    return a, b\n",
    "\n",
    "def predict_hyperplane(a: np.ndarray, b: float, X: np.ndarray) -> np.ndarray:\n",
    "    scores = np.asarray(X, dtype=float) @ a + b\n",
    "    return np.where(scores >= 0, 1, 0).astype(int)  # {0,1} decision rule\n",
    "\n",
    "def sampled_hyperplane_erm(\n",
    "    X: np.ndarray,\n",
    "    y01: np.ndarray,\n",
    "    n_candidates: int = 2000,\n",
    "    seed: int = 0,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"Sample many d-tuples, build hyperplanes, pick best empirical-risk rule.\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y01 = np.asarray(y01).astype(int)\n",
    "    n, d = X.shape\n",
    "    if n < d:\n",
    "        raise ValueError(\"Need n >= d to sample d points.\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    best = {\"risk\": float(\"inf\"), \"a\": None, \"b\": None, \"flip\": 0}\n",
    "    for _ in range(n_candidates):\n",
    "        idx = rng.choice(n, size=d, replace=False)\n",
    "        try:\n",
    "            a, b = hyperplane_from_d_points(X[idx, :])\n",
    "        except Exception:\n",
    "            continue\n",
    "        pred = predict_hyperplane(a, b, X)\n",
    "        r = empirical_risk(y01, pred)\n",
    "        # also consider the opposite side (phi+ vs phi-)\n",
    "        pred_flip = 1 - pred\n",
    "        r2 = empirical_risk(y01, pred_flip)\n",
    "        if r2 < r:\n",
    "            r, pred = r2, pred_flip\n",
    "            flip = 1\n",
    "        else:\n",
    "            flip = 0\n",
    "        if r < best[\"risk\"]:\n",
    "            best = {\"risk\": float(r), \"a\": a, \"b\": float(b), \"flip\": flip}\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a63cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo in 2D (d=2): sample hyperplane ERM on separable data (convert labels to {0,1})\n",
    "best = sampled_hyperplane_erm(X_sep, to_01(y_sep), n_candidates=3000, seed=0)\n",
    "best[\"risk\"], best[\"a\"], best[\"b\"], best[\"flip\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa76f02",
   "metadata": {},
   "source": [
    "### ğŸ“ Computing Theorem 8.16 Bound\n",
    "\n",
    "The bound from Theorem 8.16 is:\n",
    "\n",
    "$$\\mathbb{P}\\left\\{R(\\hat{\\phi}) > \\inf_{\\mathcal{M}} R(\\phi) + \\varepsilon\\right\\} \\leq \\exp(2m\\varepsilon) \\cdot (2\\binom{n}{m} + 1) \\cdot \\exp\\left(-\\frac{n\\varepsilon^2}{2}\\right)$$\n",
    "\n",
    "**Valid when**: $\\frac{2m}{n} \\leq \\varepsilon \\leq 1$\n",
    "\n",
    "**Components**:\n",
    "- $\\exp(2m\\varepsilon)$: Penalty for dimension\n",
    "- $2\\binom{n}{m} + 1$: Size of model class\n",
    "- $\\exp(-n\\varepsilon^2/2)$: Concentration benefit from data\n",
    "\n",
    "**Interpretation**: \n",
    "- More data ($n$ â†‘) â†’ bound decreases (good!)\n",
    "- More dimension ($m$ â†‘) â†’ bound increases (bad!)\n",
    "- Larger tolerance ($\\varepsilon$ â†‘) â†’ bound increases\n",
    "\n",
    "Below we compute this bound numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n: int, k: int) -> int:\n",
    "    return math.comb(n, k)\n",
    "\n",
    "def theorem_816_bound(n: int, m_dim: int, eps: float) -> float:\n",
    "    if not (0 < eps <= 1):\n",
    "        raise ValueError(\"eps in (0,1]\")\n",
    "    # exp(2 m eps) * (2*C(n,m)+1) * exp(-n eps^2 / 2)\n",
    "    return float(math.exp(2*m_dim*eps) * (2*comb(n, m_dim) + 1) * math.exp(-n*(eps**2)/2))\n",
    "\n",
    "# Example numbers (small just for illustration)\n",
    "print(theorem_816_bound(n=200, m_dim=2, eps=0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae82cd",
   "metadata": {},
   "source": [
    "## 5. VC Theory: The Mathematics of Generalization ğŸ“\n",
    "\n",
    "### The Big Question\n",
    "\n",
    "**How do we get generalization bounds for infinite model classes?**\n",
    "\n",
    "Examples of infinite classes:\n",
    "- All linear classifiers (infinite ways to set $w$ and $b$)\n",
    "- All polynomials of degree $\\leq d$\n",
    "- All neural networks with given architecture\n",
    "\n",
    "Theorem 8.16 only worked for **finite** classes. We need a new approach!\n",
    "\n",
    "### The Key Insight: Effective Complexity\n",
    "\n",
    "**VC theory's brilliant idea**: Even though a model class is infinite, on **any finite dataset**, it can only produce **finitely many** different classification patterns.\n",
    "\n",
    "### Shattering: Measuring Expressiveness\n",
    "\n",
    "**Definition**: A set of $n$ points is **shattered** by class $\\mathcal{A}$ if $\\mathcal{A}$ can produce **all $2^n$ possible labelings** of those points.\n",
    "\n",
    "#### Example: Linear Classifiers in 2D\n",
    "\n",
    "**Can we shatter 3 points?** \n",
    "\n",
    "```\n",
    "     â—  â—  â—\n",
    "```\n",
    "\n",
    "Try all $2^3 = 8$ labelings:\n",
    "- $(+,+,+)$, $(+,+,-)$, $(+,-,+)$, $(-,+,+)$, ...\n",
    "\n",
    "**Answer**: YES! For any 3 points in general position, some line achieves each labeling.\n",
    "\n",
    "**Can we shatter 4 points?**\n",
    "\n",
    "**Answer**: NO! Some labelings (like XOR: $(+,-,+,-)$) cannot be achieved by any line.\n",
    "\n",
    "### The Shattering Number\n",
    "\n",
    "**Definition**: $s(\\mathcal{A}, n)$ = maximum number of **distinct labelings** $\\mathcal{A}$ can produce on **any** set of $n$ points.\n",
    "\n",
    "**Properties**:\n",
    "- $s(\\mathcal{A}, n) \\leq 2^n$ (can't have more labelings than $2^n$)\n",
    "- If $\\mathcal{A}$ can shatter some $n$ points, then $s(\\mathcal{A}, n) = 2^n$\n",
    "- Usually grows like polynomial after some point (not exponential!)\n",
    "\n",
    "### The VC Dimension\n",
    "\n",
    "**Definition**: The **VC dimension** $V_{\\mathcal{A}}$ is the **largest** $n$ such that $\\mathcal{A}$ can shatter $n$ points.\n",
    "\n",
    "**Examples**:\n",
    "- Linear classifiers in $\\mathbb{R}^d$: $V \\approx d + 1$ (Lemma 8.38)\n",
    "- Polynomial classifiers degree $d$ in $\\mathbb{R}$: $V = d + 1$\n",
    "- Neural networks: depends on architecture (typically $\\approx$ number of parameters)\n",
    "\n",
    "### Why VC Dimension Matters\n",
    "\n",
    "**VC dimension** = **effective complexity** of model class\n",
    "\n",
    "- **Low VC dim**: Simple models, good generalization, but limited expressiveness\n",
    "- **High VC dim**: Complex models, can fit complex patterns, but risk overfitting\n",
    "\n",
    "### The Sauer-Shelah Lemma: The Key Bound\n",
    "\n",
    "**Theorem (Sauer-Shelah)**: If $V_{\\mathcal{A}} = V$, then:\n",
    "\n",
    "$$s(\\mathcal{A}, n) \\leq \\sum_{i=0}^{V-1} \\binom{n}{i} \\leq \\left(\\frac{en}{V}\\right)^V$$\n",
    "\n",
    "**What this means**:\n",
    "- Even for infinite $\\mathcal{A}$, shattering number grows **polynomially** (not exponentially!)\n",
    "- The polynomial degree is the **VC dimension**\n",
    "- This replaces $2\\binom{n}{m}$ from Theorem 8.16\n",
    "\n",
    "### VC Generalization Bounds\n",
    "\n",
    "**Theorem 8.29** (Uniform convergence): For any $\\mathcal{A}$ with shattering number $s(\\mathcal{A},n)$:\n",
    "\n",
    "$$\\mathbb{P}\\left(\\sup_{A \\in \\mathcal{A}} |\\nu_n(A) - \\nu(A)| > \\varepsilon\\right) \\leq 8 \\cdot s(\\mathcal{A}, n) \\cdot e^{-n\\varepsilon^2/32}$$\n",
    "\n",
    "(valid when $n\\varepsilon^2 \\geq 2$)\n",
    "\n",
    "**Corollary 8.31** (For classifiers): Adjusted constants for 0-1 loss:\n",
    "\n",
    "$$\\mathbb{P}\\left(\\sup_{g \\in \\mathcal{M}} |R(g) - \\hat{R}_n(g)| > \\varepsilon\\right) \\leq 8 \\cdot s(\\mathcal{M}, n) \\cdot e^{-n\\varepsilon^2/64}$$\n",
    "\n",
    "### The Complete Picture\n",
    "\n",
    "For ERM classifier $\\hat{g}_n$:\n",
    "\n",
    "$$\\boxed{R(\\hat{g}_n) \\leq \\underbrace{\\hat{R}_n(\\hat{g}_n)}_{\\text{training error}} + \\underbrace{\\varepsilon}_{\\text{generalization gap}}}$$\n",
    "\n",
    "with high probability (controlled by VC bounds).\n",
    "\n",
    "**Key takeaways**:\n",
    "1. âœ… Works for **infinite** model classes!\n",
    "2. âœ… Only requires **VC dimension** (no need to count classifiers)\n",
    "3. âœ… Explains the **bias-variance trade-off** mathematically\n",
    "4. âš ï¸ Bounds are often loose in practice (but theoretically tight!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sauer_shelah_upper(n: int, vc_dim: int) -> int:\n",
    "    \"\"\"Upper bound on shattering number via Sauerâ€“Shelah: sum_{i=0}^{V-1} C(n,i).\"\"\"\n",
    "    if vc_dim <= 0:\n",
    "        return 1\n",
    "    upper = 0\n",
    "    for i in range(0, min(vc_dim, n+1)):\n",
    "        upper += math.comb(n, i)\n",
    "    return upper\n",
    "\n",
    "def lemma_837_poly_upper(N: int, k: int) -> float:\n",
    "    \"\"\"Lemma 8.37 style: sum_{i=0}^{k-1} C(N,i) <= (eN/k)^k.\"\"\"\n",
    "    if k <= 0:\n",
    "        return 1.0\n",
    "    return float((math.e * N / k) ** k)\n",
    "\n",
    "def vc_generalization_bound_theorem_829(n: int, eps: float, shattering_number: float) -> float:\n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"eps>0\")\n",
    "    return float(8.0 * shattering_number * math.exp(-n * eps * eps / 32.0))\n",
    "\n",
    "def vc_generalization_bound_cor_831(n: int, eps: float, shattering_number: float) -> float:\n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"eps>0\")\n",
    "    return float(8.0 * shattering_number * math.exp(-n * eps * eps / 64.0))\n",
    "\n",
    "# Example: linear classifiers in R^m have VC-dim about m+1 (Lemma 8.38 informal)\n",
    "def bound_linear_classifiers(n: int, eps: float, m_dim: int) -> Dict[str, float]:\n",
    "    vc_dim = m_dim + 1\n",
    "    s_upper = sauer_shelah_upper(n, vc_dim)\n",
    "    return {\n",
    "        \"vc_dimâ‰ˆm+1\": vc_dim,\n",
    "        \"sauer_shelah_upper\": float(s_upper),\n",
    "        \"Thm8.29_bound\": vc_generalization_bound_theorem_829(n, eps, s_upper),\n",
    "        \"Cor8.31_bound\": vc_generalization_bound_cor_831(n, eps, s_upper),\n",
    "    }\n",
    "\n",
    "bound_linear_classifiers(n=200, eps=0.1, m_dim=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a58e74",
   "metadata": {},
   "source": [
    "## 6. Martingale Concentration: ERM Training Risk (Theorem 8.43) ğŸ²\n",
    "\n",
    "### A Different Question\n",
    "\n",
    "So far we asked: *How well does training error estimate true error?*\n",
    "\n",
    "Now we ask: *How concentrated is the training error itself?*\n",
    "\n",
    "### The Setup\n",
    "\n",
    "Consider the **ERM classifier** $\\hat{g}_n$ (depends on random training data).\n",
    "\n",
    "The **training risk** $\\hat{R}_n(\\hat{g}_n)$ is itself a **random variable**!\n",
    "\n",
    "### Theorem 8.43: Azuma-Hoeffding Concentration\n",
    "\n",
    "For the ERM training risk:\n",
    "\n",
    "$$\\mathbb{P}\\left(|\\hat{R}_n(\\hat{g}_n) - \\mathbb{E}[\\hat{R}_n(\\hat{g}_n)]| > \\varepsilon\\right) < 2e^{-n\\varepsilon^2/2}$$\n",
    "\n",
    "**What this means**:\n",
    "- Training error concentrates around its **expectation**\n",
    "- With $n$ examples, deviation larger than $\\varepsilon$ is exponentially unlikely\n",
    "- This is a **martingale concentration** result (Azuma's inequality)\n",
    "\n",
    "### Why This is Different\n",
    "\n",
    "**Previous results** (Theorems 8.29, 8.31): \n",
    "- About gap between $R(g)$ and $\\hat{R}_n(g)$ \n",
    "- Need VC dimension / shattering numbers\n",
    "\n",
    "**This result** (Theorem 8.43):\n",
    "- About concentration of $\\hat{R}_n(\\hat{g}_n)$ around its mean\n",
    "- True for **any** ERM procedure (no VC dimension needed!)\n",
    "- Uses martingale theory (bounded differences)\n",
    "\n",
    "### The Intuition: Martingale Argument\n",
    "\n",
    "Think of revealing training examples one by one:\n",
    "- After seeing $i$ examples, we have partial information\n",
    "- Adding one more example changes training error by **at most $1/n$**\n",
    "- This \"bounded difference\" property â†’ concentration via Azuma\n",
    "\n",
    "### Practical Implication\n",
    "\n",
    "This bound tells you how **stable** the ERM training error is:\n",
    "- If you resample training data, training error won't change much\n",
    "- Useful for understanding **variance** of the learning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erm_training_risk_concentration_bound(n: int, eps: float) -> float:\n",
    "    \"\"\"Thm 8.43: 2 exp(-n eps^2 / 2).\"\"\"\n",
    "    if n <= 0 or eps <= 0:\n",
    "        raise ValueError(\"n>0, eps>0.\")\n",
    "    return float(2.0 * math.exp(-n * eps * eps / 2.0))\n",
    "\n",
    "print(erm_training_risk_concentration_bound(500, 0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4ad60",
   "metadata": {},
   "source": [
    "## ğŸ“ Chapter Summary: Putting It All Together\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "We've journeyed through the foundations of **supervised learning theory**. Here's how everything connects:\n",
    "\n",
    "#### 1ï¸âƒ£ **The Goal**: Learn from Data\n",
    "- **Input**: Training examples $(X_i, Y_i)$\n",
    "- **Output**: Classifier $g(X)$ that predicts well on new data\n",
    "- **Measure**: Risk $R(g) = \\mathbb{P}(Y \\neq g(X))$\n",
    "\n",
    "#### 2ï¸âƒ£ **The Method**: Empirical Risk Minimization\n",
    "- **Can't compute** true risk $R(g)$ (don't know true distribution)\n",
    "- **Can compute** empirical risk $\\hat{R}_n(g)$ (average on training data)\n",
    "- **ERM principle**: Pick $\\hat{g} = \\arg\\min_{g \\in \\mathcal{M}} \\hat{R}_n(g)$\n",
    "\n",
    "#### 3ï¸âƒ£ **The Challenge**: Generalization Gap\n",
    "- Training error $\\hat{R}_n(\\hat{g})$ is **optimistically biased** âš ï¸\n",
    "- True error $R(\\hat{g})$ is what we really care about\n",
    "- **Gap**: $R(\\hat{g}) - \\hat{R}_n(\\hat{g})$ depends on:\n",
    "  - Model complexity (VC dimension)\n",
    "  - Training set size\n",
    "  - Test set independence\n",
    "\n",
    "#### 4ï¸âƒ£ **The Solutions**: \n",
    "\n",
    "**For Simple Cases**:\n",
    "- âœ… **Independent test set** + Hoeffding bound\n",
    "  - Most practical approach\n",
    "  - Requires holding out data\n",
    "  \n",
    "**For Finite Model Classes**:\n",
    "- âœ… **Theorem 8.16**: Bounds using $|\\mathcal{M}|$\n",
    "  - Works but requires finite class\n",
    "  \n",
    "**For Infinite Classes**:\n",
    "- âœ… **VC Theory** (Theorems 8.29, 8.31)\n",
    "  - Replace size $|\\mathcal{M}|$ with shattering number $s(\\mathcal{M}, n)$\n",
    "  - Bounded by Sauer-Shelah: $s(\\mathcal{M}, n) \\leq (en/V)^V$\n",
    "  - VC dimension $V$ measures **effective complexity**\n",
    "\n",
    "**For Training Stability**:\n",
    "- âœ… **Theorem 8.43**: Martingale concentration\n",
    "  - Training error concentrates around its mean\n",
    "  - Independent of model complexity\n",
    "\n",
    "### ğŸ”‘ Key Insights\n",
    "\n",
    "1. **Bias-Variance Trade-off**:\n",
    "   - Simple models: high bias, low variance â†’ underfitting\n",
    "   - Complex models: low bias, high variance â†’ overfitting\n",
    "   - Sweet spot: minimize $\\text{bias}^2 + \\text{variance}$\n",
    "\n",
    "2. **Sample Complexity**:\n",
    "   - Need $n \\gtrsim V/\\varepsilon^2$ examples for $\\varepsilon$-accurate learning\n",
    "   - More complex models (high $V$) need more data!\n",
    "\n",
    "3. **The Kernel Trick**:\n",
    "   - Transforms non-linear problems â†’ linear in feature space\n",
    "   - Never need explicit features, just kernel function\n",
    "   - Enables infinite-dimensional feature spaces!\n",
    "\n",
    "4. **Perceptron Theorem**:\n",
    "   - Converges if data is linearly separable\n",
    "   - Number of updates bounded by margin\n",
    "   - Foundation for SVMs and neural networks\n",
    "\n",
    "### ğŸ“Š Practical Takeaways\n",
    "\n",
    "**When building a classifier**:\n",
    "1. âœ… Always use a separate test set (never train on it!)\n",
    "2. âœ… Check if data is linearly separable (try linear first)\n",
    "3. âœ… Use kernels for non-linear problems (RBF is popular)\n",
    "4. âœ… Monitor training vs. test error (detect overfitting)\n",
    "5. âœ… Consider model complexity vs. data size (VC dimension)\n",
    "\n",
    "**When evaluating**:\n",
    "1. âœ… Don't trust training error alone\n",
    "2. âœ… Use precision/recall for imbalanced data\n",
    "3. âœ… Use Hoeffding bounds to quantify test set reliability\n",
    "4. âœ… Consider worst-case VC bounds for safety-critical applications\n",
    "\n",
    "### ğŸ¯ What You've Mastered\n",
    "\n",
    "- âœ… **Foundation**: Loss functions, risk, empirical risk\n",
    "- âœ… **Algorithms**: Perceptron, kernel perceptron\n",
    "- âœ… **Theory**: ERM principle, generalization bounds\n",
    "- âœ… **VC Theory**: Shattering, VC dimension, Sauer-Shelah\n",
    "- âœ… **Concentration**: Hoeffding, Azuma-Hoeffding\n",
    "- âœ… **Practice**: Train/test splits, kernels, evaluation metrics\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Function Index (Quick Reference for Problem Solving)\n",
    "\n",
    "Copy these functions when solving exercises or exam problems!\n",
    "\n",
    "### ğŸ“ Core Risk & Metrics\n",
    "```python\n",
    "empirical_risk(y_true, y_pred)              # Compute 0-1 loss rate\n",
    "precision_recall_f1(y_true01, y_pred01)     # Full metric suite\n",
    "metrics_on_testset(y_true, y_pred, label_space=\"pm1\")\n",
    "train_test_split(X, y, test_size=0.3, seed=0)\n",
    "```\n",
    "\n",
    "### ğŸ“Š Statistical Bounds\n",
    "```python\n",
    "hoehffding_test_bound(m, eps)               # P(|R_test - R| > eps)\n",
    "required_test_size_for_eps_delta(eps, delta) # Inverse: m needed\n",
    "erm_training_risk_concentration_bound(n, eps) # Thm 8.43\n",
    "```\n",
    "\n",
    "### ğŸ§  Linear Classifiers\n",
    "```python\n",
    "perceptron_train(X, y_pm1, max_epochs=50, seed=0)\n",
    "perceptron_predict(model, X)\n",
    "```\n",
    "\n",
    "### ğŸ”§ Kernels & Kernel Perceptron\n",
    "```python\n",
    "# Kernel functions\n",
    "kernel_linear(X, Y)\n",
    "kernel_poly(X, Y, gamma=1.0, r=0.0, degree=2)\n",
    "kernel_rbf_l2(X, Y, gamma=1.0)\n",
    "kernel_rbf_l1(X, Y, gamma=1.0)\n",
    "\n",
    "# Utilities\n",
    "is_psd(K, tol=1e-10)                        # Check valid kernel\n",
    "feature_map_from_gram(K)                     # Lemma 8.3 construction\n",
    "\n",
    "# Training\n",
    "kernel_perceptron_train(X, y_pm1, kernel, max_epochs=20, seed=0)\n",
    "kernel_perceptron_predict(model, X_new)\n",
    "```\n",
    "\n",
    "### ğŸ² Finite Hyperplane ERM (Thm 8.16)\n",
    "```python\n",
    "hyperplane_from_d_points(points)             # Build hyperplane\n",
    "sampled_hyperplane_erm(X, y01, n_candidates=2000, seed=0)\n",
    "theorem_816_bound(n, m_dim, eps)             # Compute Thm 8.16 bound\n",
    "```\n",
    "\n",
    "### ğŸ“š VC Theory Bounds\n",
    "```python\n",
    "sauer_shelah_upper(n, vc_dim)                # Upper bound on s(M,n)\n",
    "lemma_837_poly_upper(N, k)                   # (eN/k)^k bound\n",
    "vc_generalization_bound_theorem_829(n, eps, s)\n",
    "vc_generalization_bound_cor_831(n, eps, s)\n",
    "bound_linear_classifiers(n, eps, m_dim)      # All-in-one for linear\n",
    "```\n",
    "\n",
    "### ğŸ¯ Helper Functions\n",
    "```python\n",
    "to_pm1(y)          # Convert {0,1} â†’ {-1,+1}\n",
    "to_01(y_pm1)       # Convert {-1,+1} â†’ {0,1}\n",
    "zero_one_loss(y_true, y_pred)\n",
    "confusion_matrix_01(y_true01, y_pred01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Ready for More?\n",
    "\n",
    "You now have a solid foundation in:\n",
    "- **Pattern recognition algorithms** (perceptron, kernels)\n",
    "- **Statistical learning theory** (VC dimension, generalization)\n",
    "- **Practical machine learning** (train/test, evaluation)\n",
    "\n",
    "**Next steps**:\n",
    "- Chapter 10: High-dimensional statistics\n",
    "- Chapter 11: Dimensionality reduction (PCA, manifold learning)\n",
    "- Advanced topics: Neural networks, ensemble methods, optimization\n",
    "\n",
    "**Good luck on your exams and projects!** ğŸ“šâœ¨"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
