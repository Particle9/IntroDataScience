{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b503a67c",
   "metadata": {},
   "source": [
    "# Chapter 8 Toolkit â€” Pattern Recognition + VC Theory (Lecture Notes pp. 123â€“145)\n",
    "\n",
    "This notebook turns the chapter into a **reusable Python toolkit** ðŸ§°\n",
    "\n",
    "Youâ€™ll get:\n",
    "- **Linear classifiers** (perceptron) âœ…\n",
    "- **Kernelization** (kernel perceptron + kernel checks) âœ…\n",
    "- **Empirical risk minimization (ERM)** + train/test guarantees âœ…\n",
    "- **Precision/Recall utilities** (with edge-case warnings) âœ…\n",
    "- **Finite-hyperplane ERM idea** (Thm 8.16) as a practical sampler âœ…\n",
    "- **VC theory bounds** (shattering upper bounds via Sauerâ€“Shelah, generalization bounds) âœ…\n",
    "- **Martingale/Azuma-style concentration** for ERM risk (Thm 8.43) âœ…\n",
    "\n",
    "> Tip: Run top-to-bottom once. Then copy the â€œFunction indexâ€ section whenever you solve exam-style problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Tuple, Optional, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3062e",
   "metadata": {},
   "source": [
    "## 8.0 Setup: 0â€“1 loss, empirical risk, and metrics ðŸŽ¯\n",
    "\n",
    "The chapterâ€™s core object is the **risk**:\n",
    "- True risk:  \\(R(g)=\\mathbb{E}[\\mathbf{1}\\{Y\\neq g(X)\\}]\\)\n",
    "- Empirical risk:  \\(\\hat R_n(g)=\\frac1n\\sum_{i=1}^n \\mathbf{1}\\{Y_i\\neq g(X_i)\\}\\)\n",
    "\n",
    "Below are utilities you can reuse for any classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea350f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pm1(y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert labels in {0,1} to {-1,+1}. If already {-1,+1}, keep.\"\"\"\n",
    "    y = np.asarray(y).astype(int)\n",
    "    vals = set(np.unique(y).tolist())\n",
    "    if vals.issubset({-1, 1}):\n",
    "        return y\n",
    "    if vals.issubset({0, 1}):\n",
    "        return 2*y - 1\n",
    "    raise ValueError(\"y must be in {0,1} or {-1,1}.\")\n",
    "\n",
    "def to_01(y_pm1: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert labels in {-1,+1} to {0,1}.\"\"\"\n",
    "    y = np.asarray(y_pm1).astype(int)\n",
    "    vals = set(np.unique(y).tolist())\n",
    "    if not vals.issubset({-1, 1}):\n",
    "        raise ValueError(\"y must be in {-1,1}.\")\n",
    "    return (y + 1)//2\n",
    "\n",
    "def zero_one_loss(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    return (y_true != y_pred).astype(float)\n",
    "\n",
    "def empirical_risk(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(zero_one_loss(y_true, y_pred)))\n",
    "\n",
    "@dataclass\n",
    "class Confusion:\n",
    "    tp: int; fp: int; tn: int; fn: int\n",
    "\n",
    "def confusion_matrix_01(y_true01: np.ndarray, y_pred01: np.ndarray) -> Confusion:\n",
    "    y_true01 = np.asarray(y_true01).astype(int)\n",
    "    y_pred01 = np.asarray(y_pred01).astype(int)\n",
    "    tp = int(np.sum((y_true01==1) & (y_pred01==1)))\n",
    "    fp = int(np.sum((y_true01==0) & (y_pred01==1)))\n",
    "    tn = int(np.sum((y_true01==0) & (y_pred01==0)))\n",
    "    fn = int(np.sum((y_true01==1) & (y_pred01==0)))\n",
    "    return Confusion(tp=tp, fp=fp, tn=tn, fn=fn)\n",
    "\n",
    "def precision_recall_f1(y_true01: np.ndarray, y_pred01: np.ndarray) -> Dict[str, float]:\n",
    "    C = confusion_matrix_01(y_true01, y_pred01)\n",
    "    precision = C.tp / (C.tp + C.fp) if (C.tp + C.fp) > 0 else float(\"nan\")\n",
    "    recall = C.tp / (C.tp + C.fn) if (C.tp + C.fn) > 0 else float(\"nan\")\n",
    "    f1 = 2*precision*recall/(precision+recall) if (precision==precision and recall==recall and (precision+recall)>0) else float(\"nan\")\n",
    "    acc = (C.tp + C.tn) / max(1, (C.tp + C.fp + C.tn + C.fn))\n",
    "    return {\"accuracy\": float(acc), \"precision\": float(precision), \"recall\": float(recall), \"f1\": float(f1), **C.__dict__}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77de065",
   "metadata": {},
   "source": [
    "### Held-out test set guarantee (Hoeffding style) âœ…\n",
    "\n",
    "If you train on `n` points and test on an **independent** test set of size `m`, then for bounded loss (like 0â€“1 loss):\n",
    "\\[\n",
    "\\mathbb{P}\\big(|\\hat R_m(\\hat g)-R(\\hat g)|>\\varepsilon\\big)\\le 2e^{-2m\\varepsilon^2}\n",
    "\\]\n",
    "This matches the chapterâ€™s idea: **test risk concentrates** because the test set is independent of the fitted model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hoehffding_test_bound(m: int, eps: float) -> float:\n",
    "    \"\"\"2 * exp(-2 m eps^2) for bounded loss in [0,1].\"\"\"\n",
    "    if m <= 0 or eps <= 0:\n",
    "        raise ValueError(\"m>0 and eps>0 required.\")\n",
    "    return float(2 * math.exp(-2 * m * eps * eps))\n",
    "\n",
    "def required_test_size_for_eps_delta(eps: float, delta: float) -> int:\n",
    "    \"\"\"Small helper: choose m so that 2 exp(-2 m eps^2) <= delta.\"\"\"\n",
    "    if not (0 < delta < 1) or eps <= 0:\n",
    "        raise ValueError(\"delta in (0,1), eps>0.\")\n",
    "    m = math.log(2/delta) / (2 * eps * eps)\n",
    "    return int(math.ceil(m))\n",
    "\n",
    "# Example:\n",
    "eps, delta = 0.03, 0.05\n",
    "print(\"m needed for eps=0.03, delta=0.05:\", required_test_size_for_eps_delta(eps, delta))\n",
    "print(\"bound check:\", hoehffding_test_bound(required_test_size_for_eps_delta(eps, delta), eps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d985dc",
   "metadata": {},
   "source": [
    "## 8.1 Linear classifiers â€” Perceptron ðŸ§ \n",
    "\n",
    "A linear separator predicts with:\n",
    "- labels in {-1,+1}:  \\(\\hat y = \\mathrm{sign}(w^\\top x + b)\\)\n",
    "\n",
    "Perceptron update (as in notes, with augmented coordinate trick):\n",
    "- if \\(y_i (w^\\top x_i + b)\\le 0\\), update \\(w \\leftarrow w + y_i x_i\\), \\(b \\leftarrow b + y_i\\)\n",
    "\n",
    "Below is a clean reusable implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerceptronModel:\n",
    "    w: np.ndarray\n",
    "    b: float\n",
    "\n",
    "def perceptron_train(\n",
    "    X: np.ndarray,\n",
    "    y_pm1: np.ndarray,\n",
    "    max_epochs: int = 50,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[PerceptronModel, Dict[str, object]]:\n",
    "    \"\"\"Train a perceptron. y must be in {-1,+1}.\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = to_pm1(y_pm1).astype(int)\n",
    "    n, d = X.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    w = np.zeros(d, dtype=float)\n",
    "    b = 0.0\n",
    "    updates = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        idx = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng.shuffle(idx)\n",
    "        mistakes = 0\n",
    "        for i in idx:\n",
    "            margin = y[i] * (X[i] @ w + b)\n",
    "            if margin <= 0:\n",
    "                w = w + y[i] * X[i]\n",
    "                b = b + y[i]\n",
    "                updates += 1\n",
    "                mistakes += 1\n",
    "        if mistakes == 0:\n",
    "            return PerceptronModel(w=w, b=b), {\"converged\": True, \"epochs\": epoch+1, \"updates\": updates}\n",
    "    return PerceptronModel(w=w, b=b), {\"converged\": False, \"epochs\": max_epochs, \"updates\": updates}\n",
    "\n",
    "def perceptron_predict(model: PerceptronModel, X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    scores = X @ model.w + model.b\n",
    "    yhat = np.where(scores >= 0, 1, -1).astype(int)\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25cee26",
   "metadata": {},
   "source": [
    "### Demo: linearly separable vs non-separable\n",
    "\n",
    "- If separable, perceptron should converge (Theorem 8.2).\n",
    "- If not separable, it may cycle â†’ use a fixed epoch budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29114d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linearly_separable(n: int = 200, seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n, 2))\n",
    "    true_w = np.array([1.2, -0.7])\n",
    "    true_b = 0.1\n",
    "    y = np.where(X @ true_w + true_b >= 0, 1, -1)\n",
    "    return X, y\n",
    "\n",
    "def make_ring_nonseparable(n: int = 300, seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    angles = rng.uniform(0, 2*np.pi, size=n)\n",
    "    radii = rng.uniform(0.0, 4.0, size=n)\n",
    "    X = np.column_stack([radii*np.cos(angles), radii*np.sin(angles)])\n",
    "    y = np.where(radii <= 1.0, 1, -1)  # inner disk vs outer region (not linearly separable)\n",
    "    return X, y\n",
    "\n",
    "def plot_2d_points(X: np.ndarray, y_pm1: np.ndarray, title: str = \"\"):\n",
    "    y = to_pm1(y_pm1)\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], s=18, label=\"+1\")\n",
    "    plt.scatter(X[y==-1, 0], X[y==-1, 1], s=18, label=\"-1\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "X_sep, y_sep = make_linearly_separable()\n",
    "plot_2d_points(X_sep, y_sep, \"Linearly separable data\")\n",
    "\n",
    "model, info = perceptron_train(X_sep, y_sep, max_epochs=30, seed=0)\n",
    "yhat = perceptron_predict(model, X_sep)\n",
    "print(info, \"train risk:\", empirical_risk(y_sep, yhat))\n",
    "\n",
    "X_ring, y_ring = make_ring_nonseparable()\n",
    "plot_2d_points(X_ring, y_ring, \"Non-separable (ring) data\")\n",
    "\n",
    "model2, info2 = perceptron_train(X_ring, y_ring, max_epochs=30, seed=0)\n",
    "yhat2 = perceptron_predict(model2, X_ring)\n",
    "print(info2, \"train risk:\", empirical_risk(y_ring, yhat2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667af8f2",
   "metadata": {},
   "source": [
    "## 8.2 Kernelization ðŸ”§\n",
    "\n",
    "If data isnâ€™t linearly separable in input space, map features via \\(\\phi(x)\\) so it becomes separable in feature space.\n",
    "\n",
    "Key trick in the notes:\n",
    "- Perceptron weights become a combination of training points: \\(w=\\sum_i c_i \\phi(x_i)\\)\n",
    "- Predictions use only **kernel evaluations** \\(k(x_i,x)=\\phi(x_i)\\cdot \\phi(x)\\)\n",
    "\n",
    "This gives the **Kernel Perceptron**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Common kernels (use whichever you need)\n",
    "# -----------------------------\n",
    "\n",
    "def kernel_linear(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    return np.asarray(X) @ np.asarray(Y).T\n",
    "\n",
    "def kernel_poly(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0, r: float = 0.0, degree: int = 2) -> np.ndarray:\n",
    "    return (gamma * (np.asarray(X) @ np.asarray(Y).T) + r) ** degree\n",
    "\n",
    "def kernel_rbf_l2(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0) -> np.ndarray:\n",
    "    X = np.asarray(X); Y = np.asarray(Y)\n",
    "    X2 = np.sum(X*X, axis=1, keepdims=True)\n",
    "    Y2 = np.sum(Y*Y, axis=1, keepdims=True).T\n",
    "    d2 = X2 + Y2 - 2*(X @ Y.T)\n",
    "    return np.exp(-gamma * d2)\n",
    "\n",
    "def kernel_rbf_l1(X: np.ndarray, Y: np.ndarray, gamma: float = 1.0) -> np.ndarray:\n",
    "    X = np.asarray(X); Y = np.asarray(Y)\n",
    "    K = np.empty((X.shape[0], Y.shape[0]), dtype=float)\n",
    "    for i in range(X.shape[0]):\n",
    "        K[i, :] = np.exp(-gamma * np.sum(np.abs(Y - X[i]), axis=1))\n",
    "    return K\n",
    "\n",
    "def is_psd(K: np.ndarray, tol: float = 1e-10) -> bool:\n",
    "    \"\"\"Check symmetric PSD: eigenvalues >= -tol.\"\"\"\n",
    "    K = np.asarray(K, dtype=float)\n",
    "    if not np.allclose(K, K.T, atol=1e-8):\n",
    "        return False\n",
    "    eigs = np.linalg.eigvalsh(K)\n",
    "    return bool(np.min(eigs) >= -tol)\n",
    "\n",
    "def feature_map_from_gram(K: np.ndarray, tol: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Lemma 8.3 constructive: if K is PSD, build B s.t. K = B B^T, rows are phi(x_i).\"\"\"\n",
    "    K = np.asarray(K, dtype=float)\n",
    "    if not np.allclose(K, K.T, atol=1e-8):\n",
    "        raise ValueError(\"K must be symmetric.\")\n",
    "    eigvals, eigvecs = np.linalg.eigh(K)\n",
    "    eigvals = np.maximum(eigvals, 0.0)\n",
    "    keep = eigvals > tol\n",
    "    B = eigvecs[:, keep] * np.sqrt(eigvals[keep])\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bc017",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KernelPerceptronModel:\n",
    "    X_train: np.ndarray\n",
    "    y_train: np.ndarray   # {-1,+1}\n",
    "    alpha: np.ndarray     # integer-like update counts\n",
    "    kernel: Callable[[np.ndarray, np.ndarray], np.ndarray]\n",
    "\n",
    "def kernel_perceptron_train(\n",
    "    X: np.ndarray,\n",
    "    y_pm1: np.ndarray,\n",
    "    kernel: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "    max_epochs: int = 20,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[KernelPerceptronModel, Dict[str, object]]:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = to_pm1(y_pm1).astype(int)\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    alpha = np.zeros(n, dtype=float)\n",
    "\n",
    "    K = kernel(X, X)\n",
    "    updates = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        idx = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng.shuffle(idx)\n",
    "        mistakes = 0\n",
    "        for i in idx:\n",
    "            # f(x_i) = sum_j alpha_j y_j k(x_j, x_i)\n",
    "            score = np.sum(alpha * y * K[:, i])\n",
    "            if y[i] * score <= 0:\n",
    "                alpha[i] += 1.0\n",
    "                updates += 1\n",
    "                mistakes += 1\n",
    "        if mistakes == 0:\n",
    "            return KernelPerceptronModel(X, y, alpha, kernel), {\"converged\": True, \"epochs\": epoch+1, \"updates\": updates}\n",
    "    return KernelPerceptronModel(X, y, alpha, kernel), {\"converged\": False, \"epochs\": max_epochs, \"updates\": updates}\n",
    "\n",
    "def kernel_perceptron_predict(model: KernelPerceptronModel, X_new: np.ndarray) -> np.ndarray:\n",
    "    X_new = np.asarray(X_new, dtype=float)\n",
    "    K = model.kernel(model.X_train, X_new)  # shape (n_train, n_new)\n",
    "    scores = (model.alpha * model.y_train)[:, None] * K\n",
    "    scores = np.sum(scores, axis=0)\n",
    "    return np.where(scores >= 0, 1, -1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfbf6f",
   "metadata": {},
   "source": [
    "### Demo: kernel fixes the ring example\n",
    "\n",
    "Try:\n",
    "- Linear kernel (should struggle)\n",
    "- Polynomial kernel or RBF kernel (should work much better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce542ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernel perceptron on ring\n",
    "kp_lin, info_lin = kernel_perceptron_train(X_ring, y_ring, kernel=lambda A,B: kernel_linear(A,B), max_epochs=15, seed=0)\n",
    "pred_lin = kernel_perceptron_predict(kp_lin, X_ring)\n",
    "print(\"linear-kernel:\", info_lin, \"train risk:\", empirical_risk(y_ring, pred_lin))\n",
    "\n",
    "# RBF kernel perceptron on ring\n",
    "kp_rbf, info_rbf = kernel_perceptron_train(X_ring, y_ring, kernel=lambda A,B: kernel_rbf_l2(A,B,gamma=1.0), max_epochs=15, seed=0)\n",
    "pred_rbf = kernel_perceptron_predict(kp_rbf, X_ring)\n",
    "print(\"rbf-kernel:\", info_rbf, \"train risk:\", empirical_risk(y_ring, pred_rbf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7a5f6",
   "metadata": {},
   "source": [
    "## 8.3 Empirical Risk Minimization (ERM) + test-set guarantee âœ…\n",
    "\n",
    "Given a model class \\(M\\), ERM chooses:\n",
    "\\[\n",
    "\\hat g_n \\in \\arg\\min_{g\\in M}\\hat R_n(g)\n",
    "\\]\n",
    "\n",
    "Important: \\(\\hat R_n(\\hat g_n)\\) is typically **downward biased**, because you optimized it on the same data.\n",
    "\n",
    "If you have an independent test set, you can bound the gap between test risk and true risk using Hoeffding (earlier cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f94d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: np.ndarray, y: np.ndarray, test_size: float = 0.3, seed: int = 0):\n",
    "    X = np.asarray(X); y = np.asarray(y)\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n); rng.shuffle(idx)\n",
    "    m = int(round(n * test_size))\n",
    "    test_idx = idx[:m]; train_idx = idx[m:]\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# Example: evaluate perceptron with a test set + Hoeffding bound\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_sep, y_sep, test_size=0.3, seed=1)\n",
    "model, info = perceptron_train(Xtr, ytr, max_epochs=30, seed=0)\n",
    "pred_te = perceptron_predict(model, Xte)\n",
    "risk_te = empirical_risk(yte, pred_te)\n",
    "\n",
    "eps = 0.05\n",
    "print(\"test risk:\", risk_te)\n",
    "print(\"P(|Rhat - R| > eps) <=\", hoehffding_test_bound(len(yte), eps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f469575",
   "metadata": {},
   "source": [
    "## 8.3.2 Precision & Recall (and why edge cases are tricky) âš ï¸\n",
    "\n",
    "Definitions for class 1:\n",
    "- Precision = P(Y=1 | g(X)=1)\n",
    "- Recall    = P(g(X)=1 | Y=1)  (also called sensitivity)\n",
    "\n",
    "On a finite test set:\n",
    "- Precision uses the denominator **#predicted positive**\n",
    "- Recall uses the denominator **#actual positive**\n",
    "\n",
    "So if your model predicts all 0, precision is undefined; if positives are rare, recall estimation is noisy.\n",
    "Utilities below compute metrics safely and help you spot these cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e55d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_on_testset(y_true, y_pred, label_space: str = \"pm1\") -> Dict[str, float]:\n",
    "    \"\"\"label_space: 'pm1' for {-1,1}, '01' for {0,1}.\"\"\"\n",
    "    if label_space == \"pm1\":\n",
    "        y_true01 = to_01(to_pm1(y_true))\n",
    "        y_pred01 = to_01(to_pm1(y_pred))\n",
    "    elif label_space == \"01\":\n",
    "        y_true01 = np.asarray(y_true).astype(int)\n",
    "        y_pred01 = np.asarray(y_pred).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"label_space must be 'pm1' or '01'\")\n",
    "    return precision_recall_f1(y_true01, y_pred01)\n",
    "\n",
    "# Edge-case demos\n",
    "y_true01 = np.array([1,0,0,0,0,0,0,0,0,0])\n",
    "always1 = np.ones_like(y_true01)\n",
    "always0 = np.zeros_like(y_true01)\n",
    "print(\"always 1:\", metrics_on_testset(y_true01, always1, label_space=\"01\"))\n",
    "print(\"always 0:\", metrics_on_testset(y_true01, always0, label_space=\"01\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46010e",
   "metadata": {},
   "source": [
    "## 8.4 ERM over finitely many hyperplanes (Thm 8.16) â€” practical sampler ðŸŽ²\n",
    "\n",
    "The notes describe a finite class \\(M_n\\) built from **hyperplanes defined by choosing m points** (feature dimension \\(m\\)).\n",
    "There are \\(2\\binom{n}{m}\\) such rules (two sides of each hyperplane).\n",
    "\n",
    "In practice, \\(\\binom{n}{m}\\) is huge. For exams and coding, a good approach is:\n",
    "- **sample** many random m-tuples\n",
    "- build hyperplanes\n",
    "- pick the one with smallest training error\n",
    "\n",
    "Below is a reusable implementation for that idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperplane_from_d_points(points: np.ndarray, tol: float = 1e-12) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Build a hyperplane a^T x + b = 0 passing through d points in R^d.\n",
    "    Returns (a, b) up to scaling.\n",
    "    Uses nullspace of [points, 1].\n",
    "    \"\"\"\n",
    "    P = np.asarray(points, dtype=float)\n",
    "    d = P.shape[1]\n",
    "    if P.shape[0] != d:\n",
    "        raise ValueError(\"Need exactly d points in R^d to determine a hyperplane uniquely (up to scale).\")\n",
    "    A = np.hstack([P, np.ones((d, 1))])  # shape (d, d+1)\n",
    "    # Find vector v in nullspace of A (A v = 0). Nullspace dimension should be 1 in general position.\n",
    "    U, S, Vt = np.linalg.svd(A)\n",
    "    v = Vt[-1, :]  # smallest singular value vector\n",
    "    if np.linalg.norm(A @ v) > 1e-8:\n",
    "        raise RuntimeError(\"Failed to find a stable nullspace vector; points may be degenerate.\")\n",
    "    a = v[:d]\n",
    "    b = v[d]\n",
    "    # normalize for stability\n",
    "    norm = np.linalg.norm(a)\n",
    "    if norm < tol:\n",
    "        raise RuntimeError(\"Degenerate hyperplane (normal ~ 0).\")\n",
    "    a = a / norm\n",
    "    b = b / norm\n",
    "    return a, b\n",
    "\n",
    "def predict_hyperplane(a: np.ndarray, b: float, X: np.ndarray) -> np.ndarray:\n",
    "    scores = np.asarray(X, dtype=float) @ a + b\n",
    "    return np.where(scores >= 0, 1, 0).astype(int)  # {0,1} decision rule\n",
    "\n",
    "def sampled_hyperplane_erm(\n",
    "    X: np.ndarray,\n",
    "    y01: np.ndarray,\n",
    "    n_candidates: int = 2000,\n",
    "    seed: int = 0,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"Sample many d-tuples, build hyperplanes, pick best empirical-risk rule.\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y01 = np.asarray(y01).astype(int)\n",
    "    n, d = X.shape\n",
    "    if n < d:\n",
    "        raise ValueError(\"Need n >= d to sample d points.\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    best = {\"risk\": float(\"inf\"), \"a\": None, \"b\": None, \"flip\": 0}\n",
    "    for _ in range(n_candidates):\n",
    "        idx = rng.choice(n, size=d, replace=False)\n",
    "        try:\n",
    "            a, b = hyperplane_from_d_points(X[idx, :])\n",
    "        except Exception:\n",
    "            continue\n",
    "        pred = predict_hyperplane(a, b, X)\n",
    "        r = empirical_risk(y01, pred)\n",
    "        # also consider the opposite side (phi+ vs phi-)\n",
    "        pred_flip = 1 - pred\n",
    "        r2 = empirical_risk(y01, pred_flip)\n",
    "        if r2 < r:\n",
    "            r, pred = r2, pred_flip\n",
    "            flip = 1\n",
    "        else:\n",
    "            flip = 0\n",
    "        if r < best[\"risk\"]:\n",
    "            best = {\"risk\": float(r), \"a\": a, \"b\": float(b), \"flip\": flip}\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a63cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo in 2D (d=2): sample hyperplane ERM on separable data (convert labels to {0,1})\n",
    "best = sampled_hyperplane_erm(X_sep, to_01(y_sep), n_candidates=3000, seed=0)\n",
    "best[\"risk\"], best[\"a\"], best[\"b\"], best[\"flip\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa76f02",
   "metadata": {},
   "source": [
    "### Theorem 8.16 bound (as a function) ðŸ“\n",
    "\n",
    "The notes give a probability bound of the form:\n",
    "\\[\n",
    "\\mathbb{P}\\{R(\\hat\\phi) > \\inf_{M} R(\\phi) + \\varepsilon\\}\n",
    "\\le \\exp(2m\\varepsilon)\\,\\big(2\\binom{n}{m}+1\\big)\\,\\exp(-n\\varepsilon^2/2)\n",
    "\\]\n",
    "(valid under conditions like \\(2m/n \\le \\varepsilon \\le 1\\)).\n",
    "\n",
    "Below: compute it numerically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(n: int, k: int) -> int:\n",
    "    return math.comb(n, k)\n",
    "\n",
    "def theorem_816_bound(n: int, m_dim: int, eps: float) -> float:\n",
    "    if not (0 < eps <= 1):\n",
    "        raise ValueError(\"eps in (0,1]\")\n",
    "    # exp(2 m eps) * (2*C(n,m)+1) * exp(-n eps^2 / 2)\n",
    "    return float(math.exp(2*m_dim*eps) * (2*comb(n, m_dim) + 1) * math.exp(-n*(eps**2)/2))\n",
    "\n",
    "# Example numbers (small just for illustration)\n",
    "print(theorem_816_bound(n=200, m_dim=2, eps=0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae82cd",
   "metadata": {},
   "source": [
    "## 8.5â€“8.7 VC theory: shattering number and generalization bounds ðŸ“š\n",
    "\n",
    "The notes re-express uniform convergence as:\n",
    "\\[\n",
    "\\mathbb{P}\\Big(\\sup_{A\\in\\mathcal{A}}|\\nu_n(A)-\\nu(A)|>\\varepsilon\\Big)\n",
    "\\]\n",
    "\n",
    "Key objects:\n",
    "- **Shattering number** \\(s(\\mathcal{A},n)\\)\n",
    "- **VC dimension** \\(V_{\\mathcal{A}}\\)\n",
    "- **Sauerâ€“Shelah**: \\(s(\\mathcal{A},n)\\le \\sum_{i=0}^{V-1}\\binom{n}{i}\\)\n",
    "\n",
    "Theorem 8.29 gives:\n",
    "\\[\n",
    "\\mathbb{P}\\Big(\\sup_{A\\in\\mathcal{A}}|\\nu_n(A)-\\nu(A)|>\\varepsilon\\Big)\n",
    "\\le 8\\,s(\\mathcal{A},n)\\,e^{-n\\varepsilon^2/32}\n",
    "\\]\n",
    "(under \\(n\\varepsilon^2\\ge 2\\)).\n",
    "\n",
    "Corollary 8.31 adjusts constants for classifiers (under \\(n\\varepsilon^2\\ge 8\\)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sauer_shelah_upper(n: int, vc_dim: int) -> int:\n",
    "    \"\"\"Upper bound on shattering number via Sauerâ€“Shelah: sum_{i=0}^{V-1} C(n,i).\"\"\"\n",
    "    if vc_dim <= 0:\n",
    "        return 1\n",
    "    upper = 0\n",
    "    for i in range(0, min(vc_dim, n+1)):\n",
    "        upper += math.comb(n, i)\n",
    "    return upper\n",
    "\n",
    "def lemma_837_poly_upper(N: int, k: int) -> float:\n",
    "    \"\"\"Lemma 8.37 style: sum_{i=0}^{k-1} C(N,i) <= (eN/k)^k.\"\"\"\n",
    "    if k <= 0:\n",
    "        return 1.0\n",
    "    return float((math.e * N / k) ** k)\n",
    "\n",
    "def vc_generalization_bound_theorem_829(n: int, eps: float, shattering_number: float) -> float:\n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"eps>0\")\n",
    "    return float(8.0 * shattering_number * math.exp(-n * eps * eps / 32.0))\n",
    "\n",
    "def vc_generalization_bound_cor_831(n: int, eps: float, shattering_number: float) -> float:\n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"eps>0\")\n",
    "    return float(8.0 * shattering_number * math.exp(-n * eps * eps / 64.0))\n",
    "\n",
    "# Example: linear classifiers in R^m have VC-dim about m+1 (Lemma 8.38 informal)\n",
    "def bound_linear_classifiers(n: int, eps: float, m_dim: int) -> Dict[str, float]:\n",
    "    vc_dim = m_dim + 1\n",
    "    s_upper = sauer_shelah_upper(n, vc_dim)\n",
    "    return {\n",
    "        \"vc_dimâ‰ˆm+1\": vc_dim,\n",
    "        \"sauer_shelah_upper\": float(s_upper),\n",
    "        \"Thm8.29_bound\": vc_generalization_bound_theorem_829(n, eps, s_upper),\n",
    "        \"Cor8.31_bound\": vc_generalization_bound_cor_831(n, eps, s_upper),\n",
    "    }\n",
    "\n",
    "bound_linear_classifiers(n=200, eps=0.1, m_dim=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a58e74",
   "metadata": {},
   "source": [
    "## 8.8â€“8.9 Martingale concentration for ERM empirical risk (Thm 8.43) ðŸŽ²\n",
    "\n",
    "The notes show:\n",
    "\\[\n",
    "\\mathbb{P}\\Big(|\\hat R_n(\\phi_n^*) - \\mathbb{E}[\\hat R_n(\\phi_n^*)]| > \\varepsilon\\Big)\n",
    "< 2 e^{-n\\varepsilon^2/2}\n",
    "\\]\n",
    "\n",
    "This is an Azuma/Hoeffding-style concentration for the **random ERM training error** itself.\n",
    "\n",
    "Below is a helper that computes that bound.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def erm_training_risk_concentration_bound(n: int, eps: float) -> float:\n",
    "    \"\"\"Thm 8.43: 2 exp(-n eps^2 / 2).\"\"\"\n",
    "    if n <= 0 or eps <= 0:\n",
    "        raise ValueError(\"n>0, eps>0.\")\n",
    "    return float(2.0 * math.exp(-n * eps * eps / 2.0))\n",
    "\n",
    "print(erm_training_risk_concentration_bound(500, 0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4ad60",
   "metadata": {},
   "source": [
    "## âœ… Function index (copy/paste friendly)\n",
    "\n",
    "### Core risk + metrics\n",
    "- `empirical_risk(y_true, y_pred)`\n",
    "- `precision_recall_f1(y_true01, y_pred01)` / `metrics_on_testset(...)`\n",
    "- `train_test_split(X,y, test_size, seed)`\n",
    "- `hoehffding_test_bound(m, eps)` / `required_test_size_for_eps_delta(eps, delta)`\n",
    "\n",
    "### Linear classifiers\n",
    "- `perceptron_train(X, y_pm1, max_epochs, shuffle, seed)`\n",
    "- `perceptron_predict(model, X)`\n",
    "\n",
    "### Kernels + kernel perceptron\n",
    "- `kernel_linear`, `kernel_poly`, `kernel_rbf_l2`, `kernel_rbf_l1`\n",
    "- `is_psd(K)` and `feature_map_from_gram(K)` (Lemma 8.3 constructive)\n",
    "- `kernel_perceptron_train(X, y_pm1, kernel, ...)`\n",
    "- `kernel_perceptron_predict(model, X_new)`\n",
    "\n",
    "### Finite hyperplane ERM idea (Thm 8.16)\n",
    "- `hyperplane_from_d_points(points)`\n",
    "- `sampled_hyperplane_erm(X, y01, n_candidates, seed)`\n",
    "- `theorem_816_bound(n, m_dim, eps)`\n",
    "\n",
    "### VC theory bounds\n",
    "- `sauer_shelah_upper(n, vc_dim)`\n",
    "- `vc_generalization_bound_theorem_829(n, eps, s)`\n",
    "- `vc_generalization_bound_cor_831(n, eps, s)`\n",
    "- `bound_linear_classifiers(n, eps, m_dim)`\n",
    "\n",
    "### Martingale concentration (Thm 8.43)\n",
    "- `erm_training_risk_concentration_bound(n, eps)`\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
