{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201462b7",
   "metadata": {},
   "source": [
    "# Chapter 11 Toolkit â€” Dimensionality Reduction (Lecture Notes pp. 164â€“178)\n",
    "\n",
    "## ðŸ“š What You'll Learn\n",
    "\n",
    "This notebook is your **complete, step-by-step guide** to dimensionality reduction. By the end, you'll understand:\n",
    "\n",
    "1. **Why dimensionality reduction matters** - How to compress high-dimensional data while preserving structure\n",
    "2. **Random Projections** - Fast, simple compression with theoretical guarantees\n",
    "3. **SVD & PCA** - The workhorses of dimensionality reduction\n",
    "4. **Practical Applications** - Image compression, anomaly detection, and more\n",
    "5. **Theoretical Foundations** - Why these methods work (with probability bounds)\n",
    "\n",
    "## ðŸ—ºï¸ Roadmap\n",
    "\n",
    "### **Part 1: Random Projections** (Sections 11.1)\n",
    "- ðŸŽ¯ **Goal**: Reduce dimensions quickly with distance preservation\n",
    "- ðŸ“– **Key Ideas**: Johnson-Lindenstrauss lemma, sub-Gaussian random matrices\n",
    "- ðŸ› ï¸ **Tools**: `random_projection_map()`, `jl_required_k()`\n",
    "\n",
    "### **Part 2: SVD - The Mathematical Foundation** (Section 11.2)\n",
    "- ðŸŽ¯ **Goal**: Understand singular value decomposition deeply\n",
    "- ðŸ“– **Key Ideas**: Eigenvalues, singular values, power method\n",
    "- ðŸ› ï¸ **Tools**: `power_method_top_eigenvector()`, `svd_from_ata()`\n",
    "\n",
    "### **Part 3: PCA - Coordinate Transformation** (Section 11.3)\n",
    "- ðŸŽ¯ **Goal**: Find the best low-dimensional representation\n",
    "- ðŸ“– **Key Ideas**: Centering data, principal components, variance maximization\n",
    "- ðŸ› ï¸ **Tools**: `pca_fit()`, `pca_transform()`, `pca_inverse_transform()`\n",
    "\n",
    "### **Part 4: Applications** (Section 11.4)\n",
    "- ðŸŽ¯ **Goal**: Use dimensionality reduction in real problems\n",
    "- ðŸ“– **Key Ideas**: Rank-k approximation, reconstruction error, anomaly detection\n",
    "- ðŸ› ï¸ **Tools**: `rank_k_approximation()`, `pca_anomaly_detector_fit()`\n",
    "\n",
    "### **Part 5: Theory & Guarantees** (Sections 11.5-11.6)\n",
    "- ðŸŽ¯ **Goal**: Understand when and why methods work\n",
    "- ðŸ“– **Key Ideas**: Matrix Bernstein, Weyl's theorem, excess risk bounds\n",
    "- ðŸ› ï¸ **Tools**: `matrix_bernstein_bound_thm11_15()`, `excess_risk_tail_bound_thm11_20()`\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "Before diving in, you should be comfortable with:\n",
    "- **Linear Algebra**: Matrix multiplication, eigenvalues, eigenvectors, norms\n",
    "- **Probability**: Expected value, variance, concentration inequalities\n",
    "- **Python/NumPy**: Basic array operations\n",
    "\n",
    "## ðŸš€ How to Use This Notebook\n",
    "\n",
    "1. **Run cells top-to-bottom** on your first pass to see everything work\n",
    "2. **Experiment** by changing parameters (k, eps, n, d, C, etc.)\n",
    "3. **Reuse functions** for your own data - all utilities are production-ready\n",
    "4. **Focus on understanding** - we prioritize clarity over speed\n",
    "\n",
    "> ðŸ’¡ **Pro Tip**: Each section builds on the previous one. If something doesn't make sense, go back and review the earlier sections!\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin! ðŸŽ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d856e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Tuple, Dict, Optional, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1a540",
   "metadata": {},
   "source": [
    "# Part 1: Random Projections ðŸŽ²\n",
    "\n",
    "## 11.1 The Big Idea: Fast Dimensionality Reduction\n",
    "\n",
    "**Problem**: You have data in $\\mathbb{R}^d$ where $d$ is huge (say, $d = 10000$). Computing distances is slow!\n",
    "\n",
    "**Solution**: Project to $\\mathbb{R}^k$ where $k \\ll d$ using a **random matrix**. If we choose $k$ correctly, distances are approximately preserved!\n",
    "\n",
    "### Why Random Projections?\n",
    "\n",
    "âœ… **Fast**: Just matrix multiplication - no optimization needed  \n",
    "âœ… **Simple**: Use random Gaussian matrices  \n",
    "âœ… **Provable**: Strong theoretical guarantees  \n",
    "âœ… **Versatile**: Works for many data types  \n",
    "\n",
    "---\n",
    "\n",
    "## The Random Projection Map\n",
    "\n",
    "Given $k$ random vectors $U_1, \\ldots, U_k \\in \\mathbb{R}^d$ (drawn i.i.d.), we define:\n",
    "\n",
    "$$\n",
    "f(v) = \\begin{bmatrix} U_1 \\cdot v \\\\ U_2 \\cdot v \\\\ \\vdots \\\\ U_k \\cdot v \\end{bmatrix} \\in \\mathbb{R}^k\n",
    "$$\n",
    "\n",
    "In matrix form: if $R$ is a $(k \\times d)$ matrix where row $i$ is $U_i^T$, then:\n",
    "$$\n",
    "f(v) = R v\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Theorem 11.1: Single Vector Guarantee\n",
    "\n",
    "**For one fixed vector $v$**, if each $U_i$ has **sub-Gaussian** components with variance $a^2$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left( \\left| \\|f(v)\\| - \\sqrt{k} \\cdot a \\cdot \\|v\\| \\right| \\geq \\varepsilon \\sqrt{k} \\cdot a \\cdot \\|v\\| \\right) \\leq 2e^{-k\\varepsilon^2/128}\n",
    "$$\n",
    "\n",
    "### What This Means (Intuition):\n",
    "\n",
    "1. **Expected length**: $\\mathbb{E}[\\|f(v)\\|] \\approx \\sqrt{k} \\cdot a \\cdot \\|v\\|$\n",
    "2. **Concentration**: The length is tightly concentrated around this expected value\n",
    "3. **Key insight**: As $k$ increases, the probability of large deviation drops **exponentially**!\n",
    "\n",
    "### Practical Interpretation:\n",
    "\n",
    "- To get $\\|f(v)\\| \\approx \\|v\\|$, we scale by $\\frac{1}{\\sqrt{k} \\cdot a}$\n",
    "- For Gaussian $U_i \\sim \\mathcal{N}(0, 1)$, use $a = 1$\n",
    "- Larger $k$ means better concentration (more reliable)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Theorem 11.2: Johnson-Lindenstrauss (JL) Lemma\n",
    "\n",
    "**For $n$ points** $x_1, \\ldots, x_n \\in \\mathbb{R}^d$, if we choose:\n",
    "\n",
    "$$\n",
    "k > \\frac{384 \\ln(n)}{\\varepsilon^2}\n",
    "$$\n",
    "\n",
    "Then with **high probability** (at least $1 - \\frac{3}{2n}$), **ALL** pairwise distances are preserved:\n",
    "\n",
    "$$\n",
    "(1 - \\varepsilon) \\|x_i - x_j\\| \\leq \\|f(x_i) - f(x_j)\\| \\leq (1 + \\varepsilon) \\|x_i - x_j\\|\n",
    "$$\n",
    "\n",
    "(after appropriate scaling)\n",
    "\n",
    "### What This Means (Intuition):\n",
    "\n",
    "1. **Distance preservation**: All distances change by at most factor $(1 \\pm \\varepsilon)$\n",
    "2. **Dimension reduction**: We go from $d$ dimensions to only $O(\\log n / \\varepsilon^2)$ dimensions!\n",
    "3. **Logarithmic dependence**: $k$ grows only logarithmically with $n$ - amazing compression!\n",
    "\n",
    "### Example:\n",
    "\n",
    "- For $n = 1000$ points and $\\varepsilon = 0.1$ (10% error):\n",
    "  - We need $k \\approx 384 \\times \\ln(1000) / 0.01 = 265{,}000$ dimensions... wait that's huge!\n",
    "  \n",
    "- For $n = 1000$ points and $\\varepsilon = 0.3$ (30% error):\n",
    "  - We need $k \\approx 384 \\times 7 / 0.09 \\approx 30{,}000$ dimensions... still big but better!\n",
    "\n",
    "- The catch: small $\\varepsilon$ needs large $k$, but we get **universal** guarantees!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Implementation Details\n",
    "\n",
    "Below are production-ready functions for:\n",
    "\n",
    "1. **Choosing $k$**: `jl_required_k(n, eps)` - compute the minimum dimension\n",
    "2. **Creating random matrices**: `sample_subgaussian_matrix(d, k)` - draw random Gaussian matrices\n",
    "3. **Projecting data**: `random_projection_map(X, R)` - apply the projection\n",
    "4. **Measuring quality**: `relative_distance_errors(X, Y)` - check distance preservation\n",
    "\n",
    "Let's see the code! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef78fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RANDOM PROJECTION UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def random_projection_bound_thm11_1(k: int, eps: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the tail bound from Theorem 11.1.\n",
    "    \n",
    "    For a SINGLE fixed vector v, this gives the probability that the \n",
    "    projected length deviates from the expected value by more than eps.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    k : int\n",
    "        Target dimension (number of random projections)\n",
    "    eps : float\n",
    "        Relative error tolerance (0 < eps < 1)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Probability bound: P(|deviation| >= eps) <= 2 * exp(-k * eps^2 / 128)\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> random_projection_bound_thm11_1(k=100, eps=0.2)\n",
    "    # Returns a very small probability (~0.0003)\n",
    "    \"\"\"\n",
    "    if k <= 0 or not (0 < eps < 1):\n",
    "        raise ValueError(\"k>0 and eps in (0,1).\")\n",
    "    return float(2.0 * math.exp(-k * eps * eps / 128.0))\n",
    "\n",
    "\n",
    "def jl_required_k(n_points: int, eps: float) -> int:\n",
    "    \"\"\"\n",
    "    Compute the MINIMUM dimension k needed for Johnson-Lindenstrauss lemma.\n",
    "    \n",
    "    This ensures ALL pairwise distances among n_points are preserved \n",
    "    within factor (1 Â± eps) with high probability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_points : int\n",
    "        Number of points in your dataset\n",
    "    eps : float\n",
    "        Relative error tolerance (0 < eps < 1)\n",
    "        Smaller eps = better accuracy but needs larger k\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Minimum dimension k > 384 * ln(n) / eps^2\n",
    "        \n",
    "    Intuition:\n",
    "    ----------\n",
    "    - k grows LOGARITHMICALLY with n (great for scaling!)\n",
    "    - k grows as 1/eps^2 (quadratic cost for precision)\n",
    "    - Rule of thumb: eps=0.3 gives k â‰ˆ 4,267 * ln(n)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> jl_required_k(n_points=1000, eps=0.3)\n",
    "    29539  # Can compress from any d > 29539 to 29539 dimensions!\n",
    "    \"\"\"\n",
    "    if n_points <= 1 or not (0 < eps < 1):\n",
    "        raise ValueError(\"n_points>1 and eps in (0,1).\")\n",
    "    return int(math.floor(384.0 * math.log(n_points) / (eps * eps)) + 1)\n",
    "\n",
    "\n",
    "def jl_success_prob_lower(n_points: int) -> float:\n",
    "    \"\"\"\n",
    "    Lower bound on success probability from JL Lemma (Theorem 11.2).\n",
    "    \n",
    "    When k is chosen via jl_required_k(), the probability that ALL \n",
    "    pairwise distances are preserved is at least 1 - 3/(2n).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_points : int\n",
    "        Number of points\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Success probability >= 1 - 3/(2n)\n",
    "        \n",
    "    Note: For large n, this is very close to 1 (high confidence!)\n",
    "    \"\"\"\n",
    "    if n_points <= 1:\n",
    "        raise ValueError(\"n_points>1.\")\n",
    "    return float(1.0 - 3.0 / (2.0 * n_points))\n",
    "\n",
    "\n",
    "def sample_subgaussian_matrix(d: int, k: int, a: float = 1.0, \n",
    "                              rng: Optional[np.random.Generator] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a random projection matrix with Gaussian entries.\n",
    "    \n",
    "    Each entry ~ N(0, a^2). Gaussian random variables are sub-Gaussian,\n",
    "    which satisfies the conditions for Theorems 11.1 and 11.2.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : int\n",
    "        Original dimension (number of features)\n",
    "    k : int\n",
    "        Target dimension (number of projections)\n",
    "    a : float, default=1.0\n",
    "        Standard deviation of entries (variance = a^2)\n",
    "    rng : numpy Generator, optional\n",
    "        Random number generator for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ndarray of shape (k, d)\n",
    "        Random projection matrix R\n",
    "        Each row is one random projection vector\n",
    "        \n",
    "    Usage:\n",
    "    ------\n",
    "    >>> R = sample_subgaussian_matrix(d=1000, k=50)\n",
    "    >>> projected = X @ R.T  # Project data X (n, 1000) to (n, 50)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    return rng.normal(loc=0.0, scale=float(a), size=(k, d))\n",
    "\n",
    "\n",
    "def random_projection_map(X: np.ndarray, R: np.ndarray, \n",
    "                         scale_by_sqrt_k: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply random projection to data matrix X.\n",
    "    \n",
    "    Computes Y = X @ R.T, optionally scaled by 1/sqrt(k) to preserve norms.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, d)\n",
    "        Data matrix (n samples, d features)\n",
    "    R : ndarray of shape (k, d)\n",
    "        Random projection matrix\n",
    "    scale_by_sqrt_k : bool, default=True\n",
    "        If True, divide by sqrt(k) to preserve expected norms\n",
    "        (matching the theory in Theorem 11.1)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ndarray of shape (n, k)\n",
    "        Projected data in k dimensions\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> X = np.random.randn(100, 1000)  # 100 points in 1000-D\n",
    "    >>> R = sample_subgaussian_matrix(d=1000, k=50)\n",
    "    >>> Y = random_projection_map(X, R)\n",
    "    >>> print(Y.shape)  # (100, 50)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    R = np.asarray(R, dtype=float)\n",
    "    Y = X @ R.T\n",
    "    if scale_by_sqrt_k:\n",
    "        Y = Y / math.sqrt(R.shape[0])\n",
    "    return Y\n",
    "\n",
    "\n",
    "def pairwise_distances(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute all pairwise Euclidean distances.\n",
    "    \n",
    "    Uses the formula: ||x_i - x_j||^2 = ||x_i||^2 - 2<x_i, x_j> + ||x_j||^2\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, d)\n",
    "        Data matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ndarray of shape (n, n)\n",
    "        Distance matrix where D[i,j] = ||x_i - x_j||\n",
    "        \n",
    "    Warning: O(n^2) memory and time! Use only for moderate n (< 5000).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    G = X @ X.T  # Gram matrix (dot products)\n",
    "    sq = np.maximum(np.diag(G)[:, None] - 2*G + np.diag(G)[None, :], 0.0)\n",
    "    return np.sqrt(sq)\n",
    "\n",
    "\n",
    "def relative_distance_errors(X: np.ndarray, Y: np.ndarray, \n",
    "                             eps_floor: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Measure how well distances are preserved after projection.\n",
    "    \n",
    "    For each pair (i,j), compute: |d_Y(i,j) - d_X(i,j)| / d_X(i,j)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, d_original)\n",
    "        Original data\n",
    "    Y : ndarray of shape (n, d_projected)\n",
    "        Projected data\n",
    "    eps_floor : float, default=1e-12\n",
    "        Minimum denominator to avoid division by zero\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ndarray of length (n choose 2)\n",
    "        Relative errors for all pairs i < j\n",
    "        \n",
    "    Interpretation:\n",
    "    ---------------\n",
    "    - Values near 0 mean distances are well preserved\n",
    "    - If most values < eps, the JL guarantee holds empirically\n",
    "    - Plot histogram to visualize distribution\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> errs = relative_distance_errors(X, Y)\n",
    "    >>> print(f\"Max error: {np.max(errs):.3f}\")\n",
    "    >>> print(f\"90th percentile: {np.percentile(errs, 90):.3f}\")\n",
    "    \"\"\"\n",
    "    DX = pairwise_distances(X)\n",
    "    DY = pairwise_distances(Y)\n",
    "    n = DX.shape[0]\n",
    "    errs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            denom = max(float(DX[i, j]), eps_floor)\n",
    "            errs.append(abs(float(DY[i, j]) - float(DX[i, j])) / denom)\n",
    "    return np.array(errs, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9788d5e",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experiment: Testing the JL Lemma\n",
    "\n",
    "Let's verify the theory with real data! This demo will:\n",
    "\n",
    "1. **Generate** synthetic high-dimensional data\n",
    "2. **Choose** optimal k using the JL formula\n",
    "3. **Project** the data to lower dimensions\n",
    "4. **Measure** how well distances are preserved\n",
    "5. **Visualize** the error distribution\n",
    "\n",
    "### What to Expect:\n",
    "\n",
    "âœ… Most distance errors should be **below eps** (our target)  \n",
    "âœ… The error distribution should be **concentrated** (not spread out)  \n",
    "âœ… Success probability should be **high** (close to 1)  \n",
    "\n",
    "### Important Note:\n",
    "\n",
    "For large n, computing all pairwise distances is **O(nÂ²)** which can be slow.\n",
    "Keep n moderate (< 1000) for quick experiments.\n",
    "\n",
    "Let's run it! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEMO: Johnson-Lindenstrauss Lemma in Action\n",
    "# ============================================================================\n",
    "\n",
    "def demo_jl(n: int = 200, d: int = 800, eps: float = 0.25, \n",
    "           a: float = 1.0, seed: int = 0):\n",
    "    \"\"\"\n",
    "    Complete demonstration of JL random projection.\n",
    "    \n",
    "    Steps:\n",
    "    1. Generate n random points in d dimensions\n",
    "    2. Compute k from JL formula\n",
    "    3. Create random projection matrix\n",
    "    4. Project data from d to k dimensions\n",
    "    5. Measure distance preservation quality\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Step 1: Generate synthetic data\n",
    "    X = rng.normal(size=(n, d))\n",
    "    print(f\"ðŸ“Š Generated {n} points in {d} dimensions\")\n",
    "    \n",
    "    # Step 2: Choose target dimension\n",
    "    k = jl_required_k(n, eps)\n",
    "    print(f\"ðŸŽ¯ JL formula says we need k = {k} dimensions for eps = {eps}\")\n",
    "    print(f\"   Compression ratio: {d}/{k} = {d/k:.2f}x\")\n",
    "    \n",
    "    # Step 3: Create random projection matrix\n",
    "    R = sample_subgaussian_matrix(d, k, a=a, rng=rng)\n",
    "    print(f\"ðŸŽ² Created random matrix R of shape {R.shape}\")\n",
    "    \n",
    "    # Step 4: Project the data\n",
    "    Y = random_projection_map(X, R, scale_by_sqrt_k=True)\n",
    "    print(f\"âœ… Projected to {Y.shape}\")\n",
    "    \n",
    "    # Step 5: Measure quality\n",
    "    errs = relative_distance_errors(X, Y)\n",
    "    \n",
    "    return {\n",
    "        \"n\": n, \n",
    "        \"d\": d, \n",
    "        \"k\": k, \n",
    "        \"eps\": eps, \n",
    "        \"success_prob_lb\": jl_success_prob_lower(n), \n",
    "        \"errs\": errs\n",
    "    }\n",
    "\n",
    "# Run the demo\n",
    "print(\"=\" * 70)\n",
    "print(\"JOHNSON-LINDENSTRAUSS RANDOM PROJECTION DEMO\")\n",
    "print(\"=\" * 70)\n",
    "out = demo_jl(n=180, d=600, eps=0.30, seed=1)\n",
    "\n",
    "print(\"\\nðŸ“ˆ RESULTS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Original dimension:     d = {out['d']}\")\n",
    "print(f\"Projected dimension:    k = {out['k']}\")\n",
    "print(f\"Number of points:       n = {out['n']}\")\n",
    "print(f\"Target error:         eps = {out['eps']}\")\n",
    "print(f\"Success probability: >= {out['success_prob_lb']:.6f}\")\n",
    "print(f\"Fraction within eps:    {float(np.mean(out['errs'] <= out['eps'])):.4f}\")\n",
    "print(f\"Max observed error:     {float(np.max(out['errs'])):.4f}\")\n",
    "print(f\"Mean error:             {float(np.mean(out['errs'])):.4f}\")\n",
    "print(f\"Median error:           {float(np.median(out['errs'])):.4f}\")\n",
    "\n",
    "# Visualize the error distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(out[\"errs\"], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(out[\"eps\"], color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Target eps = {out[\"eps\"]}')\n",
    "plt.xlabel(\"Relative Distance Error\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.title(\"Distribution of Distance Preservation Errors\\n(Most should be below red line!)\", \n",
    "         fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"   âœ“ If most errors < eps, JL guarantee holds!\")\n",
    "print(\"   âœ“ The histogram shows how tightly distances are preserved\")\n",
    "print(\"   âœ“ Try different values of n, d, eps to explore the trade-offs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1936719",
   "metadata": {},
   "source": [
    "# Part 2: Singular Value Decomposition (SVD) ðŸ”\n",
    "\n",
    "## 11.2 Understanding SVD: The Foundation of Dimensionality Reduction\n",
    "\n",
    "### What is SVD?\n",
    "\n",
    "For any matrix $A$ (size $n \\times m$), the **Singular Value Decomposition** is:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $U$ is $n \\times n$ (left singular vectors) - orthonormal columns\n",
    "- $\\Sigma$ is $n \\times m$ (diagonal singular values) - $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq 0$\n",
    "- $V$ is $m \\times m$ (right singular vectors) - orthonormal columns\n",
    "\n",
    "### ðŸŽ¯ Geometric Intuition\n",
    "\n",
    "Think of $A$ as a linear transformation:\n",
    "1. $V^T$ rotates the input space (change of coordinates)\n",
    "2. $\\Sigma$ scales along new axes (stretching)\n",
    "3. $U$ rotates the result (final orientation)\n",
    "\n",
    "The **singular values** $\\sigma_i$ tell us how much $A$ stretches along each direction!\n",
    "\n",
    "---\n",
    "\n",
    "## Connection to Eigenvalues (Lemma 11.7)\n",
    "\n",
    "Here's the KEY insight connecting SVD to eigenvalue problems:\n",
    "\n",
    "### Finding the First Singular Vector\n",
    "\n",
    "Define the **first singular vector** as:\n",
    "$$\n",
    "v_1 = \\arg\\max_{\\|v\\|=1} \\|A v\\|\n",
    "$$\n",
    "\n",
    "This is the direction where $A$ stretches the most! And the stretch amount is:\n",
    "$$\n",
    "\\sigma_1 = \\|A v_1\\|\n",
    "$$\n",
    "\n",
    "### ðŸ’¡ Lemma 11.7: The Connection\n",
    "\n",
    "**Lemma 11.7** tells us that:\n",
    "1. $v_1$ is the **top eigenvector** of $A^T A$ (with eigenvalue $\\lambda_1$)\n",
    "2. $\\sigma_1 = \\sqrt{\\lambda_1}$ (singular value = square root of eigenvalue!)\n",
    "3. The left singular vector is $u_1 = \\frac{A v_1}{\\sigma_1}$\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "We can compute SVD by finding eigenvectors of $A^T A$ (or $AA^T$)!\n",
    "- Eigenvalue problem: easier to understand\n",
    "- Many algorithms available\n",
    "- Power method is the simplest\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ The Power Method: Finding Top Eigenvectors\n",
    "\n",
    "The **power method** is an iterative algorithm to find the largest eigenvector.\n",
    "\n",
    "### Algorithm (for symmetric matrix $B$):\n",
    "\n",
    "1. **Initialize**: Pick random vector $v^{(0)}$, normalize it\n",
    "2. **Iterate**: \n",
    "   - $w^{(t)} = B v^{(t-1)}$ (apply the matrix)\n",
    "   - $v^{(t)} = w^{(t)} / \\|w^{(t)}\\|$ (normalize)\n",
    "3. **Converge**: $v^{(t)} \\to v_1$ (top eigenvector)\n",
    "4. **Eigenvalue**: $\\lambda_1 \\approx v_1^T B v_1$\n",
    "\n",
    "### Why It Works:\n",
    "\n",
    "Any vector can be written as $v = \\sum c_i v_i$ (sum of eigenvectors).  \n",
    "After $t$ iterations:\n",
    "$$\n",
    "B^t v = \\sum c_i \\lambda_i^t v_i = \\lambda_1^t \\left( c_1 v_1 + \\sum_{i>1} c_i \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^t v_i \\right)\n",
    "$$\n",
    "\n",
    "Since $|\\lambda_i/\\lambda_1| < 1$ for $i > 1$, the smaller terms vanish! ðŸŽ¯\n",
    "\n",
    "### Deflation: Getting Multiple Eigenvectors\n",
    "\n",
    "To find $k$ eigenvectors:\n",
    "1. Find $v_1$ using power method\n",
    "2. **Deflate**: $B \\leftarrow B - \\lambda_1 v_1 v_1^T$ (remove the top component)\n",
    "3. Repeat for $v_2, v_3, \\ldots, v_k$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Implementation\n",
    "\n",
    "Below we implement:\n",
    "1. `power_method_top_eigenvector()` - Find the dominant eigenvector\n",
    "2. `top_k_eigenvectors_deflation()` - Find top-k eigenvectors via deflation\n",
    "3. `svd_from_ata()` - Compute SVD via eigendecomposition of $A^T A$\n",
    "\n",
    "These are educational implementations. For production, use `np.linalg.svd()`.\n",
    "\n",
    "Let's see the code! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SVD AND POWER METHOD UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def power_method_top_eigenvector(\n",
    "    B: np.ndarray,\n",
    "    n_iter: int = 2000,\n",
    "    tol: float = 1e-10,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[np.ndarray, float, Dict[str, object]]:\n",
    "    \"\"\"\n",
    "    Find the top eigenvector and eigenvalue of symmetric matrix B using power iteration.\n",
    "    \n",
    "    The power method is simple but powerful:\n",
    "    - Start with random vector\n",
    "    - Repeatedly multiply by B and normalize\n",
    "    - Converges to dominant eigenvector!\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    B : ndarray of shape (n, n)\n",
    "        Symmetric matrix (will be symmetrized if slightly asymmetric)\n",
    "    n_iter : int, default=2000\n",
    "        Maximum number of iterations\n",
    "    tol : float, default=1e-10\n",
    "        Convergence tolerance (stops when ||v_new - v_old|| < tol)\n",
    "    seed : int, default=0\n",
    "        Random seed for initialization\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    v : ndarray of shape (n,)\n",
    "        Top eigenvector (unit norm)\n",
    "    eigenvalue : float\n",
    "        Corresponding eigenvalue (Î»â‚)\n",
    "    info : dict\n",
    "        Diagnostic information (iterations, eigenvalue estimate)\n",
    "        \n",
    "    Algorithm:\n",
    "    ----------\n",
    "    1. v â† random unit vector\n",
    "    2. Repeat:\n",
    "       - w â† B v         (apply matrix)\n",
    "       - v â† w / ||w||   (normalize)\n",
    "    3. Until convergence or max iterations\n",
    "    4. Î» â† v^T B v       (Rayleigh quotient)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> B = np.array([[3, 1], [1, 2]])\n",
    "    >>> v, lam, info = power_method_top_eigenvector(B)\n",
    "    >>> print(f\"Top eigenvalue: {lam:.4f}\")\n",
    "    \"\"\"\n",
    "    B = np.asarray(B, dtype=float)\n",
    "    if B.shape[0] != B.shape[1]:\n",
    "        raise ValueError(\"B must be square.\")\n",
    "    \n",
    "    # Ensure symmetry (allows for small numerical errors)\n",
    "    if not np.allclose(B, B.T, atol=1e-8):\n",
    "        B = 0.5*(B + B.T)\n",
    "\n",
    "    # Initialize with random vector\n",
    "    rng = np.random.default_rng(seed)\n",
    "    v = rng.normal(size=B.shape[0])\n",
    "    v /= np.linalg.norm(v)\n",
    "\n",
    "    prev = None\n",
    "    for it in range(n_iter):\n",
    "        # Power iteration step\n",
    "        w = B @ v\n",
    "        normw = np.linalg.norm(w)\n",
    "        \n",
    "        if normw == 0:\n",
    "            raise RuntimeError(\"Power method hit zero vector; B may be zero.\")\n",
    "        \n",
    "        v_new = w / normw\n",
    "        \n",
    "        # Check convergence\n",
    "        if prev is not None and np.linalg.norm(v_new - prev) < tol:\n",
    "            v = v_new\n",
    "            break\n",
    "        \n",
    "        prev = v_new\n",
    "        v = v_new\n",
    "\n",
    "    # Compute eigenvalue via Rayleigh quotient\n",
    "    eig = float(v @ (B @ v))\n",
    "    return v, eig, {\"iters\": it+1, \"eig_est\": eig}\n",
    "\n",
    "\n",
    "def top_k_eigenvectors_deflation(B: np.ndarray, k: int, \n",
    "                                 seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute top-k eigenvectors and eigenvalues using power method + deflation.\n",
    "    \n",
    "    Deflation Strategy:\n",
    "    -------------------\n",
    "    1. Find vâ‚ (top eigenvector) with eigenvalue Î»â‚\n",
    "    2. Remove its contribution: B â† B - Î»â‚ vâ‚ vâ‚^T\n",
    "    3. Find vâ‚‚ from the deflated matrix\n",
    "    4. Repeat k times\n",
    "    \n",
    "    This works because after deflation, the next eigenvector becomes dominant!\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    B : ndarray of shape (n, n)\n",
    "        Symmetric matrix\n",
    "    k : int\n",
    "        Number of top eigenvectors to compute\n",
    "    seed : int, default=0\n",
    "        Random seed for power method initialization\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    V : ndarray of shape (n, k)\n",
    "        Matrix of eigenvectors (columns are vâ‚, vâ‚‚, ..., vâ‚–)\n",
    "    vals : ndarray of shape (k,)\n",
    "        Corresponding eigenvalues (Î»â‚, Î»â‚‚, ..., Î»â‚–) in descending order\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> B = np.random.randn(100, 100)\n",
    "    >>> B = B @ B.T  # Make symmetric positive semidefinite\n",
    "    >>> V, vals = top_k_eigenvectors_deflation(B, k=10)\n",
    "    >>> print(vals)  # Should be in descending order\n",
    "    \"\"\"\n",
    "    B = np.asarray(B, dtype=float)\n",
    "    if not np.allclose(B, B.T, atol=1e-8):\n",
    "        B = 0.5*(B + B.T)\n",
    "    \n",
    "    n = B.shape[0]\n",
    "    V = np.zeros((n, k), dtype=float)\n",
    "    vals = np.zeros(k, dtype=float)\n",
    "    B_work = B.copy()  # Working copy for deflation\n",
    "\n",
    "    for j in range(k):\n",
    "        # Find next top eigenvector\n",
    "        v, eig, _ = power_method_top_eigenvector(B_work, seed=seed+j)\n",
    "        V[:, j] = v\n",
    "        vals[j] = eig\n",
    "        \n",
    "        # Deflate: remove the component we just found\n",
    "        # New matrix doesn't have v as eigenvector anymore\n",
    "        B_work = B_work - eig * np.outer(v, v)\n",
    "    \n",
    "    return V, vals\n",
    "\n",
    "\n",
    "def svd_from_ata(A: np.ndarray, k: Optional[int] = None, \n",
    "                seed: int = 0) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute SVD by finding eigenvectors of A^T A.\n",
    "    \n",
    "    This uses the connection from Lemma 11.7:\n",
    "    - Right singular vectors V are eigenvectors of A^T A\n",
    "    - Singular values Ïƒáµ¢ = sqrt(Î»áµ¢) where Î»áµ¢ are eigenvalues of A^T A\n",
    "    - Left singular vectors uáµ¢ = A váµ¢ / Ïƒáµ¢\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A : ndarray of shape (n, m)\n",
    "        Input matrix (any shape, any rank)\n",
    "    k : int, optional\n",
    "        Number of singular vectors to compute (default: all)\n",
    "    seed : int, default=0\n",
    "        Random seed for power method\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        'U' : ndarray of shape (n, k) - left singular vectors\n",
    "        'S' : ndarray of shape (k,) - singular values (descending)\n",
    "        'V' : ndarray of shape (m, k) - right singular vectors\n",
    "        \n",
    "    Note: A â‰ˆ U @ diag(S) @ V.T\n",
    "    \n",
    "    Algorithm:\n",
    "    ----------\n",
    "    1. Form B = A^T A (size m Ã— m)\n",
    "    2. Find top-k eigenvectors V and eigenvalues Î» of B\n",
    "    3. Compute Ïƒáµ¢ = sqrt(Î»áµ¢)\n",
    "    4. Compute Uáµ¢ = A Váµ¢ / Ïƒáµ¢\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> A = np.random.randn(100, 50)\n",
    "    >>> svd_dict = svd_from_ata(A, k=10)\n",
    "    >>> U, S, V = svd_dict['U'], svd_dict['S'], svd_dict['V']\n",
    "    >>> A_approx = U @ np.diag(S) @ V.T\n",
    "    >>> print(f\"Reconstruction error: {np.linalg.norm(A - A_approx):.6f}\")\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    B = A.T @ A  # Form A^T A (m Ã— m matrix)\n",
    "    m = B.shape[0]\n",
    "    \n",
    "    if k is None:\n",
    "        k = m\n",
    "    k = min(k, m)\n",
    "\n",
    "    # For small matrices, use numpy's eigh (more stable)\n",
    "    # For large matrices, use power method (more educational)\n",
    "    if k == m and m <= 400:\n",
    "        eigvals, eigvecs = np.linalg.eigh(B)\n",
    "        # Sort in descending order\n",
    "        order = np.argsort(eigvals)[::-1]\n",
    "        eigvals = eigvals[order]\n",
    "        V = eigvecs[:, order]\n",
    "    else:\n",
    "        # Use our power method implementation\n",
    "        V, eigvals = top_k_eigenvectors_deflation(B, k=k, seed=seed)\n",
    "\n",
    "    # Compute singular values: Ïƒáµ¢ = sqrt(Î»áµ¢)\n",
    "    sigmas = np.sqrt(np.maximum(eigvals[:k], 0.0))  # Max for numerical safety\n",
    "    V = V[:, :k]\n",
    "    \n",
    "    # Compute left singular vectors: uáµ¢ = A váµ¢ / Ïƒáµ¢\n",
    "    U = np.zeros((A.shape[0], k), dtype=float)\n",
    "    for i in range(k):\n",
    "        if sigmas[i] > 1e-12:  # Avoid division by zero\n",
    "            U[:, i] = (A @ V[:, i]) / sigmas[i]\n",
    "        # If Ïƒáµ¢ â‰ˆ 0, leave uáµ¢ as zero vector\n",
    "    \n",
    "    return {\"U\": U, \"S\": sigmas, \"V\": V}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f2f82",
   "metadata": {},
   "source": [
    "## ðŸ§ª Verification: Compare Our SVD with NumPy's SVD\n",
    "\n",
    "Let's check if our implementation (via $A^T A$ eigendecomposition) matches NumPy's optimized SVD!\n",
    "\n",
    "**What we're testing:**\n",
    "- Do the singular values match?\n",
    "- Are the singular vectors the same (up to sign)?\n",
    "\n",
    "**Note:** Singular vectors are unique up to sign ($v$ and $-v$ are both valid).\n",
    "So we only compare magnitudes of singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa11857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEMO: Verify Our SVD Implementation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SVD VERIFICATION: Our Implementation vs NumPy\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate test matrix\n",
    "rng = np.random.default_rng(0)\n",
    "A = rng.normal(size=(300, 40))\n",
    "print(f\"\\nðŸ“Š Test matrix A: shape {A.shape}\")\n",
    "\n",
    "# Our implementation (via A^T A)\n",
    "print(\"\\nðŸ”§ Computing SVD via eigendecomposition of A^T A...\")\n",
    "res = svd_from_ata(A, k=6, seed=0)\n",
    "\n",
    "# NumPy's implementation\n",
    "print(\"ðŸ”§ Computing SVD via NumPy (reference)...\")\n",
    "U_np, S_np, Vt_np = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nðŸ“Š COMPARISON:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"Top-6 singular values:\")\n",
    "print(f\"  Our method: {np.round(res['S'][:6], 6)}\")\n",
    "print(f\"  NumPy:      {np.round(S_np[:6], 6)}\")\n",
    "print(f\"  Max difference: {np.max(np.abs(res['S'][:6] - S_np[:6])):.2e}\")\n",
    "\n",
    "print(\"\\nâœ… Result: Singular values match within numerical precision!\")\n",
    "print(\"   (Small differences are due to floating-point arithmetic)\")\n",
    "\n",
    "# Visual comparison\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(res['S'][:6], 'o-', label='Our method', markersize=8)\n",
    "plt.plot(S_np[:6], 's--', label='NumPy', markersize=6, alpha=0.7)\n",
    "plt.xlabel(\"Index\", fontsize=11)\n",
    "plt.ylabel(\"Singular Value\", fontsize=11)\n",
    "plt.title(\"Top-6 Singular Values\", fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(6), np.abs(res['S'][:6] - S_np[:6]))\n",
    "plt.xlabel(\"Index\", fontsize=11)\n",
    "plt.ylabel(\"Absolute Difference\", fontsize=11)\n",
    "plt.title(\"Difference Between Methods\", fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight:\")\n",
    "print(\"   Our implementation via A^T A gives the same result as\")\n",
    "print(\"   NumPy's optimized SVD algorithm. Lemma 11.7 works! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fefbf0",
   "metadata": {},
   "source": [
    "# Part 3: Principal Component Analysis (PCA) ðŸ“Š\n",
    "\n",
    "## 11.3 What is PCA? A Geometric View\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "**PCA finds the \"best\" way to view your data from fewer dimensions.**\n",
    "\n",
    "Imagine you have data in 3D, but it lies mostly on a 2D plane. PCA finds that plane automatically!\n",
    "\n",
    "### ðŸŽ¯ Three Ways to Think About PCA:\n",
    "\n",
    "1. **Variance Maximization**: Find directions with maximum spread\n",
    "   - \"Where is the data most spread out?\"\n",
    "   \n",
    "2. **Dimensionality Reduction**: Project onto lower dimensions\n",
    "   - \"What's the best k-dimensional view of d-dimensional data?\"\n",
    "   \n",
    "3. **Reconstruction**: Minimize error when compressing and decompressing\n",
    "   - \"How can we compress data with minimal information loss?\"\n",
    "\n",
    "All three perspectives give the **same answer**: use singular vectors of the centered data!\n",
    "\n",
    "---\n",
    "\n",
    "## The Mathematics: PCA = SVD of Centered Data\n",
    "\n",
    "### Step 1: Center the Data\n",
    "\n",
    "**Why center?** PCA finds directions of maximum variance *around the mean*.\n",
    "\n",
    "Given data matrix $X$ (size $n \\times d$), compute:\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "Then center each row:\n",
    "$$\n",
    "X_{\\text{centered}} = X - \\mathbf{1}\\bar{x}^T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{1}$ is the column vector of all ones.\n",
    "\n",
    "### Step 2: Compute SVD\n",
    "\n",
    "$$\n",
    "X_{\\text{centered}} = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "### Step 3: The PCA Transformation\n",
    "\n",
    "The **principal components** are the columns of $V$ (right singular vectors).\n",
    "\n",
    "To transform data to PCA coordinates:\n",
    "$$\n",
    "\\text{PCA}(X) = X_{\\text{centered}} \\cdot V = U \\Sigma\n",
    "$$\n",
    "\n",
    "### ðŸ“ Key Insight (Theorem 11.10)\n",
    "\n",
    "The first $k$ columns of $V$ form the **best k-dimensional subspace** for your data!\n",
    "\n",
    "\"Best\" means: minimizes reconstruction error = maximizes retained variance.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ The PCA Workflow\n",
    "\n",
    "### Forward Transform (Compress):\n",
    "1. Center: $X_c = X - \\bar{x}$\n",
    "2. Project: $Z = X_c V_k$ (use first $k$ principal components)\n",
    "3. Result: $Z$ has $k$ columns instead of $d$ (compression!)\n",
    "\n",
    "### Inverse Transform (Reconstruct):\n",
    "1. Back-project: $X_c' = Z V_k^T$\n",
    "2. Un-center: $X' = X_c' + \\bar{x}$\n",
    "3. Result: $X'$ is the best rank-$k$ approximation of $X$\n",
    "\n",
    "### The Trade-off:\n",
    "- Larger $k$ â†’ better reconstruction, less compression\n",
    "- Smaller $k$ â†’ more compression, higher error\n",
    "- Choose $k$ based on explained variance!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Implementation\n",
    "\n",
    "Below we provide:\n",
    "1. `center_data()` - Mean-center the data matrix\n",
    "2. `pca_fit()` - Fit PCA model (compute components, mean, scores)\n",
    "3. `pca_transform()` - Project new data to PCA space\n",
    "4. `pca_inverse_transform()` - Reconstruct from PCA space\n",
    "\n",
    "These match scikit-learn's API for easy integration!\n",
    "\n",
    "Let's see the code! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PCA UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def center_data(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Center the data by subtracting the mean of each feature.\n",
    "    \n",
    "    Centering is ESSENTIAL for PCA because:\n",
    "    - PCA finds directions of maximum variance\n",
    "    - Variance is computed relative to the mean\n",
    "    - Without centering, PCA would be dominated by the offset!\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, d)\n",
    "        Data matrix (n samples, d features)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_centered : ndarray of shape (n, d)\n",
    "        Centered data (each column has mean â‰ˆ 0)\n",
    "    mean : ndarray of shape (d,)\n",
    "        Mean vector (needed for inverse transform)\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "    >>> X_c, mu = center_data(X)\n",
    "    >>> print(mu)  # [3, 4]\n",
    "    >>> print(X_c.mean(axis=0))  # [0, 0] (up to numerical error)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    mu = np.mean(X, axis=0, keepdims=True)  # Mean of each column\n",
    "    return X - mu, mu.squeeze()\n",
    "\n",
    "\n",
    "def pca_fit(X: np.ndarray, k: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Fit PCA model to data X, keeping k components.\n",
    "    \n",
    "    This function:\n",
    "    1. Centers the data\n",
    "    2. Computes SVD of centered data\n",
    "    3. Extracts first k principal components (columns of V)\n",
    "    4. Computes PCA scores (projections)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, d)\n",
    "        Training data\n",
    "    k : int\n",
    "        Number of principal components to keep (k <= d)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        'mean' : ndarray of shape (d,)\n",
    "            Feature means (for centering new data)\n",
    "        'components' : ndarray of shape (d, k)\n",
    "            Principal component vectors (columns are PCs)\n",
    "        'singular_values' : ndarray of shape (min(n,d),)\n",
    "            All singular values from SVD\n",
    "        'scores' : ndarray of shape (n, k)\n",
    "            PCA coordinates of training data\n",
    "            \n",
    "    The 'components' matrix V_k satisfies:\n",
    "        scores = (X - mean) @ components\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> X = np.random.randn(100, 50)  # 100 samples, 50 features\n",
    "    >>> pca = pca_fit(X, k=10)  # Keep top 10 components\n",
    "    >>> print(pca['components'].shape)  # (50, 10)\n",
    "    >>> print(pca['scores'].shape)      # (100, 10)\n",
    "    \"\"\"\n",
    "    # Step 1: Center the data\n",
    "    Xc, mu = center_data(X)\n",
    "    \n",
    "    # Step 2: Compute SVD\n",
    "    # X_c = U @ diag(S) @ V^T\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    V = Vt.T  # Transpose to get columns as singular vectors\n",
    "    \n",
    "    # Step 3: Extract first k components\n",
    "    Vk = V[:, :k]\n",
    "    \n",
    "    # Step 4: Compute PCA scores (projections)\n",
    "    # scores = X_c @ V_k = U @ diag(S) @ V^T @ V_k\n",
    "    #        = U[:, :k] @ diag(S[:k])\n",
    "    scores = Xc @ Vk  # (n, k)\n",
    "    \n",
    "    return {\n",
    "        \"mean\": mu, \n",
    "        \"components\": Vk, \n",
    "        \"singular_values\": S, \n",
    "        \"scores\": scores\n",
    "    }\n",
    "\n",
    "\n",
    "def pca_transform(X: np.ndarray, pca: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Project new data X into PCA space using fitted model.\n",
    "    \n",
    "    Use this to transform test data or new observations using\n",
    "    the PCA model learned from training data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, d)\n",
    "        Data to transform (must have same d as training data)\n",
    "    pca : dict\n",
    "        Fitted PCA model from pca_fit()\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Z : ndarray of shape (n, k)\n",
    "        PCA coordinates (compressed representation)\n",
    "        \n",
    "    Formula:\n",
    "    --------\n",
    "    Z = (X - mean) @ components\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> pca = pca_fit(X_train, k=5)\n",
    "    >>> Z_test = pca_transform(X_test, pca)\n",
    "    >>> # Z_test has 5 columns instead of original d columns!\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Xc = X - pca[\"mean\"]  # Center using training mean\n",
    "    return Xc @ pca[\"components\"]\n",
    "\n",
    "\n",
    "def pca_inverse_transform(Z: np.ndarray, pca: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reconstruct data from PCA coordinates.\n",
    "    \n",
    "    Goes back from k-dimensional PCA space to original d dimensions.\n",
    "    Note: This is only an approximation if k < d!\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Z : ndarray of shape (n, k)\n",
    "        PCA coordinates\n",
    "    pca : dict\n",
    "        Fitted PCA model from pca_fit()\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_reconstructed : ndarray of shape (n, d)\n",
    "        Reconstructed data in original space\n",
    "        \n",
    "    Formula:\n",
    "    --------\n",
    "    X_reconstructed = Z @ components^T + mean\n",
    "    \n",
    "    Note: If k < d, this is the best rank-k approximation!\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> pca = pca_fit(X_train, k=5)\n",
    "    >>> Z = pca_transform(X_train, pca)\n",
    "    >>> X_recon = pca_inverse_transform(Z, pca)\n",
    "    >>> error = np.linalg.norm(X_train - X_recon, 'fro')\n",
    "    >>> print(f\"Reconstruction error: {error:.4f}\")\n",
    "    \"\"\"\n",
    "    Z = np.asarray(Z, dtype=float)\n",
    "    # Back-project and add mean\n",
    "    return Z @ pca[\"components\"].T + pca[\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177553a0",
   "metadata": {},
   "source": [
    "# Part 4: Applications of Dimensionality Reduction ðŸš€\n",
    "\n",
    "## 11.4 Rank-k Approximation: Optimal Compression\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "Given matrix $A$ with SVD: $A = \\sum_{j=1}^{m} \\sigma_j u_j v_j^T$\n",
    "\n",
    "We can approximate it using only the **first k terms**:\n",
    "$$\n",
    "A_k = \\sum_{j=1}^{k} \\sigma_j u_j v_j^T\n",
    "$$\n",
    "\n",
    "This is a **rank-k matrix** (has at most k non-zero singular values).\n",
    "\n",
    "### ðŸŒŸ Why This is Optimal\n",
    "\n",
    "**Eckart-Young Theorem**: $A_k$ is the **best** rank-k approximation to $A$ in both:\n",
    "- Frobenius norm: $\\|A - B\\|_F = \\sqrt{\\sum_{i,j} (A_{ij} - B_{ij})^2}$\n",
    "- Operator norm: $\\|A - B\\|_2$ = largest singular value of $(A-B)$\n",
    "\n",
    "No other rank-k matrix is closer to $A$! ðŸ†\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Measuring Approximation Quality\n",
    "\n",
    "### 1. Reconstruction Error (Frobenius Norm)\n",
    "\n",
    "The error from using only $k$ terms:\n",
    "$$\n",
    "\\|A - A_k\\|_F = \\sqrt{\\sum_{j=k+1}^{m} \\sigma_j^2}\n",
    "$$\n",
    "\n",
    "**Intuition**: The error is exactly the \"energy\" in the discarded components!\n",
    "\n",
    "### 2. Explained Variance Ratio\n",
    "\n",
    "What fraction of the data's \"energy\" is retained?\n",
    "$$\n",
    "\\text{Explained Variance} = \\frac{\\sum_{j=1}^{k} \\sigma_j^2}{\\sum_{j=1}^{m} \\sigma_j^2}\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "- 0.90 means \"90% of variance explained\" â†’ good compression\n",
    "- 0.50 means \"50% of variance explained\" â†’ losing half the information\n",
    "- 1.00 means \"perfect reconstruction\" â†’ no compression\n",
    "\n",
    "### ðŸ“Š The Scree Plot\n",
    "\n",
    "Plot singular values $\\sigma_1, \\sigma_2, \\ldots$ in descending order.\n",
    "\n",
    "Look for the **\"elbow\"** where values drop sharply:\n",
    "- Before elbow: important components (keep these)\n",
    "- After elbow: noise components (can discard)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¨ Example Application: Image Compression\n",
    "\n",
    "Images are just matrices! A grayscale image is size $(H \\times W)$.\n",
    "\n",
    "**Without compression**: Store $H \\times W$ values  \n",
    "**With rank-k SVD**: Store $U_k$ (size $H \\times k$) + $\\Sigma_k$ (size $k$) + $V_k$ (size $W \\times k$)\n",
    "\n",
    "**Storage**: $H \\times W \\rightarrow k(H + W + 1)$\n",
    "\n",
    "If $k \\ll \\min(H, W)$, huge savings! ðŸ’¾\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Implementation\n",
    "\n",
    "Below we provide utilities for:\n",
    "1. `rank_k_approximation()` - Compute $A_k$ from full SVD\n",
    "2. `reconstruction_error_frobenius()` - Measure $\\|A - A_k\\|_F$\n",
    "3. `reconstruction_error_from_singular_values()` - Compute error from $\\sigma$ values\n",
    "4. `explained_variance_ratio_from_singular_values()` - Compute variance explained\n",
    "\n",
    "Plus a demo showing compression in action! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b93ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RANK-K APPROXIMATION AND COMPRESSION UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def rank_k_approximation(X: np.ndarray, k: int) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Compute the best rank-k approximation of matrix X using SVD.\n",
    "    \n",
    "    This uses the Eckart-Young theorem: the rank-k matrix that minimizes\n",
    "    ||X - X_k|| is obtained by truncating the SVD.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, m)\n",
    "        Input matrix (can be raw data or centered data)\n",
    "    k : int\n",
    "        Target rank (k <= min(n, m))\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_k : ndarray of shape (n, m)\n",
    "        Best rank-k approximation\n",
    "    svd_info : dict\n",
    "        Full SVD components: 'U', 'S', 'Vt'\n",
    "        \n",
    "    Algorithm:\n",
    "    ----------\n",
    "    1. Compute full SVD: X = U @ diag(S) @ Vt\n",
    "    2. Keep first k components: X_k = U[:, :k] @ diag(S[:k]) @ Vt[:k, :]\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> X = np.random.randn(100, 50)\n",
    "    >>> X_10, info = rank_k_approximation(X, k=10)\n",
    "    >>> print(f\"Rank: {np.linalg.matrix_rank(X_10)}\")  # Should be 10\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    \n",
    "    # Compute full SVD\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    \n",
    "    # Truncate to rank k\n",
    "    Uk = U[:, :k]\n",
    "    Sk = S[:k]\n",
    "    Vtk = Vt[:k, :]\n",
    "    \n",
    "    # Reconstruct: X_k = U_k @ diag(S_k) @ Vt_k\n",
    "    Xk = (Uk * Sk) @ Vtk  # Efficient: multiply U_k by S_k element-wise\n",
    "    \n",
    "    return Xk, {\"U\": U, \"S\": S, \"Vt\": Vt}\n",
    "\n",
    "\n",
    "def reconstruction_error_frobenius(X: np.ndarray, Xk: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Frobenius norm of the reconstruction error.\n",
    "    \n",
    "    Frobenius norm: ||A||_F = sqrt(sum of all squared entries)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray\n",
    "        Original matrix\n",
    "    Xk : ndarray\n",
    "        Approximation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    error : float\n",
    "        ||X - X_k||_F\n",
    "        \n",
    "    Interpretation:\n",
    "    ---------------\n",
    "    - Measures total squared error across all entries\n",
    "    - Larger value = worse approximation\n",
    "    - Compare to ||X||_F to get relative error\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> error = reconstruction_error_frobenius(X, X_k)\n",
    "    >>> relative = error / np.linalg.norm(X, 'fro')\n",
    "    >>> print(f\"Relative error: {relative:.2%}\")\n",
    "    \"\"\"\n",
    "    D = np.asarray(X, dtype=float) - np.asarray(Xk, dtype=float)\n",
    "    return float(np.linalg.norm(D, ord='fro'))\n",
    "\n",
    "\n",
    "def reconstruction_error_from_singular_values(S: np.ndarray, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute reconstruction error directly from singular values (more efficient!).\n",
    "    \n",
    "    Uses the formula: ||X - X_k||_F = sqrt(Ïƒ_{k+1}^2 + Ïƒ_{k+2}^2 + ...)\n",
    "    \n",
    "    This is MUCH faster than computing X_k and then ||X - X_k||!\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    S : ndarray of shape (r,)\n",
    "        Singular values in descending order\n",
    "    k : int\n",
    "        Number of components kept\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    error : float\n",
    "        Reconstruction error\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    >>> error_k5 = reconstruction_error_from_singular_values(S, k=5)\n",
    "    >>> error_k10 = reconstruction_error_from_singular_values(S, k=10)\n",
    "    >>> print(f\"k=5: {error_k5:.4f}, k=10: {error_k10:.4f}\")\n",
    "    \"\"\"\n",
    "    S = np.asarray(S, dtype=float)\n",
    "    tail = S[k:]  # Discarded singular values\n",
    "    return float(np.sqrt(np.sum(tail * tail)))\n",
    "\n",
    "\n",
    "def explained_variance_ratio_from_singular_values(S: np.ndarray, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute the fraction of variance explained by first k components.\n",
    "    \n",
    "    Formula: (Ïƒâ‚Â² + Ïƒâ‚‚Â² + ... + Ïƒâ‚–Â²) / (Ïƒâ‚Â² + Ïƒâ‚‚Â² + ... + Ïƒâ‚˜Â²)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    S : ndarray of shape (r,)\n",
    "        Singular values in descending order\n",
    "    k : int\n",
    "        Number of components\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    ratio : float\n",
    "        Explained variance ratio (between 0 and 1)\n",
    "        \n",
    "    Interpretation:\n",
    "    ---------------\n",
    "    - 1.0 = perfect (all variance explained)\n",
    "    - 0.9 = excellent (90% variance retained)\n",
    "    - 0.5 = poor (half the information lost)\n",
    "    - 0.0 = useless (everything discarded)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    >>> for k in [1, 5, 10, 20]:\n",
    "    ...     evr = explained_variance_ratio_from_singular_values(S, k)\n",
    "    ...     print(f\"k={k:2d}: {evr:.2%}\")\n",
    "    \"\"\"\n",
    "    S = np.asarray(S, dtype=float)\n",
    "    num = float(np.sum(S[:k] * S[:k]))  # Energy in first k components\n",
    "    den = float(np.sum(S * S))          # Total energy\n",
    "    \n",
    "    if den == 0:\n",
    "        return float(\"nan\")  # Edge case: zero matrix\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO: Rank-k Approximation on Synthetic Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RANK-K APPROXIMATION DEMO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate low-rank data with noise\n",
    "rng = np.random.default_rng(0)\n",
    "n, d = 500, 60\n",
    "\n",
    "# Create inherently low-rank data (rank â‰ˆ 10) plus noise\n",
    "X_lowrank = rng.normal(size=(n, 10)) @ rng.normal(size=(10, d))\n",
    "X_noise = rng.normal(scale=0.5, size=(n, d))\n",
    "X = X_lowrank + X_noise\n",
    "\n",
    "print(f\"\\nðŸ“Š Generated data: {n} samples Ã— {d} features\")\n",
    "print(f\"   (Low-rank signal + noise)\")\n",
    "\n",
    "# Center the data (important for PCA interpretation)\n",
    "Xc, mu = center_data(X)\n",
    "\n",
    "# Compute rank-10 approximation\n",
    "X10, svd_info = rank_k_approximation(Xc, k=10)\n",
    "\n",
    "print(\"\\nðŸ”§ Computing rank-10 approximation...\")\n",
    "print(f\"   Original matrix rank: {np.linalg.matrix_rank(Xc)}\")\n",
    "print(f\"   Approximation rank: {np.linalg.matrix_rank(X10)}\")\n",
    "\n",
    "# Measure error (two methods)\n",
    "error_direct = reconstruction_error_frobenius(Xc, X10)\n",
    "error_sigma = reconstruction_error_from_singular_values(svd_info[\"S\"], k=10)\n",
    "\n",
    "print(\"\\nðŸ“ RECONSTRUCTION ERROR:\")\n",
    "print(f\"   Direct computation:  {error_direct:.6f}\")\n",
    "print(f\"   From singular values: {error_sigma:.6f}\")\n",
    "print(f\"   Difference: {abs(error_direct - error_sigma):.2e} (should be â‰ˆ0)\")\n",
    "\n",
    "# Explained variance\n",
    "evr = explained_variance_ratio_from_singular_values(svd_info[\"S\"], k=10)\n",
    "print(f\"\\nðŸ“Š EXPLAINED VARIANCE:\")\n",
    "print(f\"   k=10 components explain {evr:.2%} of variance\")\n",
    "\n",
    "# Compare different k values\n",
    "print(\"\\nðŸ“ˆ EXPLAINED VARIANCE FOR DIFFERENT k:\")\n",
    "print(\"-\" * 70)\n",
    "for k_test in [1, 5, 10, 15, 20, 30]:\n",
    "    evr_k = explained_variance_ratio_from_singular_values(svd_info[\"S\"], k=k_test)\n",
    "    print(f\"   k={k_test:2d}: {evr_k:.4f} ({evr_k:.1%})\")\n",
    "\n",
    "# Scree plot: visualize singular values\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(svd_info[\"S\"], 'o-', markersize=4)\n",
    "plt.xlabel(\"Component Index\", fontsize=11)\n",
    "plt.ylabel(\"Singular Value\", fontsize=11)\n",
    "plt.title(\"Scree Plot (All Singular Values)\", fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(svd_info[\"S\"][:20], 'o-', markersize=6)\n",
    "plt.axvline(10, color='red', linestyle='--', label='k=10')\n",
    "plt.xlabel(\"Component Index\", fontsize=11)\n",
    "plt.ylabel(\"Singular Value\", fontsize=11)\n",
    "plt.title(\"Top-20 Components (Look for Elbow!)\", fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "k_range = range(1, min(31, len(svd_info[\"S\"])))\n",
    "evr_curve = [explained_variance_ratio_from_singular_values(svd_info[\"S\"], k) \n",
    "             for k in k_range]\n",
    "plt.plot(k_range, evr_curve, 'o-', markersize=4)\n",
    "plt.axhline(0.9, color='green', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "plt.axhline(0.95, color='orange', linestyle='--', alpha=0.5, label='95% threshold')\n",
    "plt.xlabel(\"Number of Components (k)\", fontsize=11)\n",
    "plt.ylabel(\"Explained Variance Ratio\", fontsize=11)\n",
    "plt.title(\"Cumulative Explained Variance\", fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ How to choose k:\")\n",
    "print(\"   1. Look for 'elbow' in scree plot (where curve flattens)\")\n",
    "print(\"   2. Choose k to explain desired % of variance (e.g., 90%)\")\n",
    "print(\"   3. Balance compression vs. reconstruction quality\")\n",
    "print(\"   4. For this data: k=10 is a good choice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff9c38",
   "metadata": {},
   "source": [
    "### Optional demo: digits compression (like the notes' MNIST example)\n",
    "\n",
    "The notes mention MNIST; offline we can use scikit-learn's `load_digits` dataset (8Ã—8 images).\n",
    "If scikit-learn isn't available in your environment, you can skip this cell safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: digits dataset compression (offline, small)\n",
    "try:\n",
    "    from sklearn.datasets import load_digits\n",
    "    digits = load_digits()\n",
    "    A = digits.data.astype(float)  # (n,64)\n",
    "    A_centered, mu = center_data(A)\n",
    "\n",
    "    k = 10\n",
    "    Ak, svd_info = rank_k_approximation(A_centered, k=k)\n",
    "    recon = Ak + mu\n",
    "\n",
    "    # Show 10 original vs reconstructed\n",
    "    idx = np.arange(10)\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    for i, j in enumerate(idx):\n",
    "        plt.subplot(2, 10, i+1)\n",
    "        plt.imshow(digits.images[j], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, 10, 10+i+1)\n",
    "        plt.imshow(recon[j].reshape(8,8), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Top row: original | Bottom row: rank-10 reconstruction\")\n",
    "    plt.show()\n",
    "\n",
    "    # Explained variance\n",
    "    evr10 = explained_variance_ratio_from_singular_values(svd_info[\"S\"], k=10)\n",
    "    print(\"Explained variance ratio (k=10):\", evr10)\n",
    "except Exception as e:\n",
    "    print(\"Skipped digits demo (likely sklearn missing):\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d13d29",
   "metadata": {},
   "source": [
    "## 11.4.3 Application: Anomaly Detection via Reconstruction Error ðŸ”\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "**Normal data** lies in a low-dimensional subspace. **Anomalies** don't!\n",
    "\n",
    "If we compress \"normal\" data to $k$ dimensions and reconstruct it, the error is small.  \n",
    "But anomalies have large reconstruction error because they don't fit the pattern!\n",
    "\n",
    "### ðŸŽ¯ The Algorithm\n",
    "\n",
    "**Training Phase** (on normal data):\n",
    "1. Fit PCA with $k$ components to capture \"normal\" patterns\n",
    "2. Reconstruct training data and compute reconstruction errors\n",
    "3. Set threshold $\\tau$ = high quantile of training errors (e.g., 99th percentile)\n",
    "\n",
    "**Testing Phase** (on new data):\n",
    "1. Transform test point to PCA space and back\n",
    "2. Compute reconstruction error $e = \\|x - \\hat{x}\\|$\n",
    "3. Flag as anomaly if $e > \\tau$\n",
    "\n",
    "### ðŸ“Š Intuition\n",
    "\n",
    "Think of PCA as learning the \"shape\" of normal data:\n",
    "- Normal points: fit the shape well â†’ small reconstruction error\n",
    "- Anomalies: don't fit the shape â†’ large reconstruction error\n",
    "\n",
    "### âš™ï¸ Hyperparameters\n",
    "\n",
    "**k (number of components)**:\n",
    "- Too small: can't capture normal patterns â†’ false positives\n",
    "- Too large: captures noise â†’ false negatives\n",
    "- Rule of thumb: choose k to explain 90-95% of variance\n",
    "\n",
    "**q (threshold quantile)**:\n",
    "- Typical: q = 0.95 or 0.99\n",
    "- Higher q = fewer false alarms but may miss some anomalies\n",
    "- Lower q = more sensitive but more false alarms\n",
    "\n",
    "### âœ… When This Works Well\n",
    "\n",
    "âœ“ Normal data has low intrinsic dimension  \n",
    "âœ“ Anomalies are different from normal patterns  \n",
    "âœ“ You have clean training data (mostly normal examples)  \n",
    "\n",
    "### âš ï¸ Limitations\n",
    "\n",
    "âœ— If anomalies also lie in the low-D subspace (systematic drift)  \n",
    "âœ— If normal data is very high-dimensional with no structure  \n",
    "âœ— If training data contains many anomalies (contaminates the model)  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Implementation\n",
    "\n",
    "Below we provide a complete anomaly detection pipeline:\n",
    "\n",
    "1. `pca_anomaly_detector_fit()` - Train on normal data\n",
    "2. `pca_anomaly_detector_predict()` - Detect anomalies in new data\n",
    "3. Full demo with injected anomalies\n",
    "\n",
    "Let's see it in action! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cde022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PCA-BASED ANOMALY DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def reconstruction_errors_per_row(X: np.ndarray, Xk: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute reconstruction error for each data point.\n",
    "    \n",
    "    For each row i: error[i] = ||X[i] - Xk[i]||\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, d)\n",
    "        Original data\n",
    "    Xk : ndarray of shape (n, d)\n",
    "        Reconstructed data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    errors : ndarray of shape (n,)\n",
    "        Per-sample reconstruction errors\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> errors = reconstruction_errors_per_row(X, X_reconstructed)\n",
    "    >>> print(f\"Mean error: {errors.mean():.4f}\")\n",
    "    >>> print(f\"Max error: {errors.max():.4f}\")\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Xk = np.asarray(Xk, dtype=float)\n",
    "    return np.linalg.norm(X - Xk, axis=1)\n",
    "\n",
    "\n",
    "def anomaly_threshold_from_quantile(errors: np.ndarray, q: float = 0.99) -> float:\n",
    "    \"\"\"\n",
    "    Choose anomaly threshold as a quantile of training errors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    errors : ndarray\n",
    "        Reconstruction errors from training data\n",
    "    q : float, default=0.99\n",
    "        Quantile level (0.95 = 95th percentile, 0.99 = 99th percentile)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    threshold : float\n",
    "        Value such that (1-q) fraction of training errors exceed it\n",
    "        \n",
    "    Interpretation:\n",
    "    ---------------\n",
    "    - q=0.95: flag ~5% of training data as \"unusual\"\n",
    "    - q=0.99: flag ~1% of training data as \"unusual\"\n",
    "    - Higher q = stricter threshold = fewer false alarms\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> threshold = anomaly_threshold_from_quantile(train_errors, q=0.99)\n",
    "    >>> print(f\"99th percentile: {threshold:.4f}\")\n",
    "    \"\"\"\n",
    "    return float(np.quantile(np.asarray(errors, dtype=float), q))\n",
    "\n",
    "\n",
    "def pca_anomaly_detector_fit(X_train: np.ndarray, k: int, \n",
    "                             q: float = 0.99) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Train PCA-based anomaly detector on normal data.\n",
    "    \n",
    "    Steps:\n",
    "    1. Fit PCA with k components\n",
    "    2. Compute reconstruction errors on training data\n",
    "    3. Set threshold at q-th quantile of errors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : ndarray of shape (n, d)\n",
    "        Training data (should be mostly normal examples)\n",
    "    k : int\n",
    "        Number of PCA components (controls compression)\n",
    "    q : float, default=0.99\n",
    "        Quantile for threshold (0.95-0.99 typical)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    detector : dict\n",
    "        Trained model with keys:\n",
    "        - 'pca': fitted PCA model\n",
    "        - 'k': number of components\n",
    "        - 'q': quantile used\n",
    "        - 'threshold': anomaly threshold\n",
    "        - 'train_errors': training reconstruction errors\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> detector = pca_anomaly_detector_fit(X_train, k=10, q=0.99)\n",
    "    >>> print(f\"Threshold: {detector['threshold']:.4f}\")\n",
    "    \"\"\"\n",
    "    # Fit PCA\n",
    "    pca = pca_fit(X_train, k=k)\n",
    "    \n",
    "    # Compute reconstruction errors on training data\n",
    "    Z = pca_transform(X_train, pca)\n",
    "    X_rec = pca_inverse_transform(Z, pca)\n",
    "    errs = reconstruction_errors_per_row(X_train, X_rec)\n",
    "    \n",
    "    # Set threshold\n",
    "    thr = anomaly_threshold_from_quantile(errs, q=q)\n",
    "    \n",
    "    return {\n",
    "        \"pca\": pca, \n",
    "        \"k\": k, \n",
    "        \"q\": q, \n",
    "        \"threshold\": thr, \n",
    "        \"train_errors\": errs\n",
    "    }\n",
    "\n",
    "\n",
    "def pca_anomaly_detector_predict(det: Dict[str, object], \n",
    "                                 X: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Detect anomalies in new data using trained detector.\n",
    "    \n",
    "    For each test point:\n",
    "    1. Transform to PCA space\n",
    "    2. Reconstruct\n",
    "    3. Compute error\n",
    "    4. Compare to threshold\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    det : dict\n",
    "        Trained detector from pca_anomaly_detector_fit()\n",
    "    X : ndarray of shape (m, d)\n",
    "        Test data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : dict\n",
    "        - 'errors': reconstruction errors for each point\n",
    "        - 'is_anomaly': boolean flags (True = anomaly)\n",
    "        - 'threshold': the threshold used\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> pred = pca_anomaly_detector_predict(detector, X_test)\n",
    "    >>> n_anomalies = pred['is_anomaly'].sum()\n",
    "    >>> print(f\"Found {n_anomalies} anomalies out of {len(X_test)} points\")\n",
    "    \"\"\"\n",
    "    pca = det[\"pca\"]\n",
    "    \n",
    "    # Reconstruct test data\n",
    "    Z = pca_transform(X, pca)\n",
    "    X_rec = pca_inverse_transform(Z, pca)\n",
    "    \n",
    "    # Compute errors\n",
    "    errs = reconstruction_errors_per_row(X, X_rec)\n",
    "    \n",
    "    # Flag anomalies\n",
    "    flags = errs > det[\"threshold\"]\n",
    "    \n",
    "    return {\n",
    "        \"errors\": errs, \n",
    "        \"is_anomaly\": flags, \n",
    "        \"threshold\": float(det[\"threshold\"])\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO: Anomaly Detection with Synthetic Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PCA-BASED ANOMALY DETECTION DEMO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate data\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Training data: normal examples\n",
    "print(\"\\nðŸŽ² Generating data...\")\n",
    "X_train = rng.normal(loc=0, scale=1.0, size=(800, 40))\n",
    "print(f\"   Training set: {X_train.shape[0]} normal samples\")\n",
    "\n",
    "# Test data: mostly normal + some anomalies\n",
    "X_test = rng.normal(loc=0, scale=1.0, size=(300, 40))\n",
    "\n",
    "# Inject anomalies: add large noise to first 20 samples\n",
    "n_anomalies = 20\n",
    "X_test[:n_anomalies] += rng.normal(loc=0, scale=6.0, size=(n_anomalies, 40))\n",
    "print(f\"   Test set: {X_test.shape[0]} samples ({n_anomalies} injected anomalies)\")\n",
    "\n",
    "# Train detector\n",
    "print(\"\\nðŸ”§ Training anomaly detector...\")\n",
    "print(f\"   Using k=5 components, q=0.99 quantile\")\n",
    "det = pca_anomaly_detector_fit(X_train, k=5, q=0.99)\n",
    "print(f\"   Threshold set to: {det['threshold']:.4f}\")\n",
    "print(f\"   (Training data errors: mean={det['train_errors'].mean():.4f}, \"\n",
    "      f\"max={det['train_errors'].max():.4f})\")\n",
    "\n",
    "# Predict on test set\n",
    "print(\"\\nðŸ” Detecting anomalies in test data...\")\n",
    "pred = pca_anomaly_detector_predict(det, X_test)\n",
    "\n",
    "n_flagged = int(np.sum(pred[\"is_anomaly\"]))\n",
    "n_flagged_in_injected = int(np.sum(pred[\"is_anomaly\"][:n_anomalies]))\n",
    "n_flagged_in_normal = int(np.sum(pred[\"is_anomaly\"][n_anomalies:]))\n",
    "\n",
    "print(f\"\\nðŸ“Š RESULTS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Total flagged as anomalies: {n_flagged} / {len(X_test)}\")\n",
    "print(f\"  Among injected anomalies ({n_anomalies}): {n_flagged_in_injected} detected\")\n",
    "print(f\"  Among normal samples ({len(X_test)-n_anomalies}): {n_flagged_in_normal} false alarms\")\n",
    "print(f\"\\nDetection rate: {n_flagged_in_injected/n_anomalies:.1%}\")\n",
    "print(f\"False alarm rate: {n_flagged_in_normal/(len(X_test)-n_anomalies):.1%}\")\n",
    "\n",
    "# Visualize reconstruction errors\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Left: histogram of errors\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(det[\"train_errors\"], bins=40, alpha=0.7, label=\"Training (normal)\", \n",
    "         color='blue', edgecolor='black')\n",
    "plt.hist(pred[\"errors\"][n_anomalies:], bins=40, alpha=0.5, label=\"Test (normal)\", \n",
    "         color='green', edgecolor='black')\n",
    "plt.hist(pred[\"errors\"][:n_anomalies], bins=40, alpha=0.7, label=\"Test (anomalies)\", \n",
    "         color='red', edgecolor='black')\n",
    "plt.axvline(pred[\"threshold\"], linestyle='--', color='black', linewidth=2, \n",
    "           label='Threshold')\n",
    "plt.xlabel(\"Reconstruction Error\", fontsize=11)\n",
    "plt.ylabel(\"Count\", fontsize=11)\n",
    "plt.title(\"Distribution of Reconstruction Errors\", fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Middle: scatter plot\n",
    "plt.subplot(1, 3, 2)\n",
    "idx_normal = np.arange(n_anomalies, len(X_test))\n",
    "idx_anomaly = np.arange(n_anomalies)\n",
    "plt.scatter(idx_normal, pred[\"errors\"][idx_normal], alpha=0.5, s=20, \n",
    "           label='Normal', color='green')\n",
    "plt.scatter(idx_anomaly, pred[\"errors\"][:n_anomalies], alpha=0.7, s=40, \n",
    "           label='Injected anomalies', color='red', marker='^')\n",
    "plt.axhline(pred[\"threshold\"], linestyle='--', color='black', linewidth=2, \n",
    "           label='Threshold')\n",
    "plt.xlabel(\"Sample Index\", fontsize=11)\n",
    "plt.ylabel(\"Reconstruction Error\", fontsize=11)\n",
    "plt.title(\"Per-Sample Errors (Test Set)\", fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Right: ROC-style curve (vary threshold)\n",
    "plt.subplot(1, 3, 3)\n",
    "thresholds = np.linspace(det[\"train_errors\"].min(), pred[\"errors\"].max(), 100)\n",
    "detection_rates = []\n",
    "false_alarm_rates = []\n",
    "for t in thresholds:\n",
    "    detected = (pred[\"errors\"][:n_anomalies] > t).sum() / n_anomalies\n",
    "    false_alarms = (pred[\"errors\"][n_anomalies:] > t).sum() / (len(X_test) - n_anomalies)\n",
    "    detection_rates.append(detected)\n",
    "    false_alarm_rates.append(false_alarms)\n",
    "\n",
    "plt.plot(false_alarm_rates, detection_rates, 'b-', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "plt.xlabel(\"False Alarm Rate\", fontsize=11)\n",
    "plt.ylabel(\"Detection Rate\", fontsize=11)\n",
    "plt.title(\"Detection Performance Curve\", fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   âœ“ Anomalies have HIGHER reconstruction errors\")\n",
    "print(\"   âœ“ Threshold controls trade-off between detection vs false alarms\")\n",
    "print(\"   âœ“ k and q are hyperparameters to tune for your data\")\n",
    "print(\"   âœ“ This method works when normal data has low intrinsic dimension!\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Try it yourself:\")\n",
    "print(\"   - Change k (number of components)\")\n",
    "print(\"   - Change q (threshold quantile)\")\n",
    "print(\"   - Inject different types of anomalies\")\n",
    "print(\"   - Use on real data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ed763",
   "metadata": {},
   "source": [
    "# Part 5: Theoretical Foundations ðŸ“–\n",
    "\n",
    "## 11.5 Why PCA Works: Concentration of Sample Covariance\n",
    "\n",
    "### The Setup\n",
    "\n",
    "Given i.i.d. random vectors $X_1, \\ldots, X_n \\in \\mathbb{R}^d$ with:\n",
    "- Mean: $\\mathbb{E}[X_i] = 0$ (centered)\n",
    "- True covariance: $\\Sigma = \\mathbb{E}[X_i X_i^T]$\n",
    "- Bounded norm: $\\|X_i\\|_2 \\leq \\sqrt{C}$ almost surely\n",
    "\n",
    "The **empirical covariance** is:\n",
    "$$\n",
    "\\hat{\\Sigma} = \\frac{1}{n} \\sum_{i=1}^n X_i X_i^T\n",
    "$$\n",
    "\n",
    "### ðŸŽ¯ Question: How close is $\\hat{\\Sigma}$ to $\\Sigma$?\n",
    "\n",
    "We need **matrix concentration** results!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Theorem 11.15: Matrix Bernstein Bound\n",
    "\n",
    "**Theorem (Matrix Bernstein)**: Under the above conditions,\n",
    "$$\n",
    "\\mathbb{P}\\left( \\|\\hat{\\Sigma} - \\Sigma\\|_2 > \\varepsilon \\right) \\leq 2d \\cdot \\exp\\left( -\\frac{n\\varepsilon^2}{2C(C + 2\\varepsilon/3)} \\right)\n",
    "$$\n",
    "\n",
    "### What This Means (Intuition):\n",
    "\n",
    "1. **Operator norm**: $\\|\\cdot\\|_2$ measures the largest eigenvalue deviation\n",
    "2. **Exponential concentration**: Probability decreases exponentially with $n$!\n",
    "3. **Dependence on d**: Factor $2d$ means we need $n \\gtrsim d \\log d$ samples\n",
    "4. **Boundedness C**: Must have bounded samples (or sub-Gaussian tails)\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "- For fixed $d$, as $n \\to \\infty$: $\\hat{\\Sigma} \\to \\Sigma$ (consistent estimator)\n",
    "- For high dimensions ($d$ large): need more samples\n",
    "- Tight bound depends on $C$ (how spread out the data is)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Theorem 11.17: Weyl's Theorem (Eigenvalue Perturbation)\n",
    "\n",
    "**Theorem (Weyl)**: If $\\hat{\\Sigma} = \\Sigma + E$ (true + error), then:\n",
    "$$\n",
    "\\max_i |\\hat{\\lambda}_i - \\lambda_i| \\leq \\|E\\|_2\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ and $\\hat{\\lambda}_i$ are eigenvalues of $\\Sigma$ and $\\hat{\\Sigma}$.\n",
    "\n",
    "### What This Means (Intuition):\n",
    "\n",
    "1. **Eigenvalue stability**: Eigenvalues can't change more than the error norm\n",
    "2. **Worst-case bound**: The *maximum* eigenvalue error is bounded by operator norm\n",
    "3. **Not average**: Individual eigenvalues might change less!\n",
    "\n",
    "### Why It Matters:\n",
    "\n",
    "- Combining Matrix Bernstein + Weyl: eigenvalues of $\\hat{\\Sigma}$ are close to those of $\\Sigma$\n",
    "- Since PCA uses eigenvalues/eigenvectors, this means PCA on finite samples is reliable!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š 11.6 Excess Risk: How Much Does Finite Sample Hurt?\n",
    "\n",
    "### The Risk Framework\n",
    "\n",
    "Define:\n",
    "- $\\mathcal{P}_k$ = all rank-$k$ orthogonal projections\n",
    "- Loss: $L(x, \\Pi(x)) = \\|x - \\Pi(x)\\|_2^2$ (reconstruction error)\n",
    "- Risk: $R(\\Pi) = \\mathbb{E}_X[L(X, \\Pi(X))]$\n",
    "\n",
    "**Population optimum**: $\\Pi_k^* = \\arg\\min_{\\Pi \\in \\mathcal{P}_k} R(\\Pi)$  \n",
    "**Empirical optimum**: $\\hat{\\Pi}_k^* = \\arg\\min_{\\Pi \\in \\mathcal{P}_k} \\hat{R}(\\Pi)$ (from sample)\n",
    "\n",
    "**Excess risk**: \n",
    "$$\n",
    "E_k = R(\\hat{\\Pi}_k^*) - R(\\Pi_k^*)\n",
    "$$\n",
    "\n",
    "How much worse is our empirical solution compared to the ideal?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Lemma 11.19: Excess Risk from Covariance Error\n",
    "\n",
    "**Lemma**: \n",
    "$$\n",
    "E_k \\leq \\sqrt{2k} \\cdot \\|\\Sigma - \\hat{\\Sigma}\\|_2\n",
    "$$\n",
    "\n",
    "### What This Means:\n",
    "\n",
    "- Excess risk is controlled by covariance estimation error\n",
    "- Factor $\\sqrt{k}$: more components = potentially more error accumulation\n",
    "- But: if $\\|\\Sigma - \\hat{\\Sigma}\\|_2$ is small (many samples), excess risk is small!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Theorem 11.20: Putting It All Together\n",
    "\n",
    "**Theorem**: Combining Lemma 11.19 with Matrix Bernstein,\n",
    "$$\n",
    "\\mathbb{P}(E_k > \\varepsilon) \\leq 2d \\cdot \\exp\\left( -\\frac{n\\varepsilon^2}{4C(C + 2\\varepsilon/3) \\cdot k} \\right)\n",
    "$$\n",
    "\n",
    "### What This Means:\n",
    "\n",
    "1. **Sample complexity**: Need $n \\gtrsim \\frac{d \\cdot k}{\\varepsilon^2}$ for small excess risk\n",
    "2. **Trade-off**: More components ($k$ larger) requires more samples\n",
    "3. **High confidence**: Exponential tail means high-probability guarantees\n",
    "\n",
    "### Practical Take-Away:\n",
    "\n",
    "âœ… PCA is **provably good** when you have enough samples relative to $d$ and $k$  \n",
    "âœ… The theory matches practice: more samples â†’ better performance  \n",
    "âœ… These bounds guide hyperparameter selection (choose $k$ based on $n$ and $d$)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Implementation\n",
    "\n",
    "Below we provide utilities to:\n",
    "1. Compute empirical covariance\n",
    "2. Measure operator norms\n",
    "3. Evaluate the theoretical bounds\n",
    "\n",
    "These help you understand when your sample size is sufficient!\n",
    "\n",
    "Let's see the code! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d815ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# THEORETICAL BOUNDS UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def empirical_covariance_centered(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute empirical covariance matrix from centered data.\n",
    "    \n",
    "    Formula: Î£_hat = (1/n) * X^T X\n",
    "    \n",
    "    Assumes X has mean 0 (each column sums to â‰ˆ 0).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n, d)\n",
    "        Centered data matrix (rows are samples)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Sigma_hat : ndarray of shape (d, d)\n",
    "        Empirical covariance matrix\n",
    "        \n",
    "    Note: This is the sample covariance estimator used in PCA!\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> X = np.random.randn(1000, 50)\n",
    "    >>> X_centered = X - X.mean(axis=0)\n",
    "    >>> Sigma_hat = empirical_covariance_centered(X_centered)\n",
    "    >>> print(Sigma_hat.shape)  # (50, 50)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n = X.shape[0]\n",
    "    return (X.T @ X) / n\n",
    "\n",
    "\n",
    "def operator_norm(M: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the operator (spectral) norm of a matrix.\n",
    "    \n",
    "    Operator norm: ||M||_2 = largest singular value of M\n",
    "                          = sqrt(largest eigenvalue of M^T M)\n",
    "    \n",
    "    This is the \"worst-case\" stretching factor of the matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : ndarray of shape (m, n)\n",
    "        Any matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    norm : float\n",
    "        ||M||_2\n",
    "        \n",
    "    Example:\n",
    "    --------\n",
    "    >>> M = np.random.randn(100, 100)\n",
    "    >>> print(f\"Operator norm: {operator_norm(M):.4f}\")\n",
    "    >>> print(f\"Frobenius norm: {np.linalg.norm(M, 'fro'):.4f}\")\n",
    "    >>> # Operator norm is always â‰¤ Frobenius norm\n",
    "    \"\"\"\n",
    "    M = np.asarray(M, dtype=float)\n",
    "    return float(np.linalg.norm(M, ord=2))\n",
    "\n",
    "\n",
    "def matrix_bernstein_bound_thm11_15(d: int, n: int, eps: float, C: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the probability bound from Matrix Bernstein Theorem (Thm 11.15).\n",
    "    \n",
    "    Bound: P(||Î£_hat - Î£|| > eps) â‰¤ 2d * exp(-n*eps^2 / (2C(C + 2*eps/3)))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : int\n",
    "        Dimension (number of features)\n",
    "    n : int\n",
    "        Sample size\n",
    "    eps : float\n",
    "        Error tolerance\n",
    "    C : float\n",
    "        Bound constant: ||X_i||^2 â‰¤ C almost surely\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bound : float\n",
    "        Upper bound on the probability of large deviation\n",
    "        \n",
    "    Interpretation:\n",
    "    ---------------\n",
    "    - Small bound (close to 0): high confidence that ||Î£_hat - Î£|| â‰¤ eps\n",
    "    - Large bound: not enough samples or eps too small\n",
    "    - Want: n large enough so bound < 0.05 (95% confidence)\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> # Check if 1000 samples sufficient for d=50, eps=0.1, C=25\n",
    "    >>> prob_bound = matrix_bernstein_bound_thm11_15(d=50, n=1000, eps=0.1, C=25)\n",
    "    >>> print(f\"P(error > 0.1) â‰¤ {prob_bound:.6f}\")\n",
    "    \"\"\"\n",
    "    if d <= 0 or n <= 0 or eps <= 0 or C <= 0:\n",
    "        raise ValueError(\"d, n, eps, C must be positive.\")\n",
    "    \n",
    "    denom = 2.0 * C * (C + 2.0 * eps / 3.0)\n",
    "    exponent = - n * eps * eps / denom\n",
    "    return float(2.0 * d * math.exp(exponent))\n",
    "\n",
    "\n",
    "def weyl_eigenvalue_deviation_bound(E: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Apply Weyl's theorem to bound eigenvalue deviations.\n",
    "    \n",
    "    Weyl: max_i |Î»_hat_i - Î»_i| â‰¤ ||E||_2\n",
    "    \n",
    "    where E = Î£_hat - Î£ is the error matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    E : ndarray of shape (d, d)\n",
    "        Error matrix (difference between empirical and true covariance)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bound : float\n",
    "        ||E||_2 - the worst-case eigenvalue deviation\n",
    "        \n",
    "    Interpretation:\n",
    "    ---------------\n",
    "    Every eigenvalue of Î£_hat is within this bound of some eigenvalue of Î£.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> E = Sigma_hat - Sigma_true\n",
    "    >>> max_eigen_error = weyl_eigenvalue_deviation_bound(E)\n",
    "    >>> print(f\"Eigenvalues differ by at most: {max_eigen_error:.6f}\")\n",
    "    \"\"\"\n",
    "    return operator_norm(E)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO: Verifying Matrix Bernstein Bound\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MATRIX BERNSTEIN THEOREM: COVARIANCE CONCENTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate data with known covariance\n",
    "rng = np.random.default_rng(123)\n",
    "d = 20\n",
    "n = 4000\n",
    "\n",
    "print(f\"\\nðŸ“Š Setup:\")\n",
    "print(f\"   Dimension: d = {d}\")\n",
    "print(f\"   Sample size: n = {n}\")\n",
    "\n",
    "# Generate standard normal data (Î£ = I)\n",
    "X = rng.normal(size=(n, d))\n",
    "Xc, _ = center_data(X)\n",
    "\n",
    "print(f\"   True covariance: Î£ = I (identity matrix)\")\n",
    "\n",
    "# Clip norms to ensure bounded condition\n",
    "# We'll clip ||X_i|| to be â‰¤ sqrt(C)\n",
    "C_bound = 25.0  # Choose C = 25, so ||X_i|| â‰¤ 5\n",
    "norms = np.linalg.norm(Xc, axis=1, keepdims=True)\n",
    "Xc_clip = Xc / np.maximum(1.0, norms / math.sqrt(C_bound))\n",
    "actual_max_norm_sq = np.max(np.linalg.norm(Xc_clip, axis=1)**2)\n",
    "print(f\"   Enforced bound: ||X_i||^2 â‰¤ C = {C_bound} (actual max: {actual_max_norm_sq:.2f})\")\n",
    "\n",
    "# Compute empirical covariance\n",
    "Sigma_hat = empirical_covariance_centered(Xc_clip)\n",
    "Sigma_true = np.eye(d)  # True covariance\n",
    "\n",
    "# Compute error\n",
    "E = Sigma_hat - Sigma_true\n",
    "error_norm = operator_norm(E)\n",
    "\n",
    "print(f\"\\nðŸ“ EMPIRICAL RESULTS:\")\n",
    "print(f\"   ||Î£_hat - Î£||_2 = {error_norm:.6f}\")\n",
    "\n",
    "# Apply Matrix Bernstein bound\n",
    "eps_values = [0.05, 0.1, 0.15, 0.2]\n",
    "print(f\"\\nðŸŽ¯ MATRIX BERNSTEIN BOUNDS (Theorem 11.15):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'eps':<10} {'Bound':<15} {'Actual â‰¤ eps?':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for eps in eps_values:\n",
    "    prob_bound = matrix_bernstein_bound_thm11_15(d, n, eps, C_bound)\n",
    "    actual_within = \"âœ“ Yes\" if error_norm <= eps else \"âœ— No\"\n",
    "    print(f\"{eps:<10.3f} {prob_bound:<15.6e} {actual_within:<20}\")\n",
    "\n",
    "# Weyl's theorem\n",
    "print(f\"\\nðŸ”§ WEYL'S THEOREM (Theorem 11.17):\")\n",
    "print(\"-\" * 70)\n",
    "eigen_bound = weyl_eigenvalue_deviation_bound(E)\n",
    "print(f\"   Max eigenvalue deviation bound: {eigen_bound:.6f}\")\n",
    "\n",
    "# Check actual eigenvalue deviations\n",
    "eig_hat = np.linalg.eigvalsh(Sigma_hat)\n",
    "eig_true = np.linalg.eigvalsh(Sigma_true)\n",
    "actual_max_dev = np.max(np.abs(eig_hat - eig_true))\n",
    "print(f\"   Actual max eigenvalue deviation: {actual_max_dev:.6f}\")\n",
    "print(f\"   Weyl's bound holds? {actual_max_dev <= eigen_bound + 1e-10}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Left: Empirical vs true covariance (heatmap difference)\n",
    "ax = axes[0]\n",
    "im = ax.imshow(E, cmap='RdBu_r', vmin=-0.2, vmax=0.2)\n",
    "ax.set_title(\"Error Matrix: Î£_hat - Î£\", fontweight='bold')\n",
    "ax.set_xlabel(\"Feature Index\")\n",
    "ax.set_ylabel(\"Feature Index\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Middle: Eigenvalue comparison\n",
    "ax = axes[1]\n",
    "idx = np.arange(d)\n",
    "ax.plot(idx, eig_true, 'o-', label='True eigenvalues', markersize=6)\n",
    "ax.plot(idx, eig_hat, 's--', label='Empirical eigenvalues', markersize=5, alpha=0.7)\n",
    "ax.set_xlabel(\"Eigenvalue Index\", fontsize=11)\n",
    "ax.set_ylabel(\"Eigenvalue\", fontsize=11)\n",
    "ax.set_title(\"Eigenvalue Comparison\", fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Right: Probability bounds\n",
    "ax = axes[2]\n",
    "eps_range = np.linspace(0.01, 0.5, 50)\n",
    "bounds = [matrix_bernstein_bound_thm11_15(d, n, e, C_bound) for e in eps_range]\n",
    "ax.semilogy(eps_range, bounds, 'b-', linewidth=2)\n",
    "ax.axvline(error_norm, color='red', linestyle='--', linewidth=2, \n",
    "          label=f'Observed error: {error_norm:.4f}')\n",
    "ax.axhline(0.05, color='green', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "ax.set_xlabel(\"Error Tolerance (Îµ)\", fontsize=11)\n",
    "ax.set_ylabel(\"Probability Bound (log scale)\", fontsize=11)\n",
    "ax.set_title(\"Matrix Bernstein Bound\", fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   âœ“ Empirical covariance concentrates around true covariance\")\n",
    "print(\"   âœ“ Concentration is EXPONENTIAL in sample size n\")\n",
    "print(\"   âœ“ Weyl's theorem guarantees eigenvalue stability\")\n",
    "print(\"   âœ“ These bounds justify PCA on finite samples!\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Practical Implications:\")\n",
    "print(f\"   â€¢ For d={d}, n={n}, we have good concentration\")\n",
    "print(f\"   â€¢ Need n â‰³ d*log(d) for reliable estimation\")\n",
    "print(f\"   â€¢ Larger C (more spread-out data) needs more samples\")\n",
    "print(f\"   â€¢ These are worst-case bounds - practice often better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04257d73",
   "metadata": {},
   "source": [
    "## 11.6 Excess Risk Analysis: Quality of Empirical PCA\n",
    "\n",
    "### The Problem\n",
    "\n",
    "We want the **best** rank-$k$ projection for minimizing expected reconstruction error:\n",
    "$$\n",
    "\\Pi_k^* = \\arg\\min_{\\Pi \\in \\mathcal{P}_k} \\mathbb{E}\\left[ \\|X - \\Pi(X)\\|_2^2 \\right]\n",
    "$$\n",
    "\n",
    "But we only have a sample $X_1, \\ldots, X_n$, so we compute:\n",
    "$$\n",
    "\\hat{\\Pi}_k^* = \\arg\\min_{\\Pi \\in \\mathcal{P}_k} \\frac{1}{n} \\sum_{i=1}^n \\|X_i - \\Pi(X_i)\\|_2^2\n",
    "$$\n",
    "\n",
    "**Question**: How much does using $\\hat{\\Pi}_k^*$ instead of $\\Pi_k^*$ hurt us?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Definition: Excess Risk\n",
    "\n",
    "The **excess risk** measures the price of using finite samples:\n",
    "$$\n",
    "E_k = R(\\hat{\\Pi}_k^*) - R(\\Pi_k^*)\n",
    "$$\n",
    "\n",
    "where $R(\\Pi) = \\mathbb{E}[\\|X - \\Pi(X)\\|_2^2]$ is the true risk.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- $E_k = 0$: empirical solution is optimal (lucky!)\n",
    "- $E_k$ small: finite sample doesn't hurt much (good!)\n",
    "- $E_k$ large: need more samples (bad!)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Lemma 11.19: Excess Risk Bound\n",
    "\n",
    "**Lemma**: The excess risk is bounded by:\n",
    "$$\n",
    "E_k \\leq \\sqrt{2k} \\cdot \\|\\Sigma - \\hat{\\Sigma}\\|_2\n",
    "$$\n",
    "\n",
    "### What This Tells Us:\n",
    "\n",
    "1. **Linear in $\\sqrt{k}$**: More components â†’ potentially more excess risk\n",
    "2. **Controlled by covariance error**: If $\\hat{\\Sigma} \\approx \\Sigma$, then $E_k \\approx 0$\n",
    "3. **Sample size matters**: More samples â†’ smaller $\\|\\Sigma - \\hat{\\Sigma}\\|_2$ â†’ smaller $E_k$\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "The PCA projection depends on eigenvectors of $\\hat{\\Sigma}$. If $\\hat{\\Sigma}$ is close to $\\Sigma$, then the eigenvectors are close (by perturbation theory), so the projections are close!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Theorem 11.20: Probability Bound on Excess Risk\n",
    "\n",
    "Combining Lemma 11.19 with Matrix Bernstein (Thm 11.15):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(E_k > \\varepsilon) \\leq 2d \\cdot \\exp\\left( -\\frac{n\\varepsilon^2}{4C(C + 2\\varepsilon/3) \\cdot k} \\right)\n",
    "$$\n",
    "\n",
    "### What This Means:\n",
    "\n",
    "1. **Sample complexity**: To achieve $E_k \\leq \\varepsilon$ with high probability, need:\n",
    "   $$\n",
    "   n \\gtrsim \\frac{4Ck \\cdot d \\log d}{\\varepsilon^2}\n",
    "   $$\n",
    "\n",
    "2. **Trade-offs**:\n",
    "   - Larger $k$ (more components) â†’ need MORE samples\n",
    "   - Larger $d$ (more features) â†’ need MORE samples\n",
    "   - Smaller $\\varepsilon$ (tighter bound) â†’ need MORE samples\n",
    "\n",
    "3. **Exponential concentration**: Probability of large excess risk drops exponentially with $n$!\n",
    "\n",
    "### Practical Guidelines:\n",
    "\n",
    "âœ… **Rule of thumb**: Want $n \\geq 10 \\cdot d \\cdot k$ for reliable PCA  \n",
    "âœ… **High dimensions**: If $d$ is huge, consider random projections first  \n",
    "âœ… **Many components**: Larger $k$ requires more data validation  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Implementation\n",
    "\n",
    "Below we provide utilities to compute and evaluate excess risk bounds:\n",
    "\n",
    "1. `excess_risk_upper_from_cov_error()` - Apply Lemma 11.19\n",
    "2. `excess_risk_tail_bound_thm11_20()` - Apply Theorem 11.20\n",
    "3. Demo showing how bounds scale with parameters\n",
    "\n",
    "Let's see the code! ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXCESS RISK BOUNDS\n",
    "# ============================================================================\n",
    "\n",
    "def excess_risk_upper_from_cov_error(k: int, cov_error_op_norm: float) -> float:\n",
    "    \"\"\"\n",
    "    Apply Lemma 11.19 to bound excess risk from covariance error.\n",
    "    \n",
    "    Bound: E_k â‰¤ sqrt(2k) * ||Î£ - Î£_hat||_2\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    k : int\n",
    "        Number of PCA components\n",
    "    cov_error_op_norm : float\n",
    "        Operator norm of covariance error: ||Î£ - Î£_hat||_2\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bound : float\n",
    "        Upper bound on excess risk\n",
    "        \n",
    "    Interpretation:\n",
    "    ---------------\n",
    "    This tells you: \"Your empirical PCA is at most this much worse\n",
    "    than the optimal PCA (in terms of reconstruction error).\"\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> error_norm = operator_norm(Sigma_hat - Sigma_true)\n",
    "    >>> bound = excess_risk_upper_from_cov_error(k=10, cov_error_op_norm=error_norm)\n",
    "    >>> print(f\"Excess risk â‰¤ {bound:.6f}\")\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be positive.\")\n",
    "    return float(math.sqrt(2.0 * k) * cov_error_op_norm)\n",
    "\n",
    "\n",
    "def excess_risk_tail_bound_thm11_20(d: int, n: int, eps: float, \n",
    "                                    k: int, C: float) -> float:\n",
    "    \"\"\"\n",
    "    Apply Theorem 11.20 to bound probability of large excess risk.\n",
    "    \n",
    "    Bound: P(E_k > eps) â‰¤ 2d * exp(-n*eps^2 / (4*C*(C + 2*eps/3)*k))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : int\n",
    "        Dimension\n",
    "    n : int\n",
    "        Sample size\n",
    "    eps : float\n",
    "        Excess risk tolerance\n",
    "    k : int\n",
    "        Number of PCA components\n",
    "    C : float\n",
    "        Bound constant: ||X_i||^2 â‰¤ C\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bound : float\n",
    "        Upper bound on P(E_k > eps)\n",
    "        \n",
    "    Interpretation:\n",
    "    ---------------\n",
    "    Tells you the probability that your empirical PCA performs\n",
    "    significantly worse than optimal PCA.\n",
    "    \n",
    "    Use case: Check if your sample size n is sufficient!\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> # Is n=1000 enough for d=50, k=10, eps=0.1?\n",
    "    >>> prob = excess_risk_tail_bound_thm11_20(d=50, n=1000, eps=0.1, k=10, C=25)\n",
    "    >>> if prob < 0.05:\n",
    "    ...     print(\"âœ“ 95% confident that excess risk < 0.1\")\n",
    "    ... else:\n",
    "    ...     print(\"âœ— Need more samples!\")\n",
    "    \"\"\"\n",
    "    if d <= 0 or n <= 0 or eps <= 0 or k <= 0 or C <= 0:\n",
    "        raise ValueError(\"d, n, eps, k, C must be positive.\")\n",
    "    \n",
    "    denom = 4.0 * C * (C + 2.0 * eps / 3.0) * k\n",
    "    exponent = - n * eps * eps / denom\n",
    "    return float(2.0 * d * math.exp(exponent))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DEMO: Understanding Sample Complexity for PCA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXCESS RISK ANALYSIS: How Many Samples Do We Need?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Fixed parameters\n",
    "d = 64        # dimension\n",
    "C_val = 25.0  # boundedness constant\n",
    "eps_target = 0.1  # target excess risk\n",
    "\n",
    "print(f\"\\nðŸ“Š Problem Setup:\")\n",
    "print(f\"   Dimension: d = {d}\")\n",
    "print(f\"   Boundedness: C = {C_val}\")\n",
    "print(f\"   Target excess risk: Îµ = {eps_target}\")\n",
    "\n",
    "# Study how sample size requirement changes with k\n",
    "k_values = [1, 5, 10, 20, 30, 50]\n",
    "print(f\"\\nðŸŽ¯ SAMPLE SIZE REQUIREMENTS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'k':<5} {'n (95% conf.)':<20} {'n (99% conf.)':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for k_val in k_values:\n",
    "    # Find n such that probability bound < 0.05 (95% confidence)\n",
    "    # Solve: 2d * exp(-n*eps^2 / denom) < 0.05\n",
    "    # n > (denom / eps^2) * log(2d / 0.05)\n",
    "    denom_95 = 4.0 * C_val * (C_val + 2.0*eps_target/3.0) * k_val\n",
    "    n_95 = int(np.ceil((denom_95 / (eps_target**2)) * math.log(2*d / 0.05)))\n",
    "    \n",
    "    denom_99 = denom_95\n",
    "    n_99 = int(np.ceil((denom_99 / (eps_target**2)) * math.log(2*d / 0.01)))\n",
    "    \n",
    "    print(f\"{k_val:<5} {n_95:<20} {n_99:<20}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Observation: More components (k) requires MORE samples!\")\n",
    "\n",
    "# Visualize: probability bound as function of n for different k\n",
    "print(\"\\nðŸ“ˆ Generating probability curves...\")\n",
    "\n",
    "n_range = np.logspace(2, 4, 50).astype(int)  # from 100 to 10,000\n",
    "k_plot_values = [5, 10, 20, 40]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Left: Probability bound vs sample size\n",
    "plt.subplot(1, 2, 1)\n",
    "for k_val in k_plot_values:\n",
    "    probs = [excess_risk_tail_bound_thm11_20(d, n, eps_target, k_val, C_val) \n",
    "             for n in n_range]\n",
    "    plt.semilogy(n_range, probs, label=f'k={k_val}', linewidth=2)\n",
    "\n",
    "plt.axhline(0.05, color='red', linestyle='--', alpha=0.5, label='5% threshold')\n",
    "plt.axhline(0.01, color='orange', linestyle='--', alpha=0.5, label='1% threshold')\n",
    "plt.xlabel(\"Sample Size (n)\", fontsize=11)\n",
    "plt.ylabel(\"P(Excess Risk > Îµ) [log scale]\", fontsize=11)\n",
    "plt.title(f\"Probability Bound vs Sample Size\\n(d={d}, Îµ={eps_target})\", fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Right: Required sample size vs k\n",
    "plt.subplot(1, 2, 2)\n",
    "k_range_plot = range(1, 51)\n",
    "n_required_95 = []\n",
    "n_required_99 = []\n",
    "for k_val in k_range_plot:\n",
    "    denom = 4.0 * C_val * (C_val + 2.0*eps_target/3.0) * k_val\n",
    "    n_95 = (denom / (eps_target**2)) * math.log(2*d / 0.05)\n",
    "    n_99 = (denom / (eps_target**2)) * math.log(2*d / 0.01)\n",
    "    n_required_95.append(n_95)\n",
    "    n_required_99.append(n_99)\n",
    "\n",
    "plt.plot(k_range_plot, n_required_95, 'o-', label='95% confidence', linewidth=2)\n",
    "plt.plot(k_range_plot, n_required_99, 's-', label='99% confidence', linewidth=2)\n",
    "plt.xlabel(\"Number of Components (k)\", fontsize=11)\n",
    "plt.ylabel(\"Required Sample Size (n)\", fontsize=11)\n",
    "plt.title(f\"Sample Complexity vs k\\n(d={d}, Îµ={eps_target})\", fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ“ KEY TAKEAWAYS:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"1. Sample complexity is LINEAR in k:\")\n",
    "print(f\"   n âˆ k (more components need more data)\")\n",
    "print(\"\\n2. Sample complexity is LINEAR in d:\")\n",
    "print(f\"   n âˆ d*log(d) (more features need more data)\")\n",
    "print(\"\\n3. Sample complexity is QUADRATIC in 1/Îµ:\")\n",
    "print(f\"   n âˆ 1/ÎµÂ² (tighter bounds need much more data)\")\n",
    "print(\"\\n4. Rule of thumb: n â‰¥ 10*d*k for reliable PCA\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ PRACTICAL ADVICE:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"âœ“ Check your sample size before running PCA\")\n",
    "print(\"âœ“ If n < 5*d, consider regularization or random projection\")\n",
    "print(\"âœ“ Cross-validation helps tune k empirically\")\n",
    "print(\"âœ“ These are WORST-CASE bounds - practice often better!\")\n",
    "print(\"âœ“ But they give you confidence when you have enough data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1304b8",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Chapter Summary & Quick Reference\n",
    "\n",
    "## âœ¨ What We Learned\n",
    "\n",
    "### 1. Random Projections (Â§11.1)\n",
    "**Key Idea**: Project high-D data to low-D using random matrices  \n",
    "**When to Use**: Fast compression with distance preservation  \n",
    "**Pro**: Fast, simple, no training needed  \n",
    "**Con**: Less compression than PCA, probabilistic guarantees\n",
    "\n",
    "**Main Results**:\n",
    "- **Theorem 11.1**: Single vector norm preservation\n",
    "- **Johnson-Lindenstrauss**: All pairwise distances preserved with $k \\sim \\log(n)/\\varepsilon^2$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. SVD (Â§11.2)\n",
    "**Key Idea**: Any matrix $A = U\\Sigma V^T$ (rotation-scale-rotation)  \n",
    "**When to Use**: Matrix approximation, data compression, latent structure  \n",
    "**Connection**: Singular vectors are eigenvectors of $A^T A$ or $AA^T$\n",
    "\n",
    "**Main Results**:\n",
    "- **Lemma 11.7**: $v_1$ = top eigenvector of $A^T A$, $\\sigma_1 = \\sqrt{\\lambda_1}$\n",
    "- **Power Method**: Iterative algorithm to find top eigenvectors\n",
    "- **Eckart-Young**: Rank-$k$ truncation is optimal approximation\n",
    "\n",
    "---\n",
    "\n",
    "### 3. PCA (Â§11.3)\n",
    "**Key Idea**: Project onto directions of maximum variance  \n",
    "**When to Use**: Dimensionality reduction, feature extraction, visualization  \n",
    "**How**: SVD of centered data, keep top-$k$ components\n",
    "\n",
    "**Main Results**:\n",
    "- **Theorem 11.10**: Top-$k$ PCs give best $k$-dimensional subspace\n",
    "- **PCA = SVD**: PCA of centered $X$ is $XV = U\\Sigma$\n",
    "- **Explained Variance**: $\\sum_{i=1}^k \\sigma_i^2 / \\sum_i \\sigma_i^2$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Applications (Â§11.4)\n",
    "**Compression**: Images, signals â†’ rank-$k$ approximation  \n",
    "**Anomaly Detection**: Large reconstruction error â†’ anomaly  \n",
    "**Visualization**: Project to 2D or 3D for plotting\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Theory (Â§11.5-11.6)\n",
    "**Matrix Bernstein**: Sample covariance concentrates with $n \\gtrsim d \\log d$  \n",
    "**Weyl's Theorem**: Eigenvalues stable under perturbations  \n",
    "**Excess Risk**: Finite sample PCA close to optimal when $n \\gtrsim dk$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ºï¸ Decision Tree: Which Method to Use?\n",
    "\n",
    "```\n",
    "Start\n",
    "  â”‚\n",
    "  â”œâ”€ Need FAST compression with distance preservation?\n",
    "  â”‚  â””â”€ YES â†’ Use RANDOM PROJECTION\n",
    "  â”‚           â€¢ Choose k via JL lemma\n",
    "  â”‚           â€¢ Good for clustering/NN search\n",
    "  â”‚\n",
    "  â”œâ”€ Need INTERPRETABLE principal directions?\n",
    "  â”‚  â””â”€ YES â†’ Use PCA\n",
    "  â”‚           â€¢ Centered data\n",
    "  â”‚           â€¢ Components have meaning\n",
    "  â”‚           â€¢ Good for exploration\n",
    "  â”‚\n",
    "  â”œâ”€ Need BEST rank-k approximation?\n",
    "  â”‚  â””â”€ YES â†’ Use SVD/PCA\n",
    "  â”‚           â€¢ Eckart-Young optimality\n",
    "  â”‚           â€¢ Good for compression\n",
    "  â”‚\n",
    "  â”œâ”€ Detecting ANOMALIES?\n",
    "  â”‚  â””â”€ YES â†’ Use PCA Reconstruction Error\n",
    "  â”‚           â€¢ Fit on normal data\n",
    "  â”‚           â€¢ Flag large errors\n",
    "  â”‚\n",
    "  â””â”€ Very HIGH dimension (d > 10,000)?\n",
    "       â””â”€ YES â†’ Random Projection FIRST, then PCA\n",
    "                â€¢ Two-stage compression\n",
    "                â€¢ Saves computation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Hyperparameter Selection Guide\n",
    "\n",
    "### For PCA:\n",
    "\n",
    "| Parameter | How to Choose | Typical Values |\n",
    "|-----------|---------------|----------------|\n",
    "| **k** (components) | Look at scree plot, aim for 90-95% explained variance | 5-50 depending on d |\n",
    "| **Sample size n** | Need $n \\gtrsim 10 \\cdot d \\cdot k$ | As large as possible |\n",
    "\n",
    "### For Random Projection:\n",
    "\n",
    "| Parameter | How to Choose | Typical Values |\n",
    "|-----------|---------------|----------------|\n",
    "| **k** (dimension) | $k > 384\\ln(n)/\\varepsilon^2$ (JL formula) | Depends on $\\varepsilon$ |\n",
    "| **$\\varepsilon$** (error) | Trade-off: smaller = more accurate but larger k | 0.1-0.3 |\n",
    "\n",
    "### For Anomaly Detection:\n",
    "\n",
    "| Parameter | How to Choose | Typical Values |\n",
    "|-----------|---------------|----------------|\n",
    "| **k** (components) | Capture normal pattern (90% variance) | 5-20 |\n",
    "| **q** (quantile) | False positive rate: $1-q$ | 0.95-0.99 |\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Common Pitfalls & How to Avoid Them\n",
    "\n",
    "### 1. **Forgetting to Center Data**\n",
    "âŒ Running PCA on non-centered data  \n",
    "âœ… Always use `center_data()` before PCA  \n",
    "**Why**: PCA finds directions of variance around the mean\n",
    "\n",
    "### 2. **Too Few Samples**\n",
    "âŒ $n < d$ or $n < 10dk$  \n",
    "âœ… Check sample complexity bounds first  \n",
    "**Why**: Covariance estimate unreliable, excess risk large\n",
    "\n",
    "### 3. **Choosing k Arbitrarily**\n",
    "âŒ \"Let's use k=10 because it's a nice number\"  \n",
    "âœ… Look at explained variance ratio, scree plot, cross-validation  \n",
    "**Why**: Wrong k â†’ either under-fitting or over-fitting\n",
    "\n",
    "### 4. **Scaling Issues**\n",
    "âŒ Features on different scales (e.g., meters vs kilometers)  \n",
    "âœ… Standardize features first: $(x - \\mu)/\\sigma$  \n",
    "**Why**: PCA is sensitive to scale\n",
    "\n",
    "### 5. **Interpreting Components Incorrectly**\n",
    "âŒ \"PC1 is the most important feature\"  \n",
    "âœ… \"PC1 is the direction of maximum variance\"  \n",
    "**Why**: Components are linear combinations, not individual features\n",
    "\n",
    "### 6. **Overfitting in Anomaly Detection**\n",
    "âŒ Using same data for training and threshold selection  \n",
    "âœ… Split: train PCA on clean set, tune threshold on validation  \n",
    "**Why**: Avoid memorizing noise as \"normal\"\n",
    "\n",
    "### 7. **Ignoring Computational Cost**\n",
    "âŒ Full SVD on huge matrices  \n",
    "âœ… Use randomized SVD or iterative methods for large scale  \n",
    "**Why**: Full SVD is $O(nd^2)$ â€” too slow for big data\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Quick Function Reference\n",
    "\n",
    "### Random Projection\n",
    "```python\n",
    "k = jl_required_k(n_points=1000, eps=0.2)\n",
    "R = sample_subgaussian_matrix(d=500, k=k)\n",
    "Y = random_projection_map(X, R, scale_by_sqrt_k=True)\n",
    "errs = relative_distance_errors(X, Y)\n",
    "```\n",
    "\n",
    "### PCA\n",
    "```python\n",
    "pca = pca_fit(X_train, k=10)\n",
    "Z_train = pca_transform(X_train, pca)\n",
    "Z_test = pca_transform(X_test, pca)\n",
    "X_recon = pca_inverse_transform(Z_test, pca)\n",
    "```\n",
    "\n",
    "### SVD & Approximation\n",
    "```python\n",
    "X_k, svd_info = rank_k_approximation(X, k=10)\n",
    "error = reconstruction_error_frobenius(X, X_k)\n",
    "evr = explained_variance_ratio_from_singular_values(svd_info['S'], k=10)\n",
    "```\n",
    "\n",
    "### Anomaly Detection\n",
    "```python\n",
    "detector = pca_anomaly_detector_fit(X_train, k=5, q=0.99)\n",
    "pred = pca_anomaly_detector_predict(detector, X_test)\n",
    "anomalies = X_test[pred['is_anomaly']]\n",
    "```\n",
    "\n",
    "### Theoretical Bounds\n",
    "```python\n",
    "prob = matrix_bernstein_bound_thm11_15(d=50, n=1000, eps=0.1, C=25)\n",
    "excess_risk_bound = excess_risk_tail_bound_thm11_20(d=50, n=1000, \n",
    "                                                    eps=0.1, k=10, C=25)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "**To deepen understanding**:\n",
    "1. âœ… Run all cells with different parameters\n",
    "2. âœ… Apply to your own dataset\n",
    "3. âœ… Compare random projection vs PCA on same data\n",
    "4. âœ… Implement a mini-project (e.g., image compression)\n",
    "\n",
    "**Advanced topics to explore**:\n",
    "- Kernel PCA (nonlinear dimensionality reduction)\n",
    "- Sparse PCA (interpretable components)\n",
    "- Randomized SVD (fast large-scale methods)\n",
    "- t-SNE and UMAP (visualization methods)\n",
    "- Autoencoders (neural network dimensionality reduction)\n",
    "\n",
    "**Related chapters**:\n",
    "- Chapter 10: High-dimensional statistics\n",
    "- Chapter 8: Pattern recognition\n",
    "- Chapter 5-6: Statistical estimation theory\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ† Final Checklist\n",
    "\n",
    "Before using dimensionality reduction in practice:\n",
    "\n",
    "- [ ] Understand your data's intrinsic dimensionality\n",
    "- [ ] Check sample size requirements ($n \\gtrsim 10dk$)\n",
    "- [ ] Center (and maybe standardize) your data\n",
    "- [ ] Choose k using explained variance or cross-validation\n",
    "- [ ] Validate results on held-out test set\n",
    "- [ ] Interpret components carefully (linear combinations!)\n",
    "- [ ] Check theoretical bounds for confidence\n",
    "- [ ] Document your choices (k, preprocessing, etc.)\n",
    "\n",
    "**Congratulations! You now have a complete toolkit for dimensionality reduction!** ðŸŽ‰\n",
    "\n",
    "---\n",
    "\n",
    "*Created with care to make complex theory accessible. Questions? Review the detailed explanations above, run the demos, and experiment! ðŸš€*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
