{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201462b7",
   "metadata": {},
   "source": [
    "# Chapter 11 Toolkit — Dimensionality Reduction (Lecture Notes pp. 164–178)\n",
    "\n",
    "This notebook is a **complete, reusable toolkit** for Chapter 11:\n",
    "\n",
    "## 11.1 Random Projection + Johnson–Lindenstrauss\n",
    "- Random projection theorem (Thm 11.1): norm preservation for a fixed vector\n",
    "- JL lemma (Thm 11.2): preserve all pairwise distances for n points\n",
    "- Practical utilities: choose `k`, build random projection, measure distortion\n",
    "\n",
    "## 11.2–11.3 SVD + PCA\n",
    "- Singular vectors/values from `A^T A` (Lemma 11.7)\n",
    "- Greedy best-fit k-subspace interpretation (Thm 11.10)\n",
    "- SVD: `A = U D V^T`, PCA(A) = A V = U D (after centering)\n",
    "- Power method for top eigenvectors / singular vectors\n",
    "\n",
    "## 11.4 SVD in action\n",
    "- Rank-k approximation `A_k`\n",
    "- Reconstruction error and explained variance\n",
    "- Simple anomaly detection via reconstruction error quantiles\n",
    "\n",
    "## 11.5–11.6 Theory\n",
    "- Empirical covariance concentration (Matrix Bernstein, Thm 11.15)\n",
    "- Weyl’s theorem for eigenvalue stability (Thm 11.17)\n",
    "- Excess reconstruction risk bound (Lemma 11.19, Thm 11.20)\n",
    "\n",
    "> Run top-to-bottom once, then reuse the function sections by changing parameters (k, eps, d, n, C, etc.). ✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d856e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Tuple, Dict, Optional, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1a540",
   "metadata": {},
   "source": [
    "## 11.1 Random Projection (Thm 11.1) and JL Lemma (Thm 11.2)\n",
    "\n",
    "### Random projection map\n",
    "Given i.i.d. vectors \\(U_1,...,U_k \\in \\mathbb{R}^d\\), define:\n",
    "\\[\n",
    "f(v) = (U_1\\cdot v,\\dots,U_k\\cdot v)\\in\\mathbb{R}^k\n",
    "\\]\n",
    "\n",
    "**Thm 11.1** (for sub-Gaussian components, variance \\(a^2\\)): for \\(\\|v\\|=1\\),\n",
    "\\[\n",
    "\\mathbb{P}\\big(|\\|f(v)\\| - \\sqrt{k}|a||v|| \\ge \\varepsilon \\sqrt{k}|a||v|\\big) \\le 2e^{-k\\varepsilon^2/128}\n",
    "\\]\n",
    "\n",
    "**Thm 11.2 (JL)** for \\(n\\) points: if \\(k > \\frac{384\\ln(n)}{\\varepsilon^2}\\), then w.h.p.\n",
    "all pairwise distances are preserved up to factor \\(1\\pm \\varepsilon\\) (after dividing out \\(\\sqrt{k}\\)).\n",
    "\n",
    "Below are reusable functions that implement the algorithm and measure distortion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef78fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_projection_bound_thm11_1(k: int, eps: float) -> float:\n",
    "    \"\"\"Thm 11.1 tail bound: 2 exp(-k eps^2 / 128).\"\"\"\n",
    "    if k <= 0 or not (0 < eps < 1):\n",
    "        raise ValueError(\"k>0 and eps in (0,1).\")\n",
    "    return float(2.0 * math.exp(-k * eps * eps / 128.0))\n",
    "\n",
    "def jl_required_k(n_points: int, eps: float) -> int:\n",
    "    \"\"\"Thm 11.2 requirement: k > 384 ln(n) / eps^2.\"\"\"\n",
    "    if n_points <= 1 or not (0 < eps < 1):\n",
    "        raise ValueError(\"n_points>1 and eps in (0,1).\")\n",
    "    return int(math.floor(384.0 * math.log(n_points) / (eps * eps)) + 1)\n",
    "\n",
    "def jl_success_prob_lower(n_points: int) -> float:\n",
    "    \"\"\"From Thm 11.2: success prob >= 1 - 3/(2n).\"\"\"\n",
    "    if n_points <= 1:\n",
    "        raise ValueError(\"n_points>1.\")\n",
    "    return float(1.0 - 3.0 / (2.0 * n_points))\n",
    "\n",
    "def sample_subgaussian_matrix(d: int, k: int, a: float = 1.0, rng: Optional[np.random.Generator] = None) -> np.ndarray:\n",
    "    \"\"\"Practical choice: Gaussian N(0,a^2) entries are sub-Gaussian with parameter 1 (up to constants).\"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    return rng.normal(loc=0.0, scale=float(a), size=(k, d))\n",
    "\n",
    "def random_projection_map(X: np.ndarray, R: np.ndarray, scale_by_sqrt_k: bool = True) -> np.ndarray:\n",
    "    \"\"\"Project points X (n,d) using matrix R (k,d). Returns (n,k).\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    R = np.asarray(R, dtype=float)\n",
    "    Y = X @ R.T\n",
    "    if scale_by_sqrt_k:\n",
    "        Y = Y / math.sqrt(R.shape[0])\n",
    "    return Y\n",
    "\n",
    "def pairwise_distances(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"All-pairs Euclidean distances (n,n). O(n^2). Use for moderate n.\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    G = X @ X.T\n",
    "    sq = np.maximum(np.diag(G)[:, None] - 2*G + np.diag(G)[None, :], 0.0)\n",
    "    return np.sqrt(sq)\n",
    "\n",
    "def relative_distance_errors(X: np.ndarray, Y: np.ndarray, eps_floor: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Return relative errors |dY - dX| / dX for all i<j.\"\"\"\n",
    "    DX = pairwise_distances(X)\n",
    "    DY = pairwise_distances(Y)\n",
    "    n = DX.shape[0]\n",
    "    errs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            denom = max(float(DX[i, j]), eps_floor)\n",
    "            errs.append(abs(float(DY[i, j]) - float(DX[i, j])) / denom)\n",
    "    return np.array(errs, dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9788d5e",
   "metadata": {},
   "source": [
    "### Demo: JL distortion on synthetic data\n",
    "\n",
    "This demo shows:\n",
    "- choosing k using the JL formula\n",
    "- projecting data\n",
    "- plotting distribution of relative distance error\n",
    "\n",
    "(For large n, pairwise distances are O(n²). Keep n moderate.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_jl(n: int = 200, d: int = 800, eps: float = 0.25, a: float = 1.0, seed: int = 0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n, d))\n",
    "    k = jl_required_k(n, eps)\n",
    "    R = sample_subgaussian_matrix(d, k, a=a, rng=rng)\n",
    "    Y = random_projection_map(X, R, scale_by_sqrt_k=True)\n",
    "\n",
    "    errs = relative_distance_errors(X, Y)\n",
    "    return {\"n\": n, \"d\": d, \"k\": k, \"eps\": eps, \"success_prob_lb\": jl_success_prob_lower(n), \"errs\": errs}\n",
    "\n",
    "out = demo_jl(n=180, d=600, eps=0.30, seed=1)\n",
    "print({k:v for k,v in out.items() if k != \"errs\"})\n",
    "plt.figure()\n",
    "plt.hist(out[\"errs\"], bins=50)\n",
    "plt.title(\"Relative distance error distribution after JL projection\")\n",
    "plt.xlabel(\"relative error\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "print(\"fraction within eps:\", float(np.mean(out[\"errs\"] <= out[\"eps\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1936719",
   "metadata": {},
   "source": [
    "## 11.2 SVD: singular vectors/values and the power method (Lemma 11.7 + Sec 11.2.1)\n",
    "\n",
    "Given A (n×m), define first singular vector:\n",
    "\\[\n",
    "v_1 = \\arg\\max_{\\|v\\|=1} \\|A v\\|,\\quad \\sigma_1 = \\|A v_1\\|.\n",
    "\\]\n",
    "\n",
    "Lemma 11.7 says:\n",
    "- \\(v_1\\) is the top eigenvector of \\(A^T A\\) with eigenvalue \\(\\lambda_1\\)\n",
    "- \\(\\sigma_1 = \\sqrt{\\lambda_1}\\)\n",
    "\n",
    "We implement:\n",
    "- power method for top eigenvector of a symmetric matrix\n",
    "- top-k eigenvectors (deflation)\n",
    "- singular vectors/values from \\(A^T A\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method_top_eigenvector(\n",
    "    B: np.ndarray,\n",
    "    n_iter: int = 2000,\n",
    "    tol: float = 1e-10,\n",
    "    seed: int = 0,\n",
    ") -> Tuple[np.ndarray, float, Dict[str, object]]:\n",
    "    \"\"\"Top eigenvector for symmetric B via power method. Returns (v, eigenvalue_est, info).\"\"\"\n",
    "    B = np.asarray(B, dtype=float)\n",
    "    if B.shape[0] != B.shape[1]:\n",
    "        raise ValueError(\"B must be square.\")\n",
    "    # symmetric check is soft; allow small numerical asymmetry\n",
    "    if not np.allclose(B, B.T, atol=1e-8):\n",
    "        # symmetrize for safety\n",
    "        B = 0.5*(B + B.T)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    v = rng.normal(size=B.shape[0])\n",
    "    v /= np.linalg.norm(v)\n",
    "\n",
    "    prev = None\n",
    "    for it in range(n_iter):\n",
    "        w = B @ v\n",
    "        normw = np.linalg.norm(w)\n",
    "        if normw == 0:\n",
    "            raise RuntimeError(\"Power method hit zero vector; B may be zero.\")\n",
    "        v_new = w / normw\n",
    "        if prev is not None and np.linalg.norm(v_new - prev) < tol:\n",
    "            v = v_new\n",
    "            break\n",
    "        prev = v_new\n",
    "        v = v_new\n",
    "\n",
    "    eig = float(v @ (B @ v))\n",
    "    return v, eig, {\"iters\": it+1, \"eig_est\": eig}\n",
    "\n",
    "def top_k_eigenvectors_deflation(B: np.ndarray, k: int, seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute top-k eigenvectors/values of symmetric B using power method + deflation.\"\"\"\n",
    "    B = np.asarray(B, dtype=float)\n",
    "    if not np.allclose(B, B.T, atol=1e-8):\n",
    "        B = 0.5*(B + B.T)\n",
    "    n = B.shape[0]\n",
    "    V = np.zeros((n, k), dtype=float)\n",
    "    vals = np.zeros(k, dtype=float)\n",
    "    B_work = B.copy()\n",
    "\n",
    "    for j in range(k):\n",
    "        v, eig, _ = power_method_top_eigenvector(B_work, seed=seed+j)\n",
    "        V[:, j] = v\n",
    "        vals[j] = eig\n",
    "        # Deflate: B <- B - eig * v v^T\n",
    "        B_work = B_work - eig * np.outer(v, v)\n",
    "    return V, vals\n",
    "\n",
    "def svd_from_ata(A: np.ndarray, k: Optional[int] = None, seed: int = 0) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Compute right singular vectors/values via eigen-decomposition of A^T A.\"\"\"\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    B = A.T @ A  # (m,m)\n",
    "    m = B.shape[0]\n",
    "    if k is None:\n",
    "        k = m\n",
    "    k = min(k, m)\n",
    "\n",
    "    # For robustness, use numpy eig for full; power method for large m\n",
    "    if k == m and m <= 400:\n",
    "        eigvals, eigvecs = np.linalg.eigh(B)\n",
    "        order = np.argsort(eigvals)[::-1]\n",
    "        eigvals = eigvals[order]\n",
    "        V = eigvecs[:, order]\n",
    "    else:\n",
    "        V, eigvals = top_k_eigenvectors_deflation(B, k=k, seed=seed)\n",
    "\n",
    "    sigmas = np.sqrt(np.maximum(eigvals[:k], 0.0))\n",
    "    V = V[:, :k]\n",
    "    # Left singular vectors: u_i = A v_i / sigma_i (if sigma_i > 0)\n",
    "    U = np.zeros((A.shape[0], k), dtype=float)\n",
    "    for i in range(k):\n",
    "        if sigmas[i] > 1e-12:\n",
    "            U[:, i] = (A @ V[:, i]) / sigmas[i]\n",
    "    return {\"U\": U, \"S\": sigmas, \"V\": V}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f2f82",
   "metadata": {},
   "source": [
    "### Demo: SVD via numpy vs via \\(A^T A\\)\n",
    "\n",
    "We compare top singular values and directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa11857",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "A = rng.normal(size=(300, 40))\n",
    "res = svd_from_ata(A, k=6, seed=0)\n",
    "\n",
    "U_np, S_np, Vt_np = np.linalg.svd(A, full_matrices=False)\n",
    "print(\"Top-6 sigma (A^T A):\", np.round(res[\"S\"][:6], 6))\n",
    "print(\"Top-6 sigma (numpy):\", np.round(S_np[:6], 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fefbf0",
   "metadata": {},
   "source": [
    "## 11.3 PCA (coordinate change after centering)\n",
    "\n",
    "The notes assume the data matrix A has **empirical mean 0** (centered).\n",
    "\n",
    "If `A = U D V^T`, then PCA coordinates are:\n",
    "\\[\n",
    "\\mathrm{PCA}(A) = A V = U D.\n",
    "\\]\n",
    "\n",
    "So PCA is: **project onto right singular vectors** (columns of V).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Center rows of X (n,d). Returns (X_centered, mean_vector).\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    mu = np.mean(X, axis=0, keepdims=True)\n",
    "    return X - mu, mu.squeeze()\n",
    "\n",
    "def pca_fit(X: np.ndarray, k: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Fit PCA using SVD. Returns components (V_k), singular values, mean, scores.\"\"\"\n",
    "    Xc, mu = center_data(X)\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    V = Vt.T\n",
    "    Vk = V[:, :k]\n",
    "    scores = Xc @ Vk  # (n,k)\n",
    "    return {\"mean\": mu, \"components\": Vk, \"singular_values\": S, \"scores\": scores}\n",
    "\n",
    "def pca_transform(X: np.ndarray, pca: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Xc = X - pca[\"mean\"]\n",
    "    return Xc @ pca[\"components\"]\n",
    "\n",
    "def pca_inverse_transform(Z: np.ndarray, pca: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    Z = np.asarray(Z, dtype=float)\n",
    "    return Z @ pca[\"components\"].T + pca[\"mean\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177553a0",
   "metadata": {},
   "source": [
    "## 11.4 Rank-k approximation, reconstruction error, explained variance\n",
    "\n",
    "Given SVD: \\(A=\\sum_{j=1}^m \\sigma_j u_j v_j^T\\). Define rank-k approx:\n",
    "\\[\n",
    "A_k = \\sum_{j=1}^k \\sigma_j u_j v_j^T.\n",
    "\\]\n",
    "\n",
    "Reconstruction error (Frobenius):\n",
    "\\[\n",
    "\\|A-A_k\\|_F = \\sqrt{\\sum_{j=k+1}^m \\sigma_j^2}.\n",
    "\\]\n",
    "\n",
    "Explained variance ratio by first k components:\n",
    "\\[\n",
    "\\frac{\\sum_{j=1}^k \\sigma_j^2}{\\sum_{j=1}^m \\sigma_j^2}.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b93ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_k_approximation(X: np.ndarray, k: int) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "    \"\"\"Return X_k rank-k reconstruction using SVD on centered data? (caller decides).\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    Uk = U[:, :k]\n",
    "    Sk = S[:k]\n",
    "    Vtk = Vt[:k, :]\n",
    "    Xk = (Uk * Sk) @ Vtk\n",
    "    return Xk, {\"U\": U, \"S\": S, \"Vt\": Vt}\n",
    "\n",
    "def reconstruction_error_frobenius(X: np.ndarray, Xk: np.ndarray) -> float:\n",
    "    D = np.asarray(X, dtype=float) - np.asarray(Xk, dtype=float)\n",
    "    return float(np.linalg.norm(D, ord='fro'))\n",
    "\n",
    "def reconstruction_error_from_singular_values(S: np.ndarray, k: int) -> float:\n",
    "    S = np.asarray(S, dtype=float)\n",
    "    tail = S[k:]\n",
    "    return float(np.sqrt(np.sum(tail * tail)))\n",
    "\n",
    "def explained_variance_ratio_from_singular_values(S: np.ndarray, k: int) -> float:\n",
    "    S = np.asarray(S, dtype=float)\n",
    "    num = float(np.sum(S[:k] * S[:k]))\n",
    "    den = float(np.sum(S * S)) if np.sum(S*S) > 0 else float(\"nan\")\n",
    "    return num / den\n",
    "\n",
    "# Demo: compress synthetic data\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.normal(size=(500, 60))\n",
    "Xc, _ = center_data(X)\n",
    "\n",
    "X10, svd_info = rank_k_approximation(Xc, k=10)\n",
    "print(\"Recon error (fro):\", reconstruction_error_frobenius(Xc, X10))\n",
    "print(\"Recon error (sigma tail):\", reconstruction_error_from_singular_values(svd_info[\"S\"], k=10))\n",
    "print(\"Explained variance ratio k=10:\", explained_variance_ratio_from_singular_values(svd_info[\"S\"], k=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff9c38",
   "metadata": {},
   "source": [
    "### Optional demo: digits compression (like the notes' MNIST example)\n",
    "\n",
    "The notes mention MNIST; offline we can use scikit-learn's `load_digits` dataset (8×8 images).\n",
    "If scikit-learn isn't available in your environment, you can skip this cell safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: digits dataset compression (offline, small)\n",
    "try:\n",
    "    from sklearn.datasets import load_digits\n",
    "    digits = load_digits()\n",
    "    A = digits.data.astype(float)  # (n,64)\n",
    "    A_centered, mu = center_data(A)\n",
    "\n",
    "    k = 10\n",
    "    Ak, svd_info = rank_k_approximation(A_centered, k=k)\n",
    "    recon = Ak + mu\n",
    "\n",
    "    # Show 10 original vs reconstructed\n",
    "    idx = np.arange(10)\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    for i, j in enumerate(idx):\n",
    "        plt.subplot(2, 10, i+1)\n",
    "        plt.imshow(digits.images[j], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(2, 10, 10+i+1)\n",
    "        plt.imshow(recon[j].reshape(8,8), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Top row: original | Bottom row: rank-10 reconstruction\")\n",
    "    plt.show()\n",
    "\n",
    "    # Explained variance\n",
    "    evr10 = explained_variance_ratio_from_singular_values(svd_info[\"S\"], k=10)\n",
    "    print(\"Explained variance ratio (k=10):\", evr10)\n",
    "except Exception as e:\n",
    "    print(\"Skipped digits demo (likely sklearn missing):\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d13d29",
   "metadata": {},
   "source": [
    "## 11.4.3 Anomaly detection via reconstruction error\n",
    "\n",
    "Procedure (as described):\n",
    "1) Fit PCA/SVD on normal training data, choose k\n",
    "2) Reconstruct each training point and compute reconstruction error\n",
    "3) Choose a quantile threshold (e.g., 0.99)\n",
    "4) Flag test points with reconstruction error above threshold\n",
    "\n",
    "Below are reusable helpers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cde022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_errors_per_row(X: np.ndarray, Xk: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Xk = np.asarray(Xk, dtype=float)\n",
    "    return np.linalg.norm(X - Xk, axis=1)\n",
    "\n",
    "def anomaly_threshold_from_quantile(errors: np.ndarray, q: float = 0.99) -> float:\n",
    "    return float(np.quantile(np.asarray(errors, dtype=float), q))\n",
    "\n",
    "def pca_anomaly_detector_fit(X_train: np.ndarray, k: int, q: float = 0.99) -> Dict[str, object]:\n",
    "    pca = pca_fit(X_train, k=k)\n",
    "    Z = pca_transform(X_train, pca)\n",
    "    X_rec = pca_inverse_transform(Z, pca)\n",
    "    errs = reconstruction_errors_per_row(X_train, X_rec)\n",
    "    thr = anomaly_threshold_from_quantile(errs, q=q)\n",
    "    return {\"pca\": pca, \"k\": k, \"q\": q, \"threshold\": thr, \"train_errors\": errs}\n",
    "\n",
    "def pca_anomaly_detector_predict(det: Dict[str, object], X: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    pca = det[\"pca\"]\n",
    "    Z = pca_transform(X, pca)\n",
    "    X_rec = pca_inverse_transform(Z, pca)\n",
    "    errs = reconstruction_errors_per_row(X, X_rec)\n",
    "    flags = errs > det[\"threshold\"]\n",
    "    return {\"errors\": errs, \"is_anomaly\": flags, \"threshold\": float(det[\"threshold\"])}\n",
    "\n",
    "# Demo: create anomalies by adding heavy noise to some points\n",
    "rng = np.random.default_rng(0)\n",
    "X_train = rng.normal(size=(800, 40))\n",
    "X_test = rng.normal(size=(300, 40))\n",
    "X_test[:20] += rng.normal(scale=6.0, size=(20, 40))  # inject anomalies\n",
    "\n",
    "det = pca_anomaly_detector_fit(X_train, k=5, q=0.99)\n",
    "pred = pca_anomaly_detector_predict(det, X_test)\n",
    "\n",
    "print(\"threshold:\", pred[\"threshold\"])\n",
    "print(\"flagged anomalies:\", int(np.sum(pred[\"is_anomaly\"])))\n",
    "print(\"flagged among injected 20:\", int(np.sum(pred[\"is_anomaly\"][:20])))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(det[\"train_errors\"], bins=50, alpha=0.7, label=\"train errors\")\n",
    "plt.hist(pred[\"errors\"], bins=50, alpha=0.5, label=\"test errors\")\n",
    "plt.axvline(pred[\"threshold\"], linestyle='--', label=\"threshold\")\n",
    "plt.title(\"Reconstruction errors for anomaly detection\")\n",
    "plt.xlabel(\"||x - x_rec||\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ed763",
   "metadata": {},
   "source": [
    "## 11.5 Theory: empirical covariance concentration (Thm 11.15) + Weyl (Thm 11.17)\n",
    "\n",
    "Empirical covariance (centered data):\n",
    "\\[\n",
    "\\hat\\Sigma = \\frac1n\\sum_{i=1}^n X_i X_i^T.\n",
    "\\]\n",
    "\n",
    "**Matrix Bernstein** (Thm 11.15): if \\(\\|X_i\\|_2 \\le \\sqrt{C}\\) a.s. and Var(X_i)=Σ, then:\n",
    "\\[\n",
    "\\mathbb{P}(\\|\\hat\\Sigma-\\Sigma\\| > \\varepsilon)\\le 2d\\exp\\left(-\\frac{n\\varepsilon^2}{2C(C+2\\varepsilon/3)}\\right)\n",
    "\\]\n",
    "\n",
    "**Weyl**: if \\(\\hat\\Sigma = \\Sigma + E\\), then\n",
    "\\[\n",
    "\\max_i |\\hat\\lambda_i - \\lambda_i| \\le \\|E\\|.\n",
    "\\]\n",
    "\n",
    "Below are helpers to compute empirical covariance, operator norm, and the bounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d815ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_covariance_centered(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Assumes X is centered (mean ~ 0). Returns (d,d).\"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n = X.shape[0]\n",
    "    return (X.T @ X) / n\n",
    "\n",
    "def operator_norm(M: np.ndarray) -> float:\n",
    "    \"\"\"Spectral/operator norm (largest singular value).\"\"\"\n",
    "    M = np.asarray(M, dtype=float)\n",
    "    return float(np.linalg.norm(M, ord=2))\n",
    "\n",
    "def matrix_bernstein_bound_thm11_15(d: int, n: int, eps: float, C: float) -> float:\n",
    "    \"\"\"Thm 11.15: 2 d exp( - n eps^2 / (2 C (C + 2 eps/3)) ).\"\"\"\n",
    "    if d <= 0 or n <= 0 or eps <= 0 or C <= 0:\n",
    "        raise ValueError(\"d,n,eps,C must be positive.\")\n",
    "    denom = 2.0 * C * (C + 2.0*eps/3.0)\n",
    "    return float(2.0 * d * math.exp(- n * eps * eps / denom))\n",
    "\n",
    "def weyl_eigenvalue_deviation_bound(E: np.ndarray) -> float:\n",
    "    \"\"\"Thm 11.17 bound: max |λhat - λ| <= ||E||.\"\"\"\n",
    "    return operator_norm(E)\n",
    "\n",
    "# Tiny demo: covariance estimation error vs bound (C must be valid a.s. bound; here we clip)\n",
    "rng = np.random.default_rng(0)\n",
    "d = 20\n",
    "n = 4000\n",
    "X = rng.normal(size=(n, d))\n",
    "Xc, _ = center_data(X)\n",
    "\n",
    "# Artificially enforce a.s. bound by clipping norms (so C is valid)\n",
    "norms = np.linalg.norm(Xc, axis=1, keepdims=True)\n",
    "Xc_clip = Xc / np.maximum(1.0, norms/5.0)  # clip to norm<=5\n",
    "C = 25.0  # (sqrt(C)=5)\n",
    "\n",
    "Sigma_hat = empirical_covariance_centered(Xc_clip)\n",
    "# True Sigma is approx I for clipped; treat I as reference for demo\n",
    "E = Sigma_hat - np.eye(d)\n",
    "eps = operator_norm(E)\n",
    "\n",
    "print(\"||Sigma_hat - I|| =\", eps)\n",
    "print(\"Bernstein bound P(||...|| > eps) <= \", matrix_bernstein_bound_thm11_15(d, n, eps, C))\n",
    "print(\"Weyl bound on eigenvalue max deviation:\", weyl_eigenvalue_deviation_bound(E))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04257d73",
   "metadata": {},
   "source": [
    "## 11.6 Reconstruction error risk + excess risk (Lemma 11.19, Thm 11.20)\n",
    "\n",
    "Define the class of rank-k orthogonal projections:\n",
    "\\[\n",
    "\\mathcal{P}_k = \\{\\Pi:\\mathbb{R}^d\\to\\mathbb{R}^d \\mid \\Pi \\text{ is orthogonal projection of rank } k\\}.\n",
    "\\]\n",
    "\n",
    "Loss:\n",
    "\\[\n",
    "L(x,\\Pi(x)) = \\|x-\\Pi(x)\\|_2^2\n",
    "\\]\n",
    "Risk:\n",
    "\\[\n",
    "R(\\Pi) = \\mathbb{E}[L(X,\\Pi(X))]\n",
    "\\]\n",
    "\n",
    "Excess risk:\n",
    "\\[\n",
    "E_k = R(\\hat\\Pi_k^*) - R(\\Pi_k^*).\n",
    "\\]\n",
    "\n",
    "Lemma 11.19:\n",
    "\\[\n",
    "E_k \\le \\sqrt{2k}\\,\\|\\Sigma-\\hat\\Sigma\\|_2\n",
    "\\]\n",
    "\n",
    "Thm 11.20 (plugging Thm 11.15):\n",
    "\\[\n",
    "\\mathbb{P}(E_k > \\varepsilon)\\le 2d\\exp\\left(-\\frac{n\\varepsilon^2}{4C(C+2\\varepsilon/3)\\,k}\\right)\n",
    "\\]\n",
    "\n",
    "Below are helpers for these bounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def excess_risk_upper_from_cov_error(k: int, cov_error_op_norm: float) -> float:\n",
    "    \"\"\"Lemma 11.19: E_k <= sqrt(2k) * ||Sigma - Sigma_hat||.\"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k>0.\")\n",
    "    return float(math.sqrt(2.0 * k) * cov_error_op_norm)\n",
    "\n",
    "def excess_risk_tail_bound_thm11_20(d: int, n: int, eps: float, k: int, C: float) -> float:\n",
    "    \"\"\"Thm 11.20 tail bound for E_k.\"\"\"\n",
    "    if d <= 0 or n <= 0 or eps <= 0 or k <= 0 or C <= 0:\n",
    "        raise ValueError(\"d,n,eps,k,C positive.\")\n",
    "    denom = 4.0 * C * (C + 2.0*eps/3.0) * k\n",
    "    return float(2.0 * d * math.exp(- n * eps * eps / denom))\n",
    "\n",
    "# Demo numbers\n",
    "print(\"Example tail bound:\", excess_risk_tail_bound_thm11_20(d=64, n=2000, eps=0.5, k=10, C=25.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1304b8",
   "metadata": {},
   "source": [
    "## ✅ Function index (copy/paste friendly)\n",
    "\n",
    "### Random Projection + JL\n",
    "- `random_projection_bound_thm11_1(k, eps)`\n",
    "- `jl_required_k(n_points, eps)` + `jl_success_prob_lower(n_points)`\n",
    "- `sample_subgaussian_matrix(d, k, a, rng)`\n",
    "- `random_projection_map(X, R, scale_by_sqrt_k=True)`\n",
    "- `pairwise_distances(X)` + `relative_distance_errors(X, Y)`\n",
    "\n",
    "### SVD / power method (Lemma 11.7, Sec 11.2.1)\n",
    "- `power_method_top_eigenvector(B, n_iter, tol, seed)`\n",
    "- `top_k_eigenvectors_deflation(B, k, seed)`\n",
    "- `svd_from_ata(A, k, seed)`\n",
    "\n",
    "### PCA + compression\n",
    "- `center_data(X)`\n",
    "- `pca_fit(X, k)` + `pca_transform(X, pca)` + `pca_inverse_transform(Z, pca)`\n",
    "- `rank_k_approximation(X, k)`\n",
    "- `reconstruction_error_frobenius(X, Xk)`\n",
    "- `reconstruction_error_from_singular_values(S, k)`\n",
    "- `explained_variance_ratio_from_singular_values(S, k)`\n",
    "\n",
    "### Anomaly detection (Sec 11.4.3)\n",
    "- `pca_anomaly_detector_fit(X_train, k, q)`\n",
    "- `pca_anomaly_detector_predict(det, X)`\n",
    "\n",
    "### Theory bounds (Thm 11.15, 11.17, 11.19, 11.20)\n",
    "- `empirical_covariance_centered(X_centered)`\n",
    "- `operator_norm(M)`\n",
    "- `matrix_bernstein_bound_thm11_15(d, n, eps, C)`\n",
    "- `weyl_eigenvalue_deviation_bound(E)`\n",
    "- `excess_risk_upper_from_cov_error(k, cov_error_op_norm)`\n",
    "- `excess_risk_tail_bound_thm11_20(d, n, eps, k, C)`\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
