{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6747a8",
   "metadata": {},
   "source": [
    "# Chapter 4 — Risk, Supervised Learning, Classification, and MLE (Reusable Python Toolkit)\n",
    "\n",
    "This notebook is a **practical companion** to the provided lecture notes PDF (Chapter 4: *Risk*).  \n",
    "It includes:\n",
    "- Concise explanations of each concept in the notes  \n",
    "- **Reusable Python functions** for each important component (risk, empirical risk, ERM, regression-function estimation, Bayes rule, MLE, linear/logistic regression)\n",
    "\n",
    "> **Tip**: You can reuse these functions by only changing **inputs/parameters** (data arrays, loss choice, model function, optimizer settings, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330b3eb",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "We'll use standard scientific Python libraries. If something is missing in your environment, install with:\n",
    "```bash\n",
    "pip install numpy scipy scikit-learn matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578587a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Iterable, Optional, Tuple, Union\n",
    "\n",
    "# Optional imports used in later sections (safe if unavailable until used)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4925e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical helpers\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Stable logistic sigmoid: σ(x) = 1/(1+exp(-x)).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input values (any shape)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Sigmoid-transformed values, same shape as input\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    # Avoid overflow: split positive/negative\n",
    "    out = np.empty_like(x, dtype=float)\n",
    "    pos = x >= 0\n",
    "    out[pos] = 1.0 / (1.0 + np.exp(-x[pos]))\n",
    "    expx = np.exp(x[~pos])\n",
    "    out[~pos] = expx / (1.0 + expx)\n",
    "    return out\n",
    "\n",
    "def add_intercept(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Append a column of ones to X for intercept term in linear models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix, shape (n_samples,) or (n_samples, n_features)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Design matrix with intercept column, shape (n_samples, n_features+1)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71211d29",
   "metadata": {},
   "source": [
    "## 1) Statistical Models and Risk\n",
    "\n",
    "A **statistical model** is a family of probability distributions $\\{f(x;\\theta) : \\theta\\in\\Theta\\}$, where:\n",
    "- $f(x;\\theta)$ = probability density/mass function parameterized by $\\theta$\n",
    "- $\\theta$ = model parameter(s)\n",
    "- $\\Theta$ = parameter space (set of all possible parameter values)\n",
    "- **Parametric**: The parameter $\\theta$ is finite-dimensional (e.g., $\\theta=(\\mu,\\sigma)$ for normal distribution)\n",
    "- **Non-parametric**: The parameter space $\\Theta$ is infinite-dimensional (e.g., all possible continuous functions)\n",
    "\n",
    "In supervised learning, we observe training pairs $(X_i, Y_i)$ and select a function $g$ from a model class $\\mathcal{M}$ that minimizes the **risk**:\n",
    "\n",
    "$$\n",
    "R(g)=\\mathbb{E}[L(Z,g)]\\quad\\text{where } Z=(X,Y)\n",
    "$$\n",
    "\n",
    "**Variable definitions:**\n",
    "- $Z = (X,Y)$ = random variable pair (feature, label)\n",
    "- $X$ = feature/input variable\n",
    "- $Y$ = response/output/label variable  \n",
    "- $g$ = prediction function from model class $\\mathcal{M}$\n",
    "- $L(Z,g)$ = loss function measuring prediction error for data point $Z$ using predictor $g$\n",
    "- $R(g)$ = risk (expected loss)\n",
    "- $\\mathbb{E}[\\cdot]$ = expectation operator\n",
    "\n",
    "In practice, we minimize **empirical risk** $\\hat{R}(g) = \\frac{1}{n}\\sum_{i=1}^n L(Z_i, g)$ (average loss on training set) as an approximation of true risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loss functions (plug-and-play) ---\n",
    "\n",
    "def squared_loss(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Pointwise squared loss (y - yhat)^2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True target values, shape (n_samples,)\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values, shape (n_samples,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Squared loss for each sample, shape (n_samples,)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "def zero_one_loss(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Pointwise 0-1 loss: 1 if wrong else 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True class labels, shape (n_samples,)\n",
    "    y_pred : np.ndarray\n",
    "        Predicted class labels, shape (n_samples,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        0-1 loss for each sample (0=correct, 1=incorrect), shape (n_samples,)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    return (y_true != y_pred).astype(float)\n",
    "\n",
    "def negative_log_likelihood_loss(logpdf: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "                                 params: np.ndarray,\n",
    "                                 z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Generic negative log-likelihood loss: -log p_params(z).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logpdf : Callable\n",
    "        Function that computes log p(z; params) for each data point\n",
    "    params : np.ndarray\n",
    "        Model parameters\n",
    "    z : np.ndarray\n",
    "        Data points\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Negative log-likelihood for each data point\n",
    "    \"\"\"\n",
    "    return -logpdf(params, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881da858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Risk utilities ---\n",
    "\n",
    "def empirical_risk(loss_fn: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "                   y_true: np.ndarray,\n",
    "                   y_pred: np.ndarray,\n",
    "                   reducer: Callable[[np.ndarray], float] = np.mean) -> float:\n",
    "    \"\"\"Compute empirical risk: average (or other aggregate) of pointwise losses.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_fn : Callable\n",
    "        Loss function mapping (y_true, y_pred) to pointwise loss array\n",
    "    y_true : np.ndarray\n",
    "        True target values, shape (n_samples,)\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values, shape (n_samples,)\n",
    "    reducer : Callable, optional\n",
    "        Aggregation function (default: np.mean)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Empirical risk (aggregated loss)\n",
    "    \"\"\"\n",
    "    return float(reducer(loss_fn(y_true, y_pred)))\n",
    "\n",
    "def monte_carlo_risk(loss_on_sample: Callable[[np.ndarray], np.ndarray],\n",
    "                     sampler: Callable[[int], np.ndarray],\n",
    "                     n_mc: int = 10000,\n",
    "                     reducer: Callable[[np.ndarray], float] = np.mean,\n",
    "                     random_state: Optional[int] = None) -> float:\n",
    "    \"\"\"Approximate R = E[L(Z)] by Monte Carlo sampling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_on_sample : Callable\n",
    "        Function that computes loss given sampled data Z\n",
    "    sampler : Callable\n",
    "        Function to generate n_mc samples from distribution of Z\n",
    "    n_mc : int, optional\n",
    "        Number of Monte Carlo samples (default: 10000)\n",
    "    reducer : Callable, optional\n",
    "        Aggregation function (default: np.mean)\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Monte Carlo estimate of expected risk\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    Z = sampler(n_mc) if sampler.__code__.co_argcount == 1 else sampler(n_mc, rng)\n",
    "    losses = loss_on_sample(Z)\n",
    "    return float(reducer(losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cef3d3",
   "metadata": {},
   "source": [
    "### Empirical Risk Minimization (ERM) Template\n",
    "\n",
    "The lecture notes frame learning as: choose a model class $\\mathcal{M}=\\{g_\\lambda\\}$ parameterized by $\\lambda$, and minimize risk over $\\lambda$.\n",
    "\n",
    "**Variable definitions:**\n",
    "- $\\mathcal{M}$ = model class (set of candidate prediction functions)\n",
    "- $g_\\lambda$ = prediction function parameterized by $\\lambda$\n",
    "- $\\lambda$ = model parameters to optimize\n",
    "\n",
    "Below is a **general ERM optimizer**: you provide a parameterized model, a loss function, and data; it returns the fitted parameters by minimizing the average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50541f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def erm_fit(model_predict: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "            loss_fn: Callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            init_params: np.ndarray,\n",
    "            method: str = \"L-BFGS-B\",\n",
    "            bounds: Optional[Iterable[Tuple[Optional[float], Optional[float]]]] = None,\n",
    "            options: Optional[Dict] = None) -> Dict:\n",
    "    \"\"\"Generic ERM: minimize average loss over parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_predict : Callable\n",
    "        Function with signature (params, X) -> y_pred that generates predictions\n",
    "    loss_fn : Callable\n",
    "        Function with signature (y_true, y_pred) -> pointwise loss array\n",
    "    X : np.ndarray\n",
    "        Feature matrix, shape (n_samples, n_features) or (n_samples,)\n",
    "    y : np.ndarray\n",
    "        Target values, shape (n_samples,)\n",
    "    init_params : np.ndarray\n",
    "        Initial parameter vector for optimization\n",
    "    method : str, optional\n",
    "        Scipy optimization method (default: \"L-BFGS-B\")\n",
    "    bounds : Iterable of tuples, optional\n",
    "        Parameter bounds as [(min1, max1), (min2, max2), ...]\n",
    "    options : dict, optional\n",
    "        Additional options for scipy.optimize.minimize\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with keys:\n",
    "        - params: optimized parameter vector\n",
    "        - fun: final objective value (empirical risk)\n",
    "        - success: whether optimization succeeded\n",
    "        - message: optimization status message\n",
    "        - result: full scipy optimization result object\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    def objective(params):\n",
    "        y_pred = model_predict(params, X)\n",
    "        return np.mean(loss_fn(y, y_pred))\n",
    "\n",
    "    res = minimize(objective, np.asarray(init_params, dtype=float),\n",
    "                   method=method, bounds=bounds, options=options)\n",
    "    return {\n",
    "        \"params\": res.x,\n",
    "        \"fun\": float(res.fun),\n",
    "        \"success\": bool(res.success),\n",
    "        \"message\": res.message,\n",
    "        \"result\": res\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3a83d",
   "metadata": {},
   "source": [
    "## 2) Regression and the Regression Function $r(x)=\\mathbb{E}[Y\\mid X=x]$\n",
    "\n",
    "In the notes, under squared loss, the risk is:\n",
    "\n",
    "$$\n",
    "R(g)=\\mathbb{E}[(Y-g(X))^2]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Y$ = true response variable\n",
    "- $g(X)$ = predicted value using function $g$ and features $X$\n",
    "- $R(g)$ = expected squared error (risk)\n",
    "\n",
    "This decomposes into three terms:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(Y-r(X))^2] + \\mathbb{E}[(r(X)-g(X))^2] + 2\\,\\mathbb{E}[(Y-r(X))(r(X)-g(X))]\n",
    "$$\n",
    "\n",
    "**Term definitions:**\n",
    "- $r(X) = \\mathbb{E}[Y\\mid X]$ = regression function (conditional expectation, the optimal predictor)\n",
    "- First term: $\\mathbb{E}[(Y-r(X))^2]$ = **irreducible error** (noise, Bayes error)\n",
    "- Second term: $\\mathbb{E}[(r(X)-g(X))^2]$ = **approximation error** (how far $g$ is from optimal $r$)\n",
    "- Third term: $2\\mathbb{E}[(Y-r(X))(r(X)-g(X))]$ = **cross term** (equals zero by tower property)\n",
    "\n",
    "**Key insight**: Minimizing squared-loss risk within a model class $\\mathcal{M}$ is equivalent to finding the member of $\\mathcal{M}$ closest to $r(x)$ in the mean-square sense.\n",
    "\n",
    "Below: \n",
    "- **(A)** A utility to compute the three decomposition terms given samples and an estimate of $r(X)$  \n",
    "- **(B)** Reusable estimators for $r(x)$ (kernel regression & k-NN regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_decomposition(y: np.ndarray,\n",
    "                      gX: np.ndarray,\n",
    "                      rX: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Empirical version of MSE decomposition:\n",
    "       E[(Y-g)^2] = E[(Y-r)^2] + E[(r-g)^2] + 2E[(Y-r)(r-g)]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        True response values, shape (n_samples,)\n",
    "    gX : np.ndarray\n",
    "        Model predictions g(X), shape (n_samples,)\n",
    "    rX : np.ndarray\n",
    "        Regression function values r(X) = E[Y|X], shape (n_samples,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with keys:\n",
    "        - I_noise: irreducible error E[(Y-r)^2]\n",
    "        - II_approx: approximation error E[(r-g)^2]\n",
    "        - III_cross: cross term 2E[(Y-r)(r-g)] (should be ≈0)\n",
    "        - total: total MSE E[(Y-g)^2]\n",
    "    \"\"\"\n",
    "    y = np.asarray(y); gX = np.asarray(gX); rX = np.asarray(rX)\n",
    "    I = np.mean((y - rX) ** 2)\n",
    "    II = np.mean((rX - gX) ** 2)\n",
    "    III = 2.0 * np.mean((y - rX) * (rX - gX))\n",
    "    total = np.mean((y - gX) ** 2)\n",
    "    return {\"I_noise\": float(I), \"II_approx\": float(II), \"III_cross\": float(III), \"total\": float(total)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_regression_predict(X_train: np.ndarray,\n",
    "                              y_train: np.ndarray,\n",
    "                              X_query: np.ndarray,\n",
    "                              bandwidth: float = 0.1,\n",
    "                              kernel: str = \"gaussian\") -> np.ndarray:\n",
    "    \"\"\"Nadaraya–Watson kernel regression: estimate r(x)=E[Y|X=x].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training features, shape (n_train,) or (n_train, n_features)\n",
    "    y_train : np.ndarray\n",
    "        Training targets, shape (n_train,)\n",
    "    X_query : np.ndarray\n",
    "        Query points where to predict, shape (n_query,) or (n_query, n_features)\n",
    "    bandwidth : float, optional\n",
    "        Smoothing parameter h (larger = smoother, default: 0.1)\n",
    "    kernel : str, optional\n",
    "        Kernel type: 'gaussian' or 'epanechnikov' (default: 'gaussian')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_hat : np.ndarray\n",
    "        Predicted values at query points, shape (n_query,)\n",
    "    \"\"\"\n",
    "    X_train = np.asarray(X_train); y_train = np.asarray(y_train)\n",
    "    X_query = np.asarray(X_query)\n",
    "\n",
    "    if X_train.ndim == 1:\n",
    "        X_train = X_train.reshape(-1, 1)\n",
    "    if X_query.ndim == 1:\n",
    "        X_query = X_query.reshape(-1, 1)\n",
    "\n",
    "    # pairwise squared distances\n",
    "    diffs = X_query[:, None, :] - X_train[None, :, :]\n",
    "    d2 = np.sum(diffs * diffs, axis=-1)\n",
    "    u2 = d2 / (bandwidth ** 2)\n",
    "\n",
    "    if kernel == \"gaussian\":\n",
    "        w = np.exp(-0.5 * u2)\n",
    "    elif kernel == \"epanechnikov\":\n",
    "        u = np.sqrt(u2)\n",
    "        w = np.clip(1 - u**2, 0, None)\n",
    "    else:\n",
    "        raise ValueError(\"kernel must be 'gaussian' or 'epanechnikov'\")\n",
    "\n",
    "    denom = np.sum(w, axis=1)\n",
    "    # avoid division by zero\n",
    "    denom = np.where(denom == 0, 1e-12, denom)\n",
    "    return (w @ y_train) / denom\n",
    "\n",
    "def knn_regression_predict(X_train: np.ndarray,\n",
    "                           y_train: np.ndarray,\n",
    "                           X_query: np.ndarray,\n",
    "                           k: int = 10) -> np.ndarray:\n",
    "    \"\"\"k-Nearest Neighbors regression: r(x) ≈ average of k nearest y values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training features, shape (n_train,) or (n_train, n_features)\n",
    "    y_train : np.ndarray\n",
    "        Training targets, shape (n_train,)\n",
    "    X_query : np.ndarray\n",
    "        Query points where to predict, shape (n_query,) or (n_query, n_features)\n",
    "    k : int, optional\n",
    "        Number of nearest neighbors to average (default: 10)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_hat : np.ndarray\n",
    "        Predicted values at query points, shape (n_query,)\n",
    "    \"\"\"\n",
    "    X_train = np.asarray(X_train); y_train = np.asarray(y_train)\n",
    "    X_query = np.asarray(X_query)\n",
    "\n",
    "    if X_train.ndim == 1:\n",
    "        X_train = X_train.reshape(-1, 1)\n",
    "    if X_query.ndim == 1:\n",
    "        X_query = X_query.reshape(-1, 1)\n",
    "\n",
    "    diffs = X_query[:, None, :] - X_train[None, :, :]\n",
    "    d2 = np.sum(diffs * diffs, axis=-1)\n",
    "    idx = np.argpartition(d2, kth=min(k-1, d2.shape[1]-1), axis=1)[:, :k]\n",
    "    return np.mean(y_train[idx], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379174f7",
   "metadata": {},
   "source": [
    "### Mini Demo: Regression Risk + Decomposition\n",
    "\n",
    "This is a **toy** illustration to demonstrate the MSE decomposition. Replace the synthetic generator with your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30572ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_synthetic_regression(n: int = 300, noise_std: float = 0.15, random_state: int = 0):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X = rng.uniform(0, 1, size=n)\n",
    "    f = lambda x: np.sin(2*np.pi*x) + 0.3*np.cos(6*np.pi*x)\n",
    "    r = f(X)\n",
    "    y = r + rng.normal(0, noise_std, size=n)\n",
    "    return X, y, f\n",
    "\n",
    "X, y, f = make_synthetic_regression()\n",
    "x_grid = np.linspace(0, 1, 400)\n",
    "r_grid = f(x_grid)\n",
    "\n",
    "# Estimate r(x) with kernel regression\n",
    "rhat_grid = kernel_regression_predict(X, y, x_grid, bandwidth=0.08)\n",
    "\n",
    "# A simple model class example: polynomial degree 3 fitted by least squares\n",
    "Phi = np.vstack([np.ones_like(X), X, X**2, X**3]).T\n",
    "beta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n",
    "def poly3_predict(params, Xq):\n",
    "    Xq = np.asarray(Xq)\n",
    "    return params[0] + params[1]*Xq + params[2]*Xq**2 + params[3]*Xq**3\n",
    "\n",
    "g_grid = poly3_predict(beta, x_grid)\n",
    "\n",
    "# Decomposition terms (using rhat as proxy for r)\n",
    "rhat_X = kernel_regression_predict(X, y, X, bandwidth=0.08)\n",
    "g_X = poly3_predict(beta, X)\n",
    "terms = mse_decomposition(y, g_X, rhat_X)\n",
    "terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X, y, s=12, alpha=0.35, label=\"data\")\n",
    "plt.plot(x_grid, r_grid, linewidth=2, label=\"true r(x)\")\n",
    "plt.plot(x_grid, rhat_grid, linewidth=2, label=\"kernel r̂(x)\")\n",
    "plt.plot(x_grid, g_grid, linewidth=2, label=\"poly3 g(x)\")\n",
    "plt.legend()\n",
    "plt.title(\"Regression function and a model approximation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0c67d",
   "metadata": {},
   "source": [
    "## 3) Pattern Recognition (Classification) and Bayes Rule\n",
    "\n",
    "In classification, labels are discrete (often $\\{0,1\\}$ for binary), and the notes use **0–1 loss**:\n",
    "\n",
    "$$\n",
    "L(y,u)=\\begin{cases}\n",
    "0 & \\text{if } y=u\\\\\n",
    "1 & \\text{if } y\\neq u\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y$ = true class label\n",
    "- $u$ = predicted class label\n",
    "- $L(y,u)$ = loss (0 if correct, 1 if incorrect)\n",
    "\n",
    "Risk equals the **misclassification probability**:\n",
    "\n",
    "$$\n",
    "R(g)=\\mathbb{P}(Y\\neq g(X))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $g(X)$ = classifier function that predicts class given features $X$\n",
    "- $R(g)$ = probability of misclassification\n",
    "\n",
    "For binary classification, define the **posterior probability**:\n",
    "\n",
    "$$\n",
    "r(x)=\\mathbb{P}(Y=1\\mid X=x)\n",
    "$$\n",
    "\n",
    "where $r(x)$ is the conditional probability of class 1 given features $x$.\n",
    "\n",
    "The **Bayes rule** (optimal classifier) is:\n",
    "- Predict 1 if $r(x)>1/2$, else predict 0\n",
    "\n",
    "This rule minimizes the 0-1 loss risk and achieves the **Bayes error** $R^* = \\mathbb{E}[\\min(r(X), 1-r(X))]$.\n",
    "\n",
    "Below are reusable helpers for the Bayes decision rule and computing empirical misclassification risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db60df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_rule_binary(p_y1_given_x: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"Bayes decision rule for binary classification given posterior P(Y=1|X).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_y1_given_x : np.ndarray\n",
    "        Posterior probabilities P(Y=1|X), shape (n_samples,)\n",
    "    threshold : float, optional\n",
    "        Decision threshold (default: 0.5 for 0-1 loss)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predicted class labels (0 or 1), shape (n_samples,)\n",
    "    \"\"\"\n",
    "    p = np.asarray(p_y1_given_x)\n",
    "    return (p > threshold).astype(int)\n",
    "\n",
    "def misclassification_rate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Compute empirical misclassification rate (0-1 error).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True class labels, shape (n_samples,)\n",
    "    y_pred : np.ndarray\n",
    "        Predicted class labels, shape (n_samples,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Fraction of misclassified samples\n",
    "    \"\"\"\n",
    "    return float(np.mean((np.asarray(y_true) != np.asarray(y_pred)).astype(float)))\n",
    "\n",
    "def bayes_error_from_posterior(p_y1_given_x: np.ndarray) -> float:\n",
    "    \"\"\"Compute Bayes error if you know the true posterior r(X)=P(Y=1|X).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_y1_given_x : np.ndarray\n",
    "        True posterior probabilities r(X), shape (n_samples,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Bayes error = E[min(r(X), 1-r(X))]\n",
    "    \"\"\"\n",
    "    p = np.asarray(p_y1_given_x)\n",
    "    return float(np.mean(np.minimum(p, 1 - p)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be6587",
   "metadata": {},
   "source": [
    "### Estimating $r(x)=P(Y=1|X=x)$ from Data\n",
    "\n",
    "In practice, you need to estimate the posterior probability from training data. Two common approaches:\n",
    "\n",
    "1. **Logistic regression** (parametric conditional model)  \n",
    "2. **k-NN / kernel methods** (non-parametric)\n",
    "\n",
    "We'll implement a reusable logistic regression MLE next (Section 5).  \n",
    "Here is a simple k-NN posterior estimator you can use immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c24776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_posterior_predict(X_train: np.ndarray,\n",
    "                          y_train: np.ndarray,\n",
    "                          X_query: np.ndarray,\n",
    "                          k: int = 25) -> np.ndarray:\n",
    "    \"\"\"Estimate posterior r(x)=P(Y=1|X=x) via k-NN (average of neighbor labels).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training features, shape (n_train,) or (n_train, n_features)\n",
    "    y_train : np.ndarray\n",
    "        Training binary labels (0 or 1), shape (n_train,)\n",
    "    X_query : np.ndarray\n",
    "        Query points where to estimate posterior, shape (n_query,) or (n_query, n_features)\n",
    "    k : int, optional\n",
    "        Number of nearest neighbors (default: 25)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Estimated P(Y=1|X) at query points, shape (n_query,)\n",
    "    \"\"\"\n",
    "    yhat = knn_regression_predict(X_train, y_train, X_query, k=k)\n",
    "    # since y in {0,1}, average is an estimate of P(Y=1|X)\n",
    "    return np.clip(yhat, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16074a4",
   "metadata": {},
   "source": [
    "### Mini Demo: Bayes Rule Intuition (Synthetic)\n",
    "\n",
    "We'll generate data from a known model so we can compute the **true** posterior and compare it with estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50fd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_binary(n: int = 600, random_state: int = 0):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    # X in R^2; two Gaussians\n",
    "    n0 = n // 2\n",
    "    n1 = n - n0\n",
    "    X0 = rng.normal(loc=(-1.0, 0.0), scale=0.9, size=(n0, 2))\n",
    "    X1 = rng.normal(loc=(+1.0, 0.0), scale=0.9, size=(n1, 2))\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.array([0]*n0 + [1]*n1)\n",
    "    # Shuffle\n",
    "    idx = rng.permutation(n)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "Xc, yc = make_synthetic_binary()\n",
    "\n",
    "# Posterior estimate via kNN\n",
    "p_hat = knn_posterior_predict(Xc, yc, Xc, k=35)\n",
    "y_hat = bayes_rule_binary(p_hat, threshold=0.5)\n",
    "misclassification_rate(yc, y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4038e",
   "metadata": {},
   "source": [
    "## 4) Maximum Likelihood Estimation (MLE) as Risk Minimization\n",
    "\n",
    "The notes show: if your model is a parametric density $p_\\alpha(z)$, and you choose the loss function\n",
    "\n",
    "$$\n",
    "L(z,\\alpha)=-\\log p_\\alpha(z),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $z$ = observed data point\n",
    "- $\\alpha$ = model parameters\n",
    "- $p_\\alpha(z)$ = probability density/mass function of data $z$ under parameters $\\alpha$\n",
    "- $L(z,\\alpha)$ = negative log-likelihood loss\n",
    "\n",
    "then the risk is the **expected negative log-likelihood**, and empirical risk is the average negative log-likelihood over samples:\n",
    "\n",
    "$$\n",
    "\\hat R(\\alpha)=-\\frac{1}{n}\\sum_{i=1}^n \\log p_\\alpha(Z_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ = number of samples\n",
    "- $Z_i$ = $i$-th data point\n",
    "- $\\hat R(\\alpha)$ = empirical risk (average NLL)\n",
    "\n",
    "**Key insight**: Minimizing this empirical risk is **equivalent** to maximizing the likelihood function $\\mathcal{L}(\\alpha) = \\prod_{i=1}^n p_\\alpha(Z_i)$.\n",
    "\n",
    "Below are general MLE helpers + concrete examples (Gaussian, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bd47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_fit(nll_fn: Callable[[np.ndarray, np.ndarray], float],\n",
    "            data: np.ndarray,\n",
    "            init_params: np.ndarray,\n",
    "            method: str = \"L-BFGS-B\",\n",
    "            bounds: Optional[Iterable[Tuple[Optional[float], Optional[float]]]] = None,\n",
    "            options: Optional[Dict] = None) -> Dict:\n",
    "    \"\"\"Generic MLE by minimizing negative log-likelihood (sum or mean).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nll_fn : Callable\n",
    "        Negative log-likelihood function with signature (params, data) -> float\n",
    "    data : np.ndarray\n",
    "        Observed data samples\n",
    "    init_params : np.ndarray\n",
    "        Initial parameter values for optimization\n",
    "    method : str, optional\n",
    "        Scipy optimization method (default: \"L-BFGS-B\")\n",
    "    bounds : Iterable of tuples, optional\n",
    "        Parameter bounds as [(min1, max1), (min2, max2), ...]\n",
    "    options : dict, optional\n",
    "        Additional options for scipy.optimize.minimize\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with keys:\n",
    "        - params: MLE parameter estimates\n",
    "        - fun: final NLL value\n",
    "        - success: whether optimization succeeded\n",
    "        - message: optimization status message\n",
    "        - result: full scipy optimization result object\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "\n",
    "    def objective(params):\n",
    "        return float(nll_fn(params, data))\n",
    "\n",
    "    res = minimize(objective, np.asarray(init_params, dtype=float),\n",
    "                   method=method, bounds=bounds, options=options)\n",
    "    return {\"params\": res.x, \"fun\": float(res.fun), \"success\": bool(res.success),\n",
    "            \"message\": res.message, \"result\": res}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79de7bb",
   "metadata": {},
   "source": [
    "### Example: Gaussian MLE $(\\mu,\\sigma)$\n",
    "\n",
    "For i.i.d. samples $z_1,\\dots,z_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$, MLE has a closed-form solution:\n",
    "\n",
    "$$\\hat\\mu = \\frac{1}{n}\\sum_{i=1}^n z_i$$\n",
    "\n",
    "$$\\hat\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (z_i-\\hat\\mu)^2}$$\n",
    "\n",
    "where:\n",
    "- $z_i$ = $i$-th sample\n",
    "- $n$ = number of samples\n",
    "- $\\mu$ = mean parameter\n",
    "- $\\sigma$ = standard deviation parameter\n",
    "- $\\hat\\mu$ = MLE estimate of mean (sample mean)\n",
    "- $\\hat\\sigma$ = MLE estimate of std dev (uses $1/n$, not $1/(n-1)$ like unbiased estimator)\n",
    "\n",
    "We'll provide both closed-form and numeric optimization versions for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_logpdf(params: np.ndarray, z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Log-density of N(mu, sigma^2) for each z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : np.ndarray\n",
    "        Parameters [mu, sigma] where mu=mean, sigma=std dev\n",
    "    z : np.ndarray\n",
    "        Data points\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Log-probability density for each data point\n",
    "    \"\"\"\n",
    "    mu, sigma = float(params[0]), float(params[1])\n",
    "    z = np.asarray(z)\n",
    "    if sigma <= 0:\n",
    "        return np.full_like(z, -np.inf, dtype=float)\n",
    "    return -0.5*np.log(2*np.pi*sigma**2) - 0.5*((z - mu)/sigma)**2\n",
    "\n",
    "def gaussian_nll(params: np.ndarray, z: np.ndarray) -> float:\n",
    "    \"\"\"Negative log-likelihood (sum) for Gaussian distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : np.ndarray\n",
    "        Parameters [mu, sigma]\n",
    "    z : np.ndarray\n",
    "        Data samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Total negative log-likelihood\n",
    "    \"\"\"\n",
    "    ll = gaussian_logpdf(params, z)\n",
    "    if np.isneginf(ll).any():\n",
    "        return float(\"inf\")\n",
    "    return float(-np.sum(ll))\n",
    "\n",
    "def fit_gaussian_mle_closed_form(z: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Fit Gaussian parameters using closed-form MLE formulas.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray\n",
    "        Data samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with keys 'mu' (mean) and 'sigma' (std dev)\n",
    "    \"\"\"\n",
    "    z = np.asarray(z)\n",
    "    mu_hat = float(np.mean(z))\n",
    "    sigma_hat = float(np.sqrt(np.mean((z - mu_hat)**2)))\n",
    "    return {\"mu\": mu_hat, \"sigma\": sigma_hat}\n",
    "\n",
    "def fit_gaussian_mle_numeric(z: np.ndarray,\n",
    "                             init_params: Optional[np.ndarray] = None) -> Dict:\n",
    "    \"\"\"Fit Gaussian parameters using numeric optimization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray\n",
    "        Data samples\n",
    "    init_params : np.ndarray, optional\n",
    "        Initial [mu, sigma] guess (auto-initialized if None)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        MLE fit results including 'params' key with [mu, sigma]\n",
    "    \"\"\"\n",
    "    z = np.asarray(z)\n",
    "    if init_params is None:\n",
    "        init_params = np.array([np.mean(z), np.std(z) if np.std(z) > 1e-6 else 1.0])\n",
    "    bounds = [(None, None), (1e-9, None)]\n",
    "    return mle_fit(gaussian_nll, z, init_params=init_params, bounds=bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbff746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check\n",
    "rng = np.random.default_rng(0)\n",
    "z = rng.normal(loc=2.0, scale=1.5, size=500)\n",
    "fit_gaussian_mle_closed_form(z), fit_gaussian_mle_numeric(z)[\"params\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc50fd",
   "metadata": {},
   "source": [
    "### (Optional) Jensen's Inequality Viewpoint: MLE Minimizes Cross-Entropy / KL\n",
    "\n",
    "The notes use Jensen's inequality to show that the true parameter $\\alpha^*$ minimizes the expected negative log-likelihood.\n",
    "\n",
    "In information-theoretic terms:\n",
    "- Expected NLL is the **cross-entropy** $H(P, Q) = -\\mathbb{E}_P[\\log Q]$ between true distribution $P$ and model $Q$\n",
    "- The gap to the minimum is a **KL divergence** $D_{KL}(P||Q) = \\mathbb{E}_P[\\log P - \\log Q]$ (always non-negative)\n",
    "\n",
    "where:\n",
    "- $P$ = true data distribution\n",
    "- $Q$ = model distribution parameterized by $\\alpha$\n",
    "- $H(P,Q)$ = cross-entropy\n",
    "- $D_{KL}(P||Q)$ = Kullback-Leibler divergence\n",
    "\n",
    "Below are small helpers for **discrete** distributions (useful for sanity checks and understanding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_discrete(p: np.ndarray, q: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    \"\"\"Compute KL(p||q) for discrete probability distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "        True distribution (will be normalized)\n",
    "    q : np.ndarray\n",
    "        Model distribution (will be normalized)\n",
    "    eps : float, optional\n",
    "        Small constant for numerical stability (default: 1e-12)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        KL divergence from p to q\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float); q = np.asarray(q, dtype=float)\n",
    "    p = p / np.sum(p); q = q / np.sum(q)\n",
    "    p = np.clip(p, eps, 1.0); q = np.clip(q, eps, 1.0)\n",
    "    return float(np.sum(p * np.log(p / q)))\n",
    "\n",
    "def cross_entropy_discrete(p: np.ndarray, q: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    \"\"\"Compute cross-entropy H(p,q) = -E_p[log q] for discrete distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "        True distribution (will be normalized)\n",
    "    q : np.ndarray\n",
    "        Model distribution (will be normalized)\n",
    "    eps : float, optional\n",
    "        Small constant for numerical stability (default: 1e-12)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross-entropy from p to q\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float); q = np.asarray(q, dtype=float)\n",
    "    p = p / np.sum(p); q = q / np.sum(q)\n",
    "    p = np.clip(p, eps, 1.0); q = np.clip(q, eps, 1.0)\n",
    "    return float(-np.sum(p * np.log(q)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7ad09",
   "metadata": {},
   "source": [
    "## 5) MLE and Regression (Conditional Likelihood)\n",
    "\n",
    "The notes show: for a joint model $f_{X,Y}(x,y)=f_{Y\\mid X}(y\\mid x)f_X(x)$, if the marginal $f_X$ has **no parameters**, then maximizing the joint likelihood is equivalent to maximizing the **conditional likelihood** $f_{Y\\mid X}(y\\mid x)$.\n",
    "\n",
    "**Variable definitions:**\n",
    "- $f_{X,Y}(x,y)$ = joint density of features and response\n",
    "- $f_{Y\\mid X}(y\\mid x)$ = conditional density of response given features (depends on parameters)\n",
    "- $f_X(x)$ = marginal density of features (parameter-free)\n",
    "- $x$ = feature values\n",
    "- $y$ = response values\n",
    "\n",
    "This is the fundamental reason why:\n",
    "- **Linear regression** with Gaussian noise $Y\\mid X \\sim \\mathcal{N}(X\\beta, \\sigma^2)$ → least squares minimization\n",
    "- **Logistic regression** with Bernoulli model $Y\\mid X \\sim \\text{Bernoulli}(\\text{sigmoid}(X\\beta))$ → logistic loss / log-loss minimization\n",
    "\n",
    "Below are ready-to-use functions for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression_mle(X: np.ndarray,\n",
    "                            y: np.ndarray,\n",
    "                            add_bias: bool = True) -> Dict[str, Union[np.ndarray, float]]:\n",
    "    \"\"\"MLE for linear regression under Y|X ~ N(Xβ, σ^2).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix, shape (n_samples,) or (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target values, shape (n_samples,)\n",
    "    add_bias : bool, optional\n",
    "        Whether to add intercept term (default: True)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with keys:\n",
    "        - beta: coefficient vector (includes intercept if add_bias=True)\n",
    "        - sigma: MLE estimate of noise std dev (uses 1/n)\n",
    "        - y_hat: fitted values\n",
    "        - residuals: y - y_hat\n",
    "    \"\"\"\n",
    "    X = np.asarray(X); y = np.asarray(y)\n",
    "    X_design = add_intercept(X) if add_bias else (X.reshape(-1,1) if X.ndim==1 else X)\n",
    "    beta_hat, *_ = np.linalg.lstsq(X_design, y, rcond=None)\n",
    "    y_hat = X_design @ beta_hat\n",
    "    resid = y - y_hat\n",
    "    sigma_hat = float(np.sqrt(np.mean(resid**2)))\n",
    "    return {\"beta\": beta_hat, \"sigma\": sigma_hat, \"y_hat\": y_hat, \"residuals\": resid}\n",
    "\n",
    "def predict_linear_regression(beta: np.ndarray, X: np.ndarray, add_bias: bool = True) -> np.ndarray:\n",
    "    \"\"\"Predict using fitted linear regression coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : np.ndarray\n",
    "        Coefficient vector (includes intercept if add_bias=True)\n",
    "    X : np.ndarray\n",
    "        Feature matrix for prediction\n",
    "    add_bias : bool, optional\n",
    "        Whether beta includes intercept term (default: True)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predicted values\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    X_design = add_intercept(X) if add_bias else (X.reshape(-1,1) if X.ndim==1 else X)\n",
    "    return X_design @ np.asarray(beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21324ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (synthetic)\n",
    "rng = np.random.default_rng(0)\n",
    "X = rng.uniform(-2, 2, size=300)\n",
    "y = 1.0 + 2.5*X + rng.normal(0, 0.7, size=300)\n",
    "\n",
    "lin = fit_linear_regression_mle(X, y)\n",
    "lin[\"beta\"], lin[\"sigma\"], empirical_risk(squared_loss, y, lin[\"y_hat\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cdf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_nll(beta: np.ndarray, X: np.ndarray, y: np.ndarray, add_bias: bool = True) -> float:\n",
    "    \"\"\"Negative log-likelihood for Bernoulli(sigmoid(Xβ)).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : np.ndarray\n",
    "        Coefficient vector (includes intercept if add_bias=True)\n",
    "    X : np.ndarray\n",
    "        Feature matrix, shape (n_samples,) or (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Binary labels (0 or 1), shape (n_samples,)\n",
    "    add_bias : bool, optional\n",
    "        Whether beta includes intercept term (default: True)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Total negative log-likelihood\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Uses numerically stable form: sum log(1 + exp(-z_i * eta_i))\n",
    "    where z_i = 2y_i - 1 and eta_i = Xβ\n",
    "    \"\"\"\n",
    "    X = np.asarray(X); y = np.asarray(y).astype(int)\n",
    "    X_design = add_intercept(X) if add_bias else (X.reshape(-1,1) if X.ndim==1 else X)\n",
    "    eta = X_design @ np.asarray(beta)\n",
    "    z = 2*y - 1  # +1 for y=1, -1 for y=0\n",
    "    # log(1 + exp(-z*eta)) stable via logaddexp(0, -z*eta)\n",
    "    return float(np.sum(np.logaddexp(0.0, -z * eta)))\n",
    "\n",
    "def fit_logistic_regression_mle(X: np.ndarray,\n",
    "                                y: np.ndarray,\n",
    "                                add_bias: bool = True,\n",
    "                                init_beta: Optional[np.ndarray] = None,\n",
    "                                method: str = \"L-BFGS-B\",\n",
    "                                l2: float = 0.0) -> Dict:\n",
    "    \"\"\"Fit logistic regression by MLE (optionally with L2 regularization).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix, shape (n_samples,) or (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Binary labels (0 or 1), shape (n_samples,)\n",
    "    add_bias : bool, optional\n",
    "        Whether to include intercept term (default: True)\n",
    "    init_beta : np.ndarray, optional\n",
    "        Initial coefficient values (default: zeros)\n",
    "    method : str, optional\n",
    "        Scipy optimization method (default: \"L-BFGS-B\")\n",
    "    l2 : float, optional\n",
    "        L2 regularization strength (default: 0.0, no regularization)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with keys:\n",
    "        - beta: MLE coefficient estimates\n",
    "        - fun: final NLL value\n",
    "        - success: whether optimization succeeded\n",
    "        - message: optimization status\n",
    "        - result: full scipy result object\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Objective: NLL(beta) + (l2/2)*||beta||^2\n",
    "    If add_bias=True, intercept is NOT regularized.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X); y = np.asarray(y).astype(int)\n",
    "    X_design = add_intercept(X) if add_bias else (X.reshape(-1,1) if X.ndim==1 else X)\n",
    "    d = X_design.shape[1]\n",
    "    if init_beta is None:\n",
    "        init_beta = np.zeros(d)\n",
    "\n",
    "    def objective(beta):\n",
    "        nll = logistic_nll(beta, X, y, add_bias=add_bias)\n",
    "        if l2 > 0:\n",
    "            beta_reg = np.asarray(beta).copy()\n",
    "            if add_bias:\n",
    "                beta_reg[0] = 0.0  # don't penalize intercept\n",
    "            nll = nll + 0.5*l2*np.sum(beta_reg**2)\n",
    "        return float(nll)\n",
    "\n",
    "    res = minimize(objective, np.asarray(init_beta, dtype=float), method=method)\n",
    "    return {\"beta\": res.x, \"fun\": float(res.fun), \"success\": bool(res.success),\n",
    "            \"message\": res.message, \"result\": res}\n",
    "\n",
    "def predict_proba_logistic(beta: np.ndarray, X: np.ndarray, add_bias: bool = True) -> np.ndarray:\n",
    "    \"\"\"Predict probabilities P(Y=1|X) using logistic regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : np.ndarray\n",
    "        Coefficient vector (includes intercept if add_bias=True)\n",
    "    X : np.ndarray\n",
    "        Feature matrix for prediction\n",
    "    add_bias : bool, optional\n",
    "        Whether beta includes intercept term (default: True)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predicted probabilities P(Y=1|X)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    X_design = add_intercept(X) if add_bias else (X.reshape(-1,1) if X.ndim==1 else X)\n",
    "    return sigmoid(X_design @ np.asarray(beta))\n",
    "\n",
    "def predict_logistic(beta: np.ndarray, X: np.ndarray, threshold: float = 0.5, add_bias: bool = True) -> np.ndarray:\n",
    "    \"\"\"Predict class labels using logistic regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : np.ndarray\n",
    "        Coefficient vector (includes intercept if add_bias=True)\n",
    "    X : np.ndarray\n",
    "        Feature matrix for prediction\n",
    "    threshold : float, optional\n",
    "        Classification threshold (default: 0.5)\n",
    "    add_bias : bool, optional\n",
    "        Whether beta includes intercept term (default: True)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predicted class labels (0 or 1)\n",
    "    \"\"\"\n",
    "    return (predict_proba_logistic(beta, X, add_bias=add_bias) > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dadf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (synthetic binary 1D)\n",
    "rng = np.random.default_rng(1)\n",
    "X = rng.normal(0, 1, size=500)\n",
    "true_beta = np.array([-0.2, 2.0])  # intercept + slope\n",
    "p = sigmoid(add_intercept(X) @ true_beta)\n",
    "y = rng.binomial(1, p)\n",
    "\n",
    "fit = fit_logistic_regression_mle(X, y, l2=0.0)\n",
    "beta_hat = fit[\"beta\"]\n",
    "acc = 1.0 - misclassification_rate(y, predict_logistic(beta_hat, X))\n",
    "beta_hat, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672a68e",
   "metadata": {},
   "source": [
    "## 6) Quick Function Index (Copy/Paste Friendly)\n",
    "\n",
    "### Core Risk/ERM\n",
    "- `squared_loss(y_true, y_pred)` — Pointwise squared loss $(y-\\hat{y})^2$\n",
    "- `zero_one_loss(y_true, y_pred)` — Pointwise 0-1 loss (classification)\n",
    "- `empirical_risk(loss_fn, y_true, y_pred)` — Average loss on data\n",
    "- `monte_carlo_risk(loss_on_sample, sampler, n_mc=...)` — Estimate true risk via sampling\n",
    "- `erm_fit(model_predict, loss_fn, X, y, init_params, ...)` — General ERM optimizer\n",
    "\n",
    "### Regression Function $r(x)$\n",
    "- `kernel_regression_predict(X_train, y_train, X_query, bandwidth=..., kernel=...)` — Nadaraya-Watson estimator\n",
    "- `knn_regression_predict(X_train, y_train, X_query, k=...)` — k-NN regression\n",
    "- `mse_decomposition(y, gX, rX)` — Decompose MSE into noise, approximation, and cross terms\n",
    "\n",
    "### Classification / Bayes Rule\n",
    "- `bayes_rule_binary(p_y1_given_x, threshold=0.5)` — Optimal binary classifier\n",
    "- `knn_posterior_predict(X_train, y_train, X_query, k=...)` — Estimate $P(Y=1|X)$ via k-NN\n",
    "- `misclassification_rate(y_true, y_pred)` — 0-1 error rate\n",
    "- `bayes_error_from_posterior(p_y1_given_x)` — Bayes error given true posterior\n",
    "\n",
    "### MLE\n",
    "- `mle_fit(nll_fn, data, init_params, ...)` — Generic MLE by minimizing NLL\n",
    "- `fit_gaussian_mle_closed_form(z)` — Closed-form Gaussian MLE\n",
    "- `fit_gaussian_mle_numeric(z)` — Numeric Gaussian MLE\n",
    "- `fit_linear_regression_mle(X, y)` — Linear regression via MLE (Gaussian noise)\n",
    "- `fit_logistic_regression_mle(X, y, l2=...)` — Logistic regression via MLE (Bernoulli)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d552c18",
   "metadata": {},
   "source": [
    "## 7) How to Adapt This to Your Own Exercises Fast\n",
    "\n",
    "**Step-by-step guide:**\n",
    "\n",
    "1. **Replace data**: Use your own arrays `X, y` instead of synthetic generators\n",
    "2. **Choose loss function**:\n",
    "   - Regression → `squared_loss`\n",
    "   - Classification → `zero_one_loss` (for evaluation), log-loss/NLL (for training)\n",
    "3. **Select model class**:\n",
    "   - Linear model → `fit_linear_regression_mle`\n",
    "   - Logistic model → `fit_logistic_regression_mle`\n",
    "   - Custom parametric model → use `erm_fit(...)` with your `model_predict(...)` function\n",
    "4. **Estimate true risk**: If you need expected risk (not just empirical), use `monte_carlo_risk` with a sampler for your assumed distribution\n",
    "\n",
    "All functions accept standard NumPy arrays and return dictionaries with results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
