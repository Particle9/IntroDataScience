{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c8fdbd",
   "metadata": {},
   "source": [
    "# Lecture Notes Toolkit - Fundamentals of Estimation (Ch. 5) + Random Variable Generation (Ch. 6)\n",
    "\n",
    "This notebook is a **reusable toolkit** for the material in the provided notes (pages 88-101).\n",
    "It mixes:\n",
    "- short explanations (what a definition/theorem means in practice), and  \n",
    "- **Python functions you can directly reuse** by swapping `X`, `y`, `n`, `alpha`, `a,b,M`, etc.\n",
    "\n",
    "> Tip: Run the notebook top-to-bottom once. Then treat the function sections as your \"library.\" ✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85184fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Tuple, Optional, Iterable, List\n",
    "\n",
    "# Optional plotting (safe to ignore if you don't need plots)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109c6a2",
   "metadata": {},
   "source": [
    "## Chapter 5 - Fundamentals of Estimation\n",
    "\n",
    "### 5.1-5.2 Point estimation (the practical view)\n",
    "\n",
    "- **Data**: an i.i.d. sample `X = (X1, …, Xn)` from some unknown distribution (or a parametric family).\n",
    "- **Statistic / estimator**: any function of the data. In code: a Python function `theta_hat = estimator(x)`.\n",
    "- **Bias**: `E[theta_hat] - theta*` (usually unknown in real life).\n",
    "- **Standard error (SE)**: `sqrt(Var(theta_hat))`.\n",
    "- **MSE**: `E[(theta_hat - theta*)^2]` and the key identity:\n",
    "\n",
    "\\[ \\mathrm{MSE} = \\mathrm{Var}(\\hat\\theta) + (\\mathrm{Bias})^2 \\]\n",
    "\n",
    "In practice, bias/SE/MSE are commonly assessed by **simulation** (if you know the data-generating model) or by **bootstrap** (if you only have data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Generic estimation utilities\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class EstimatorDiagnostics:\n",
    "    \"\"\"Holds Monte Carlo diagnostics for an estimator.\"\"\"\n",
    "    theta_star: float\n",
    "    mean_hat: float\n",
    "    bias: float\n",
    "    se: float\n",
    "    mse: float\n",
    "\n",
    "def simulate_sampling_distribution(\n",
    "    sampler: Callable[[int], np.ndarray],\n",
    "    estimator: Callable[[np.ndarray], float],\n",
    "    n: int,\n",
    "    theta_star: float,\n",
    "    n_sims: int = 10_000,\n",
    "    seed: Optional[int] = 0,\n",
    ") -> Tuple[np.ndarray, EstimatorDiagnostics]:\n",
    "    \"\"\"\n",
    "    Monte Carlo approximation of the sampling distribution of an estimator.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sampler : function(n) -> sample array of length n\n",
    "    estimator : function(sample) -> float\n",
    "    n : sample size per simulation\n",
    "    theta_star : true parameter value for the simulation model\n",
    "    n_sims : number of Monte Carlo replications\n",
    "    seed : RNG seed (None for random)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta_hats : array of estimated values across simulations\n",
    "    diagnostics : EstimatorDiagnostics (mean_hat, bias, se, mse)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    theta_hats = np.empty(n_sims, dtype=float)\n",
    "    for s in range(n_sims):\n",
    "        x = sampler(n) if seed is None else sampler_with_rng(sampler, n, rng)\n",
    "        theta_hats[s] = estimator(x)\n",
    "\n",
    "    mean_hat = float(np.mean(theta_hats))\n",
    "    bias = mean_hat - float(theta_star)\n",
    "    se = float(np.std(theta_hats, ddof=0))\n",
    "    mse = float(np.mean((theta_hats - theta_star) ** 2))\n",
    "    return theta_hats, EstimatorDiagnostics(theta_star, mean_hat, bias, se, mse)\n",
    "\n",
    "def sampler_with_rng(sampler: Callable[[int], np.ndarray], n: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Helper: allow samplers that internally call numpy global RNG.\n",
    "    If your sampler already uses `rng`, you can ignore this and pass seed=None above.\n",
    "    \"\"\"\n",
    "    return sampler(n)\n",
    "\n",
    "def bootstrap_se(\n",
    "    x: np.ndarray,\n",
    "    estimator: Callable[[np.ndarray], float],\n",
    "    n_boot: int = 2000,\n",
    "    seed: Optional[int] = 0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Bootstrap estimate of standard error of an estimator.\n",
    "    Works when the true distribution/parameter is unknown.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(x)\n",
    "    hats = np.empty(n_boot, dtype=float)\n",
    "    for b in range(n_boot):\n",
    "        xb = rng.choice(x, size=n, replace=True)\n",
    "        hats[b] = estimator(xb)\n",
    "    return float(np.std(hats, ddof=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb18e1",
   "metadata": {},
   "source": [
    "### Example 5.7 + 5.11 + 5.16 - Bernoulli mean estimator\n",
    "\n",
    "If `Xi ~ Bernoulli(theta*)`, the sample mean \\(\\bar X\\) is:\n",
    "- unbiased, and\n",
    "- \\(\\mathrm{SE}(\\bar X) = \\sqrt{\\theta^*(1-\\theta^*)/n}\\) → 0, so it's consistent.\n",
    "\n",
    "Below: compare theory vs simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0182702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_sampler(theta: float, seed: Optional[int] = None) -> Callable[[int], np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    def _s(n: int) -> np.ndarray:\n",
    "        return rng.binomial(1, theta, size=n).astype(float)\n",
    "    return _s\n",
    "\n",
    "def sample_mean(x: np.ndarray) -> float:\n",
    "    return float(np.mean(x))\n",
    "\n",
    "theta_star = 0.3\n",
    "n = 50\n",
    "sampler = bernoulli_sampler(theta_star, seed=123)\n",
    "theta_hats, diag = simulate_sampling_distribution(\n",
    "    sampler=lambda m: sampler(m),\n",
    "    estimator=sample_mean,\n",
    "    n=n,\n",
    "    theta_star=theta_star,\n",
    "    n_sims=5000,\n",
    "    seed=None,  # sampler has its own rng\n",
    ")\n",
    "\n",
    "theory_se = math.sqrt(theta_star * (1 - theta_star) / n)\n",
    "diag, theory_se\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of the sampling distribution\n",
    "plt.figure()\n",
    "plt.hist(theta_hats, bins=40)\n",
    "plt.title(\"Sampling distribution of sample mean (Bernoulli)\")\n",
    "plt.xlabel(\"theta_hat\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858813a",
   "metadata": {},
   "source": [
    "### 5.14-5.15 - MSE = SE² + Bias², and a practical consistency check\n",
    "\n",
    "If you can argue (or estimate) that **bias → 0** and **SE → 0** as `n` grows, then the estimator is consistent.\n",
    "\n",
    "Below: a quick numeric \"consistency curve\" for the Bernoulli mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theory_se_bernoulli_mean(theta: float, n: int) -> float:\n",
    "    return math.sqrt(theta * (1 - theta) / n)\n",
    "\n",
    "ns = [10, 20, 50, 100, 200, 500, 1000]\n",
    "theory = [theory_se_bernoulli_mean(theta_star, k) for k in ns]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ns, theory, marker='o')\n",
    "plt.title(\"SE of Bernoulli sample mean shrinks as n grows\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"theoretical SE\")\n",
    "plt.xscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d2fd4e",
   "metadata": {},
   "source": [
    "### 5.17-5.18 - Sub-Gaussian tails ⇒ MSE bound (a usable template)\n",
    "\n",
    "The notes show: if a centered RV `Y` has a sub-Gaussian tail bound\n",
    "\\(P(|Y| \\ge \\epsilon) \\le 2e^{-c_0\\epsilon^2}\\),\n",
    "then you can bound the second moment `E[Y^2]` by a constant times `1/c0`.\n",
    "\n",
    "A common way this appears in ML:\n",
    "- If `X1,…,Xn` are i.i.d. sub-Gaussian with parameter `σ`, then the sample mean has\n",
    "  \\(\\mathrm{MSE}(\\bar X) \\lesssim \\sigma^2/n\\).\n",
    "\n",
    "Below is a **practical helper**: estimate MSE of sample mean from simulations, compare to a `sigma^2/n` rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_of_sample_mean_gaussian(sigma: float, n: int, n_sims: int = 5000, seed: int = 0) -> float:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    means = rng.normal(0, sigma, size=(n_sims, n)).mean(axis=1)\n",
    "    return float(np.mean(means**2))  # theta* = 0\n",
    "\n",
    "sigma = 2.0\n",
    "for n in [10, 20, 50, 100, 200, 500]:\n",
    "    mse = mse_of_sample_mean_gaussian(sigma, n, n_sims=20000, seed=1)\n",
    "    print(f\"n={n:4d}  empirical MSE={mse:.5f}  sigma^2/n={sigma**2/n:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f1245",
   "metadata": {},
   "source": [
    "## 5.3 Non-parametric DF estimation\n",
    "\n",
    "### Empirical distribution function (EDF / ECDF)\n",
    "\n",
    "Given i.i.d. data `X1,…,Xn`, the empirical CDF is:\n",
    "\\[ \\hat F_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\} \\]\n",
    "\n",
    "Key properties from the notes:\n",
    "- **Unbiased**: `E[Fn(x)] = F(x)` for each fixed `x`\n",
    "- `Var(Fn(x)) = F(x)(1-F(x))/n`\n",
    "- **Hoeffding**: for fixed `x`, `P(|Fn(x)-F(x)|>ε) ≤ 2 exp(-2nε²)`\n",
    "- **DKW inequality** (uniform over x):  \n",
    "  `P( sup_x |Fn(x)-F(x)| > ε ) ≤ 2 exp(-2nε²)`\n",
    "\n",
    "The DKW inequality gives an **easy confidence band**:\n",
    "Choose `ε = sqrt(log(2/α) / (2n))` then  \n",
    "`P( sup_x |Fn(x)-F(x)| ≤ ε ) ≥ 1-α`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# ECDF + DKW confidence bands\n",
    "# -----------------------------\n",
    "\n",
    "def ecdf(x_sample: np.ndarray) -> Callable[[np.ndarray], np.ndarray]:\n",
    "    \"\"\"Returns a callable Fhat(x) for the empirical CDF built from x_sample.\"\"\"\n",
    "    xs = np.sort(np.asarray(x_sample))\n",
    "    n = xs.size\n",
    "\n",
    "    def Fhat(x: np.ndarray) -> np.ndarray:\n",
    "        x = np.asarray(x)\n",
    "        return np.searchsorted(xs, x, side='right') / n\n",
    "\n",
    "    return Fhat\n",
    "\n",
    "def dkw_epsilon(n: int, alpha: float = 0.05) -> float:\n",
    "    \"\"\"DKW half-width for a (1-alpha) uniform confidence band.\"\"\"\n",
    "    if not (0 < alpha < 1):\n",
    "        raise ValueError(\"alpha must be in (0,1).\")\n",
    "    return math.sqrt(math.log(2/alpha) / (2*n))\n",
    "\n",
    "def ecdf_confidence_band(\n",
    "    x_sample: np.ndarray,\n",
    "    grid: np.ndarray,\n",
    "    alpha: float = 0.05,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns (Fn(grid), lower(grid), upper(grid)) where lower/upper use DKW band.\"\"\"\n",
    "    Fhat = ecdf(x_sample)\n",
    "    Fn = Fhat(grid)\n",
    "    eps = dkw_epsilon(len(x_sample), alpha=alpha)\n",
    "    lower = np.clip(Fn - eps, 0.0, 1.0)\n",
    "    upper = np.clip(Fn + eps, 0.0, 1.0)\n",
    "    return Fn, lower, upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: ECDF + DKW band on a known distribution (Normal)\n",
    "rng = np.random.default_rng(0)\n",
    "n = 200\n",
    "x = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "\n",
    "grid = np.linspace(-3, 3, 400)\n",
    "Fn, lo, hi = ecdf_confidence_band(x, grid, alpha=0.05)\n",
    "\n",
    "# True CDF for comparison (Normal) using erf approximation\n",
    "def normal_cdf(z: np.ndarray, mu: float = 0.0, sigma: float = 1.0) -> np.ndarray:\n",
    "    z = (np.asarray(z) - mu) / (sigma * math.sqrt(2))\n",
    "    return 0.5 * (1 + np.vectorize(math.erf)(z))\n",
    "\n",
    "Ftrue = normal_cdf(grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(grid, Fn, label=\"ECDF Fn\")\n",
    "plt.plot(grid, Ftrue, label=\"True F\")\n",
    "plt.plot(grid, lo, linestyle=\"--\", label=\"DKW lower\")\n",
    "plt.plot(grid, hi, linestyle=\"--\", label=\"DKW upper\")\n",
    "plt.title(\"ECDF with DKW (uniform) confidence band\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5787f63",
   "metadata": {},
   "source": [
    "### Extra: Supremum deviation (Kolmogorov-Smirnov style statistic)\n",
    "\n",
    "The DKW inequality controls `sup_x |Fn(x)-F(x)|`. The empirical version of that sup distance is what the KS test uses.\n",
    "Below is a small helper you can use whenever you have a known target CDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sup_cdf_distance(sample: np.ndarray, cdf: Callable[[np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Computes sup_x |Fn(x) - F(x)| using the sample support.\n",
    "    (Exact supremum over R occurs at sample points for ECDF.)\n",
    "    \"\"\"\n",
    "    xs = np.sort(np.asarray(sample))\n",
    "    n = xs.size\n",
    "    F = cdf(xs)\n",
    "\n",
    "    Fn_right = np.arange(1, n+1) / n\n",
    "    Fn_left = np.arange(0, n) / n\n",
    "\n",
    "    d1 = np.max(np.abs(Fn_right - F))\n",
    "    d2 = np.max(np.abs(Fn_left - F))\n",
    "    return float(max(d1, d2))\n",
    "\n",
    "d = sup_cdf_distance(x, lambda t: normal_cdf(t, 0.0, 1.0))\n",
    "d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1734be3",
   "metadata": {},
   "source": [
    "### Relative entropy (KL) risk viewpoint (practical helper)\n",
    "\n",
    "The notes define a loss leading to KL divergence (up to constants). A common computational form is KL between two discrete distributions on a grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_discrete(p: np.ndarray, q: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    \"\"\"\n",
    "    KL(p||q) for discrete distributions on the same grid.\n",
    "    `p` and `q` should be nonnegative and sum to 1.\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    p = p / np.sum(p)\n",
    "    q = q / np.sum(q)\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    q = np.clip(q, eps, 1.0)\n",
    "    return float(np.sum(p * np.log(p / q)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ff5e3",
   "metadata": {},
   "source": [
    "## Chapter 6 - Random Variable Generation\n",
    "\n",
    "Computers are deterministic, so we generate **pseudo-random** sequences.\n",
    "The notes start with **discrete uniform pseudo-randomness** over  \n",
    "`M = {0,1,…,M-1}` and then study **congruential (linear) generators**:\n",
    "\n",
    "\\[ u_{i} = (a u_{i-1} + b) \\bmod M \\]\n",
    "\n",
    "Key concepts:\n",
    "- **Period**: how long before the sequence repeats.\n",
    "- If the period is `M` (full period), then the sequence visits every state once per cycle ⇒ uniform frequencies.\n",
    "- **Hull-Dobell theorem** gives conditions for full period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77626d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Congruential generator (LCG)\n",
    "# -----------------------------\n",
    "\n",
    "def lcg(a: int, b: int, M: int, seed: int):\n",
    "    \"\"\"\n",
    "    Linear congruential generator (LCG):\n",
    "        u_{t+1} = (a*u_t + b) mod M\n",
    "    Yields an infinite sequence of integers in {0,...,M-1}.\n",
    "    \"\"\"\n",
    "    if M <= 0:\n",
    "        raise ValueError(\"M must be positive.\")\n",
    "    u = seed % M\n",
    "    while True:\n",
    "        yield u\n",
    "        u = (a * u + b) % M\n",
    "\n",
    "def lcg_sequence(a: int, b: int, M: int, seed: int, n: int) -> np.ndarray:\n",
    "    \"\"\"Get first n values from LCG as a numpy array.\"\"\"\n",
    "    gen = lcg(a, b, M, seed)\n",
    "    return np.fromiter((next(gen) for _ in range(n)), dtype=int, count=n)\n",
    "\n",
    "def estimate_period(a: int, b: int, M: int, seed: int, max_steps: int = 5_000_000) -> int:\n",
    "    \"\"\"\n",
    "    Detect period by finding first repeat of the seed state.\n",
    "    Educational/testing for smaller M.\n",
    "    \"\"\"\n",
    "    gen = lcg(a, b, M, seed)\n",
    "    first = next(gen)\n",
    "    for t in range(1, max_steps + 1):\n",
    "        if next(gen) == first:\n",
    "            return t\n",
    "    raise RuntimeError(\"Period not found within max_steps; increase max_steps or use smaller M.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a358b",
   "metadata": {},
   "source": [
    "### 6.1 - Checking \"uniformly pseudorandom\" on a finite set\n",
    "\n",
    "Definition (informal): frequencies of each value should approach `1/M`.\n",
    "Below: frequency checker + a small demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_table(seq: np.ndarray, M: int) -> np.ndarray:\n",
    "    \"\"\"Returns normalized frequencies for values 0..M-1 from seq.\"\"\"\n",
    "    counts = np.bincount(seq.astype(int), minlength=M)\n",
    "    return counts / counts.sum()\n",
    "\n",
    "def uniformity_score(freqs: np.ndarray) -> float:\n",
    "    \"\"\"One simple score: max absolute deviation from uniform frequency.\"\"\"\n",
    "    M = len(freqs)\n",
    "    return float(np.max(np.abs(freqs - 1.0 / M)))\n",
    "\n",
    "# Demo: (3,0,16) example from notes\n",
    "a, b, M, seed = 3, 0, 16, 1\n",
    "seq = lcg_sequence(a, b, M, seed, n=200)\n",
    "freqs = frequency_table(seq, M)\n",
    "period = estimate_period(a, b, M, seed)\n",
    "period, uniformity_score(freqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14263ae",
   "metadata": {},
   "source": [
    "### Lemma 6.9 - Restricting a generator to a smaller range\n",
    "\n",
    "Map `ui` in `{0,…,M-1}` to `vi` in `{0,…,K-1}` via:\n",
    "\\[ v_i = \\lfloor (u_i / M) \\cdot K \\rfloor \\]\n",
    "\n",
    "Reusable helper below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_range(u: np.ndarray, M: int, K: int) -> np.ndarray:\n",
    "    \"\"\"Map integer sequence u in [0, M-1] to integers in [0, K-1].\"\"\"\n",
    "    u = np.asarray(u, dtype=float)\n",
    "    return np.floor((u / M) * K).astype(int)\n",
    "\n",
    "K = 8\n",
    "v = map_to_range(seq, M=M, K=K)\n",
    "freqs_v = frequency_table(v, K)\n",
    "uniformity_score(freqs_v), freqs_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8f83e",
   "metadata": {},
   "source": [
    "### Hull-Dobell theorem (full period conditions)\n",
    "\n",
    "LCG `(a,b,M)` has full period `M` iff:\n",
    "1) `gcd(b, M) = 1`  \n",
    "2) For every prime `p` dividing `M`, `p | (a-1)`  \n",
    "3) If `4 | M` then `4 | (a-1)`\n",
    "\n",
    "Reusable checker below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_factors(n: int) -> List[int]:\n",
    "    \"\"\"Return unique prime factors of n.\"\"\"\n",
    "    n = abs(int(n))\n",
    "    factors = []\n",
    "    if n < 2:\n",
    "        return factors\n",
    "    if n % 2 == 0:\n",
    "        factors.append(2)\n",
    "        while n % 2 == 0:\n",
    "            n //= 2\n",
    "    p = 3\n",
    "    while p * p <= n:\n",
    "        if n % p == 0:\n",
    "            factors.append(p)\n",
    "            while n % p == 0:\n",
    "                n //= p\n",
    "        p += 2\n",
    "    if n > 1:\n",
    "        factors.append(n)\n",
    "    return factors\n",
    "\n",
    "def hull_dobell_full_period(a: int, b: int, M: int) -> Dict[str, object]:\n",
    "    \"\"\"Checks Hull-Dobell conditions and returns which ones pass.\"\"\"\n",
    "    if M <= 0:\n",
    "        raise ValueError(\"M must be positive.\")\n",
    "    cond1 = math.gcd(b, M) == 1\n",
    "    primes = prime_factors(M)\n",
    "    cond2 = all(((a - 1) % p == 0) for p in primes)\n",
    "    cond3 = True if (M % 4 != 0) else ((a - 1) % 4 == 0)\n",
    "    return {\n",
    "        \"gcd(b,M)=1\": cond1,\n",
    "        \"p | (a-1) for all primes p|M\": cond2,\n",
    "        \"if 4|M then 4|(a-1)\": cond3,\n",
    "        \"ALL (full period expected)\": (cond1 and cond2 and cond3),\n",
    "        \"prime_factors(M)\": primes,\n",
    "    }\n",
    "\n",
    "hull_dobell_full_period(a=3, b=0, M=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bde8db",
   "metadata": {},
   "source": [
    "### Lemma 6.11 - Long-run mean and variance for a uniform sequence on M\n",
    "\n",
    "If `u_i` is uniformly pseudorandom on `{0,…,M-1}`, then (empirically):\n",
    "\n",
    "- mean → `(M-1)/2`\n",
    "- variance → `(M^2-1)/12`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a151df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_discrete_uniform_moments(M: int) -> Tuple[float, float]:\n",
    "    mean = (M - 1) / 2.0\n",
    "    var = (M**2 - 1) / 12.0\n",
    "    return mean, var\n",
    "\n",
    "def empirical_moments(u: np.ndarray) -> Tuple[float, float]:\n",
    "    u = np.asarray(u, dtype=float)\n",
    "    return float(np.mean(u)), float(np.var(u, ddof=0))\n",
    "\n",
    "# Demo on a full-period LCG example (common choice when M=2^k)\n",
    "a2, b2, M2, seed2 = 5, 1, 2**10, 7\n",
    "hull_dobell_full_period(a2, b2, M2)[\"ALL (full period expected)\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "u2 = lcg_sequence(a2, b2, M2, seed2, n=100000)\n",
    "emp = empirical_moments(u2)\n",
    "theo = theoretical_discrete_uniform_moments(M2)\n",
    "emp, theo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(u2 / M2, bins=50)\n",
    "plt.title(\"LCG values scaled to [0,1] (histogram)\")\n",
    "plt.xlabel(\"u/M\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d49df",
   "metadata": {},
   "source": [
    "## Quick \"function index\" (copy/paste friendly)\n",
    "\n",
    "### Estimation (Ch. 5)\n",
    "- `simulate_sampling_distribution(sampler, estimator, n, theta_star, n_sims, seed)`\n",
    "- `bootstrap_se(x, estimator, n_boot, seed)`\n",
    "- `ecdf(x_sample)`\n",
    "- `dkw_epsilon(n, alpha)`\n",
    "- `ecdf_confidence_band(x_sample, grid, alpha)`\n",
    "- `sup_cdf_distance(sample, cdf)`\n",
    "- `kl_divergence_discrete(p, q)`\n",
    "\n",
    "### Random generation (Ch. 6)\n",
    "- `lcg(a,b,M,seed)` / `lcg_sequence(a,b,M,seed,n)`\n",
    "- `estimate_period(a,b,M,seed)` (educational for small M)\n",
    "- `frequency_table(seq,M)` / `uniformity_score(freqs)`\n",
    "- `map_to_range(u, M, K)`\n",
    "- `prime_factors(M)` / `hull_dobell_full_period(a,b,M)`\n",
    "- `theoretical_discrete_uniform_moments(M)` / `empirical_moments(u)`\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
