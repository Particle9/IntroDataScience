{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf2a314",
   "metadata": {},
   "source": [
    "# 1) Risk & Supervised Learning\n",
    "\n",
    "This notebook covers fundamental concepts in supervised learning:\n",
    "- Loss functions and risk metrics\n",
    "- Train/test/validation splits\n",
    "- Grid search for hyperparameter tuning\n",
    "- Cross-validation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df325e3",
   "metadata": {},
   "source": [
    "### Loss & Risk Metrics\n",
    "\n",
    "**What is this?**\n",
    "Loss functions measure how wrong a prediction is compared to the true value. Risk is the average (expected) loss across all predictions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Mean Squared Error (MSE)**: Used for regression. Penalizes large errors heavily. \n",
    "  - Formula: $(y_{true} - y_{pred})^2$\n",
    "  - Where: $y_{true}$ = actual/true value, $y_{pred}$ = predicted value\n",
    "  \n",
    "- **Mean Absolute Error (MAE)**: Used for regression. Less sensitive to outliers than MSE. \n",
    "  - Formula: $|y_{true} - y_{pred}|$\n",
    "  - Where: $y_{true}$ = actual/true value, $y_{pred}$ = predicted value\n",
    "  \n",
    "- **Zero-One Loss**: Used for classification. Returns 1 if prediction is wrong, 0 if correct.\n",
    "  - Formula: $\\mathbb{1}(y_{true} \\neq y_{pred})$ where $\\mathbb{1}$ is the indicator function\n",
    "  - Where: $y_{true}$ = actual label, $y_{pred}$ = predicted label\n",
    "  \n",
    "- **Log Loss (Binary Cross-Entropy)**: Used for probabilistic binary classification. Penalizes confident wrong predictions heavily. \n",
    "  - Formula: $-(y \\log(p) + (1-y)\\log(1-p))$\n",
    "  - Where: $y$ = true binary label (0 or 1), $p$ = predicted probability (between 0 and 1)\n",
    "  \n",
    "- **Average Risk**: The mean of all losses - tells you overall model performance.\n",
    "  - Formula: $R = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, \\hat{y}_i)$\n",
    "  - Where: $n$ = number of samples, $L$ = loss function, $y_i$ = true value for sample $i$, $\\hat{y}_i$ = predicted value for sample $i$\n",
    "\n",
    "**When to use:**\n",
    "- Use MSE when you want to penalize large errors more\n",
    "- Use MAE when outliers shouldn't dominate your loss\n",
    "- Use Zero-One for simple classification accuracy\n",
    "- Use Log Loss when you have probability predictions and want to encourage confident correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86e97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mean Squared Error\n",
    "def loss_squared(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) loss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True/actual values (ground truth)\n",
    "    y_pred : array-like\n",
    "        Predicted values from model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mse : ndarray\n",
    "        Array of squared errors (one per sample)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "# Mean Absolute Error\n",
    "def loss_absolute(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Error (MAE) loss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True/actual values (ground truth)\n",
    "    y_pred : array-like\n",
    "        Predicted values from model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mae : ndarray\n",
    "        Array of absolute errors (one per sample)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    return np.abs(y_true - y_pred)\n",
    "\n",
    "# Zero-One Loss\n",
    "def loss_zero_one(y_true, y_pred_label):\n",
    "    \"\"\"\n",
    "    Calculate Zero-One Loss for classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like of int\n",
    "        True/actual class labels (ground truth)\n",
    "    y_pred_label : array-like of int\n",
    "        Predicted class labels from model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : ndarray of float\n",
    "        Array where 1.0 = incorrect prediction, 0.0 = correct prediction (one per sample)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    y_pred_label = np.asarray(y_pred_label, int)\n",
    "    return (y_true != y_pred_label).astype(float)\n",
    "\n",
    "# Log Loss for Binary Classification\n",
    "def loss_logloss_binary(y_true, y_pred_prob, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Calculate Log Loss (Binary Cross-Entropy) for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like of float\n",
    "        True binary labels (0 or 1)\n",
    "    y_pred_prob : array-like of float\n",
    "        Predicted probabilities (between 0 and 1) for positive class\n",
    "    eps : float, default=1e-12\n",
    "        Small constant to avoid log(0) by clipping probabilities to [eps, 1-eps]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    log_loss : ndarray\n",
    "        Array of log loss values (one per sample)\n",
    "    \"\"\"\n",
    "    # y_true in {0,1}, y_pred_prob in [0,1]\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    p = np.asarray(y_pred_prob, float)\n",
    "    p = np.clip(p, eps, 1 - eps)  # Clip to avoid log(0)\n",
    "    return -(y_true*np.log(p) + (1-y_true)*np.log(1-p))\n",
    "\n",
    "# Average Risk (Expected Loss)\n",
    "def average_risk(y_true, y_pred, loss_fn):\n",
    "    \"\"\"\n",
    "    Calculate average risk (expected loss) across all samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True/actual values or labels (ground truth)\n",
    "    y_pred : array-like\n",
    "        Predicted values or labels from model\n",
    "    loss_fn : callable\n",
    "        Loss function that takes (y_true, y_pred) and returns array of losses\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    avg_risk : float\n",
    "        Mean loss across all samples (scalar value representing overall model performance)\n",
    "    \"\"\"\n",
    "    # returns the average loss\n",
    "    losses = loss_fn(y_true, y_pred)\n",
    "    return float(np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b7f3d",
   "metadata": {},
   "source": [
    "### Train Test Validation Split\n",
    "\n",
    "**What is this?**\n",
    "Splitting your data into separate sets to train your model and evaluate its performance fairly.\n",
    "\n",
    "**Why we need it:**\n",
    "- **Training set**: Used to train/fit the model\n",
    "- **Validation set**: Used during model development to tune hyperparameters and make decisions\n",
    "- **Test set**: Used only once at the end to get an unbiased estimate of model performance\n",
    "\n",
    "**Key points:**\n",
    "- Default split is 70% train, 15% validation, 15% test\n",
    "- Always shuffle data before splitting (unless time-series)\n",
    "- Use `random_state` for reproducibility\n",
    "- The validation set helps prevent overfitting during model selection\n",
    "- Never touch the test set until final evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7741fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_test_split(X, y=None, train_frac=0.7, val_frac=0.15, test_frac=0.15, shuffle=True, random_state=None):\n",
    "    \"\"\"\n",
    "    Split X (and optional y) into train/validation/test sets using scikit-learn.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Feature matrix to split\n",
    "    y : array-like of shape (n_samples,), default=None\n",
    "        Target vector to split (optional). If provided, stratified split maintains label distribution\n",
    "    train_frac : float, default=0.7\n",
    "        Fraction of data for training set (must be between 0 and 1)\n",
    "    val_frac : float, default=0.15\n",
    "        Fraction of data for validation set (must be between 0 and 1)\n",
    "    test_frac : float, default=0.15\n",
    "        Fraction of data for test set (must be between 0 and 1)\n",
    "        Note: train_frac + val_frac + test_frac must equal 1.0\n",
    "    shuffle : bool, default=True\n",
    "        Whether to shuffle data before splitting (recommended unless time-series data)\n",
    "    random_state : int or None, default=None\n",
    "        Random seed for reproducibility. Use fixed int for consistent splits across runs\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    If y is None:\n",
    "        X_train : array-like\n",
    "            Training features (train_frac of original data)\n",
    "        X_val : array-like\n",
    "            Validation features (val_frac of original data)\n",
    "        X_test : array-like\n",
    "            Test features (test_frac of original data)\n",
    "    \n",
    "    If y is provided:\n",
    "        X_train : array-like\n",
    "            Training features\n",
    "        X_val : array-like\n",
    "            Validation features\n",
    "        X_test : array-like\n",
    "            Test features\n",
    "        y_train : array-like\n",
    "            Training labels\n",
    "        y_val : array-like\n",
    "            Validation labels\n",
    "        y_test : array-like\n",
    "            Test labels\n",
    "    \"\"\"\n",
    "    fracs = float(train_frac) + float(val_frac) + float(test_frac)\n",
    "    if not np.isclose(fracs, 1.0):\n",
    "        raise ValueError(\"train_frac + val_frac + test_frac must sum to 1.0\")\n",
    "\n",
    "    # first split off the training set\n",
    "    test_plus_val = val_frac + test_frac\n",
    "    if y is None:\n",
    "        X_train, X_temp = train_test_split(\n",
    "            X, train_size=train_frac, test_size=test_plus_val,\n",
    "            random_state=random_state, shuffle=shuffle\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            X, y, train_size=train_frac, test_size=test_plus_val,\n",
    "            random_state=random_state, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "    # handle edge cases where val or test fraction is zero\n",
    "    if np.isclose(test_plus_val, 0.0):\n",
    "        # no val/test portion\n",
    "        X_val, X_test = X_temp[:0], X_temp[:0]\n",
    "        if y is not None:\n",
    "            y_val, y_test = y_temp[:0], y_temp[:0]\n",
    "    elif np.isclose(val_frac, 0.0):\n",
    "        X_val, X_test = X_temp[:0], X_temp\n",
    "        if y is not None:\n",
    "            y_val, y_test = y_temp[:0], y_temp\n",
    "    elif np.isclose(test_frac, 0.0):\n",
    "        X_val, X_test = X_temp, X_temp[:0]\n",
    "        if y is not None:\n",
    "            y_val, y_test = y_temp, y_temp[:0]\n",
    "    else:\n",
    "        # split the temp set into val and test according to their relative proportions\n",
    "        test_rel = test_frac / (val_frac + test_frac)\n",
    "        if y is None:\n",
    "            X_val, X_test = train_test_split(\n",
    "                X_temp, test_size=test_rel, random_state=random_state, shuffle=shuffle\n",
    "            )\n",
    "        else:\n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_temp, y_temp, test_size=test_rel, random_state=random_state, shuffle=shuffle\n",
    "            )\n",
    "\n",
    "    if y is None:\n",
    "        return X_train, X_val, X_test\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da61c3de",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "**What is this?**\n",
    "A systematic way to find the best hyperparameters for your model by trying all combinations of parameter values.\n",
    "\n",
    "**How it works:**\n",
    "1. Define a grid of hyperparameter values to try\n",
    "2. For each combination, train the model using k-fold cross-validation\n",
    "3. Evaluate performance using a scoring metric\n",
    "4. Return the best combination\n",
    "\n",
    "**Key parameters:**\n",
    "- `estimator`: The machine learning model to tune (must have fit/predict methods)\n",
    "- `param_grid`: Dictionary mapping parameter names (str) to lists of values to try\n",
    "  - Example: `{'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}` tries 6 combinations\n",
    "- `cv`: Number of cross-validation folds (default: 5)\n",
    "  - Higher values = more reliable but slower\n",
    "- `scoring`: Metric to optimize \n",
    "  - Classification: 'accuracy', 'f1', 'precision', 'recall', 'roc_auc'\n",
    "  - Regression: 'neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'\n",
    "- `n_jobs`: Number of parallel jobs (-1 = use all CPU cores)\n",
    "- `verbose`: Controls output verbosity (0=silent, 1=progress, 2=detailed)\n",
    "\n",
    "**Output:**\n",
    "- `best_estimator_`: The fitted model with best hyperparameters\n",
    "- `best_params_`: Dictionary of the best hyperparameter values found\n",
    "- `best_score_`: The best cross-validation score achieved\n",
    "- `cv_results_`: Full results for all parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search(model, param_grid, X_train, y_train, cv=5, scoring=None, verbose=1, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Perform grid search to find the best hyperparameters for a given model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator object\n",
    "        The machine learning model to tune (e.g., sklearn model or any estimator with fit/predict methods).\n",
    "        Must implement the scikit-learn estimator interface.\n",
    "    param_grid : dict or list of dicts\n",
    "        Dictionary with parameter names (str) as keys and lists of parameter settings to try as values.\n",
    "        Example: {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']} will try all 6 combinations.\n",
    "        Can also be a list of such dicts to search over different parameter spaces.\n",
    "    X_train : array-like of shape (n_samples, n_features)\n",
    "        Training feature matrix. Each row is a sample, each column is a feature.\n",
    "    y_train : array-like of shape (n_samples,)\n",
    "        Training target vector. Labels for classification or values for regression.\n",
    "    cv : int, cross-validation generator, or iterable, default=5\n",
    "        Number of cross-validation folds. If int, performs k-fold CV.\n",
    "        If CV object (e.g., StratifiedKFold), uses that splitter.\n",
    "    scoring : str, callable, or None, default=None\n",
    "        Strategy to evaluate model performance on test set.\n",
    "        - Classification: 'accuracy', 'f1', 'precision', 'recall', 'roc_auc'\n",
    "        - Regression: 'neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'\n",
    "        - If None, uses model's default score() method\n",
    "    verbose : int, default=1\n",
    "        Controls verbosity of output during grid search:\n",
    "        - 0: No output\n",
    "        - 1: Prints progress for each parameter combination\n",
    "        - 2+: More detailed output\n",
    "    n_jobs : int, default=-1\n",
    "        Number of jobs to run in parallel during cross-validation:\n",
    "        - -1: Use all available CPU cores\n",
    "        - 1: No parallelism (sequential)\n",
    "        - n: Use n cores\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : estimator object\n",
    "        The model fitted with the best hyperparameters found by grid search.\n",
    "        This is the estimator that gave the best CV score, refitted on full training data.\n",
    "    best_params : dict\n",
    "        Dictionary of the best hyperparameter values found.\n",
    "        Example: {'C': 1, 'kernel': 'rbf'}\n",
    "    best_score : float\n",
    "        The best mean cross-validation score achieved across all parameter combinations.\n",
    "        This is the average score across all CV folds for the best parameters.\n",
    "    grid_search_results : GridSearchCV object\n",
    "        The complete GridSearchCV object containing:\n",
    "        - cv_results_: Full results dictionary with scores for all combinations\n",
    "        - best_index_: Index of best parameter combination\n",
    "        - n_splits_: Number of CV splits used\n",
    "    \"\"\"\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        n_jobs=n_jobs,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    return grid.best_estimator_, grid.best_params_, grid.best_score_, grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366611a3",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "**What is this?**\n",
    "A resampling technique to evaluate model performance by splitting data into multiple train/test folds.\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "1. Split data into $k$ equal-sized folds: $D = D_1 \\cup D_2 \\cup ... \\cup D_k$\n",
    "2. For each fold $i = 1, ..., k$: \n",
    "   - Train on $(k-1)$ folds: $D \\setminus D_i$\n",
    "   - Test on fold $i$: $D_i$\n",
    "   - Compute score: $S_i$\n",
    "3. Average the $k$ test scores: $\\text{CV Score} = \\frac{1}{k}\\sum_{i=1}^{k} S_i$\n",
    "\n",
    "**Mathematical notation:**\n",
    "- $k$ = number of folds\n",
    "- $D$ = full dataset of size $n$\n",
    "- $D_i$ = fold $i$ (approximately $n/k$ samples)\n",
    "- $S_i$ = performance score on fold $i$\n",
    "- $\\bar{S}$ = mean score, $\\sigma_S$ = standard deviation of scores\n",
    "\n",
    "**Benefits:**\n",
    "- More reliable than single train/test split\n",
    "- Uses all data for both training and testing\n",
    "- Reduces variance in performance estimates\n",
    "\n",
    "**Common k values:**\n",
    "- $k=5$: Standard choice (good bias-variance trade-off)\n",
    "- $k=10$: More computation but less bias\n",
    "- $k=n$ (Leave-One-Out): Maximum data usage but high variance\n",
    "\n",
    "**Stratified CV:** Maintains class proportions $P(y=c)$ in each fold (important for imbalanced data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "\n",
    "def cross_validate_model(model, X, y, cv=5, scoring='accuracy', stratified=True):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on a model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator object\n",
    "        The machine learning model to evaluate. Must implement fit/predict methods.\n",
    "        Will be cloned and fitted k times (once per fold).\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Complete feature matrix. Will be split into k folds for training/testing.\n",
    "        Each row is a sample, each column is a feature.\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Complete target vector. Labels for classification or values for regression.\n",
    "        Will be split into k folds along with X.\n",
    "    cv : int, default=5\n",
    "        Number of folds (k) for cross-validation.\n",
    "        Common values: 5 (standard), 10 (more robust), 3 (faster)\n",
    "    scoring : str or callable, default='accuracy'\n",
    "        Scoring metric to evaluate model performance:\n",
    "        - Classification: 'accuracy', 'f1', 'precision', 'recall', 'roc_auc', 'f1_macro', 'f1_weighted'\n",
    "        - Regression: 'neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'\n",
    "        Note: scikit-learn uses negative values for error metrics (higher is better)\n",
    "    stratified : bool, default=True\n",
    "        Whether to use stratified folds (recommended for classification):\n",
    "        - True: Maintains class distribution in each fold (for classification tasks)\n",
    "        - False: Uses standard k-fold (may have imbalanced folds)\n",
    "        Only applicable when y contains integer class labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    scores : ndarray of shape (cv,)\n",
    "        Array of cross-validation scores, one per fold.\n",
    "        Example: [0.85, 0.87, 0.83, 0.89, 0.86] for cv=5\n",
    "    mean_score : float\n",
    "        Mean of CV scores across all folds: (1/k) * sum(scores).\n",
    "        This is the primary performance estimate.\n",
    "    std_score : float\n",
    "        Standard deviation of CV scores: measures stability/consistency of model.\n",
    "        Lower std = more stable performance across different data splits.\n",
    "    \"\"\"\n",
    "    # Choose appropriate cross-validation splitter\n",
    "    if stratified and hasattr(y, 'dtype') and y.dtype in [int, 'int64', 'int32']:\n",
    "        # Stratified: maintains class proportions in each fold\n",
    "        cv_splitter = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        # Standard k-fold: simple random splits\n",
    "        cv_splitter = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation: trains and evaluates model k times\n",
    "    scores = cross_val_score(model, X, y, cv=cv_splitter, scoring=scoring)\n",
    "    \n",
    "    return scores, float(np.mean(scores)), float(np.std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
