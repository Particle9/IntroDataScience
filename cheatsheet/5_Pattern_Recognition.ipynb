{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46962877",
   "metadata": {},
   "source": [
    "# 5) Pattern Recognition & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a056ac5",
   "metadata": {},
   "source": [
    "### 5.1 Confusion Matrix and Metrics\n",
    "\n",
    "**What is this?**\n",
    "A table showing the performance of a binary classifier by comparing predictions to true labels.\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "                Predicted\n",
    "              0 (Neg)  1 (Pos)\n",
    "Actual  0     TN       FP\n",
    "        1     FN       TP\n",
    "```\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Precision** = $\\frac{TP}{TP+FP}$ = Of all positive predictions, how many were correct?\n",
    "  - High precision = Few false alarms\n",
    "- **Recall (Sensitivity)** = $\\frac{TP}{TP+FN}$ = Of all actual positives, how many did we catch?\n",
    "  - High recall = Few missed positives\n",
    "- **Accuracy** = $\\frac{TP+TN}{TP+TN+FP+FN}$ = Overall correctness\n",
    "\n",
    "![title](img/confusion_matrix.png)\n",
    "\n",
    "**Trade-offs:**\n",
    "- Precision \u2191 Recall \u2193 (and vice versa)\n",
    "- Spam filter: High precision = Less good email marked as spam\n",
    "- Disease screening: High recall = Catch more sick patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f542aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_counts(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    y_pred = np.asarray(y_pred, int)\n",
    "    TP = int(np.sum((y_true==1) & (y_pred==1)))\n",
    "    TN = int(np.sum((y_true==0) & (y_pred==0)))\n",
    "    FP = int(np.sum((y_true==0) & (y_pred==1)))\n",
    "    FN = int(np.sum((y_true==1) & (y_pred==0)))\n",
    "    return {\"TP\":TP, \"TN\":TN, \"FP\":FP, \"FN\":FN}\n",
    "\n",
    "def precision_recall_accuracy(counts):\n",
    "    TP, TN, FP, FN = counts[\"TP\"], counts[\"TN\"], counts[\"FP\"], counts[\"FN\"]\n",
    "    precision = TP/(TP+FP) if (TP+FP)>0 else 0.0\n",
    "    recall = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    accuracy = (TP+TN)/(TP+TN+FP+FN) if (TP+TN+FP+FN)>0 else 0.0\n",
    "    return precision, recall, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5ce7f",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "**What is this?**\n",
    "The harmonic mean of precision and recall, providing a balanced measure of classifier performance.\n",
    "\n",
    "**Formula:** $F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "**Alternative form:** $F_1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$\n",
    "\n",
    "**Why harmonic mean?**\n",
    "- Penalizes extreme values (unlike arithmetic mean)\n",
    "- If either precision or recall is low, F1 is low\n",
    "- Example: Precision=1.0, Recall=0.1 \u2192 F1=0.18 (not 0.55)\n",
    "\n",
    "**When to use:**\n",
    "- Imbalanced datasets\n",
    "- Need balance between precision and recall\n",
    "- Single metric for model comparison\n",
    "\n",
    "**Variants:**\n",
    "- **F-beta score**: $F_\\beta = (1+\\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\beta^2 \\cdot \\text{Precision} + \\text{Recall}}$\n",
    "- \u03b2 > 1: Favor recall\n",
    "- \u03b2 < 1: Favor precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute F1 score for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels\n",
    "    y_pred : array-like\n",
    "        Predicted binary labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    f1 : float\n",
    "        F1 score\n",
    "    \"\"\"\n",
    "    counts = confusion_counts(y_true, y_pred)\n",
    "    precision, recall, _ = precision_recall_accuracy(counts)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return float(f1)\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1.0):\n",
    "    \"\"\"\n",
    "    Compute F-beta score.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels\n",
    "    y_pred : array-like\n",
    "        Predicted binary labels\n",
    "    beta : float, default=1.0\n",
    "        Weight of recall vs precision\n",
    "        beta > 1: favor recall\n",
    "        beta < 1: favor precision\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fbeta : float\n",
    "        F-beta score\n",
    "    \"\"\"\n",
    "    counts = confusion_counts(y_true, y_pred)\n",
    "    precision, recall, _ = precision_recall_accuracy(counts)\n",
    "    \n",
    "    if (beta**2 * precision + recall) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    fbeta = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n",
    "    return float(fbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad8059",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC\n",
    "\n",
    "**What is this?**\n",
    "ROC (Receiver Operating Characteristic) curve visualizes classifier performance across all threshold values.\n",
    "\n",
    "**How it works:**\n",
    "1. For each threshold, compute TPR and FPR:\n",
    "   - **TPR (True Positive Rate / Recall)**: $\\frac{TP}{TP+FN}$ (y-axis)\n",
    "   - **FPR (False Positive Rate)**: $\\frac{FP}{FP+TN}$ (x-axis)\n",
    "2. Plot (FPR, TPR) points for all thresholds\n",
    "3. Connect points to form ROC curve\n",
    "\n",
    "**AUC (Area Under Curve):**\n",
    "- AUC = 1.0: Perfect classifier\n",
    "- AUC = 0.5: Random classifier (diagonal line)\n",
    "- AUC < 0.5: Worse than random (flip predictions!)\n",
    "\n",
    "**Interpretation:**\n",
    "- Closer to top-left corner = Better classifier\n",
    "- AUC summarizes performance in single number\n",
    "- Threshold-independent metric\n",
    "\n",
    "**When to use:** Comparing classifiers, handling imbalanced data, when costs unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ec5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_curve(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Compute ROC curve points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels (0 or 1)\n",
    "    y_scores : array-like\n",
    "        Predicted scores/probabilities\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fpr : array\n",
    "        False positive rates\n",
    "    tpr : array\n",
    "        True positive rates\n",
    "    thresholds : array\n",
    "        Thresholds used\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    y_scores = np.asarray(y_scores, float)\n",
    "    \n",
    "    # Get unique thresholds (sorted descending)\n",
    "    thresholds = np.unique(y_scores)[::-1]\n",
    "    \n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        counts = confusion_counts(y_true, y_pred)\n",
    "        \n",
    "        # TPR = TP / (TP + FN)\n",
    "        tpr = counts[\"TP\"] / (counts[\"TP\"] + counts[\"FN\"]) if (counts[\"TP\"] + counts[\"FN\"]) > 0 else 0\n",
    "        # FPR = FP / (FP + TN)\n",
    "        fpr = counts[\"FP\"] / (counts[\"FP\"] + counts[\"TN\"]) if (counts[\"FP\"] + counts[\"TN\"]) > 0 else 0\n",
    "        \n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "    \n",
    "    return np.array(fpr_list), np.array(tpr_list), thresholds\n",
    "\n",
    "def compute_auc(fpr, tpr):\n",
    "    \"\"\"Compute Area Under ROC Curve using trapezoidal rule.\"\"\"\n",
    "    # Sort by fpr\n",
    "    indices = np.argsort(fpr)\n",
    "    fpr_sorted = fpr[indices]\n",
    "    tpr_sorted = tpr[indices]\n",
    "    \n",
    "    # Trapezoidal integration\n",
    "    auc = float(np.trapz(tpr_sorted, fpr_sorted))\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fb977",
   "metadata": {},
   "source": [
    "### 5.2 Thresholding Score into Labels\n",
    "\n",
    "**What is this?**\n",
    "Converts continuous scores (e.g., probabilities, confidence values) into binary class labels.\n",
    "\n",
    "**Algorithm:** \n",
    "- If score \u2265 threshold \u2192 Predict class 1 (positive)\n",
    "- If score < threshold \u2192 Predict class 0 (negative)\n",
    "\n",
    "**Threshold selection:**\n",
    "- **threshold = 0.5**: Default for balanced classes\n",
    "- **Lower threshold**: More positive predictions (\u2191 recall, \u2193 precision)\n",
    "- **Higher threshold**: Fewer positive predictions (\u2193 recall, \u2191 precision)\n",
    "\n",
    "**Use cases:**\n",
    "- After logistic regression (threshold probability)\n",
    "- After SVM (threshold decision function)\n",
    "- Tune threshold based on business costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63affe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_label(scores, threshold=0.0):\n",
    "    scores = np.asarray(scores, float)\n",
    "    return (scores >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07f317",
   "metadata": {},
   "source": [
    "### 5.3 Cost-Sensitive Decisions\n",
    "\n",
    "**What is this?**\n",
    "Making classification decisions when different types of errors have different costs.\n",
    "\n",
    "**Cost Components:**\n",
    "- `c_fp`: Cost of False Positive (e.g., unnecessary treatment)\n",
    "- `c_fn`: Cost of False Negative (e.g., missed disease)\n",
    "- `c_tp`: Cost of True Positive (usually 0)\n",
    "- `c_tn`: Cost of True Negative (usually 0)\n",
    "\n",
    "**Total Cost:** $\\text{Cost} = c_{FP} \\cdot FP + c_{FN} \\cdot FN + c_{TP} \\cdot TP + c_{TN} \\cdot TN$\n",
    "\n",
    "**Threshold Sweeping:**\n",
    "Try many threshold values and choose the one that minimizes total cost.\n",
    "\n",
    "**Example:**\n",
    "- Medical test: FN (missed disease) might cost 100x more than FP (unnecessary follow-up)\n",
    "- Lower threshold to catch more cases (\u2191 recall) even if more false alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost_from_counts(counts, c_fp=1.0, c_fn=1.0, c_tp=0.0, c_tn=0.0):\n",
    "    return (counts[\"FP\"]*c_fp + counts[\"FN\"]*c_fn + counts[\"TP\"]*c_tp + counts[\"TN\"]*c_tn)\n",
    "\n",
    "def sweep_thresholds(y_true, scores, thresholds, c_fp=1.0, c_fn=1.0):\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    scores = np.asarray(scores, float)\n",
    "    best = None\n",
    "    for t in thresholds:\n",
    "        y_pred = (scores >= t).astype(int)\n",
    "        counts = confusion_counts(y_true, y_pred)\n",
    "        cost = total_cost_from_counts(counts, c_fp=c_fp, c_fn=c_fn)\n",
    "        row = {\"threshold\": float(t), \"cost\": float(cost), **counts}\n",
    "        if best is None or row[\"cost\"] < best[\"cost\"]:\n",
    "            best = row\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc427845",
   "metadata": {},
   "source": [
    "### 5.4 Logistic Regression (from scratch)\n",
    "\n",
    "**What is this?**\n",
    "A linear model for binary classification that predicts probabilities using the sigmoid function.\n",
    "\n",
    "**Model:** $P(y=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$\n",
    "\n",
    "**How it works:**\n",
    "1. **Predict probability:** Apply sigmoid to linear combination of features\n",
    "2. **Loss function:** Negative log-likelihood (cross-entropy)\n",
    "   - $L = -\\sum [y \\log(p) + (1-y)\\log(1-p)]$\n",
    "3. **Training:** Minimize loss using optimization (e.g., gradient descent, CG)\n",
    "\n",
    "**Output:**\n",
    "- `w`: Feature weights (coefficients)\n",
    "- `b`: Bias (intercept)\n",
    "- Higher probability \u2192 More confident in class 1\n",
    "\n",
    "**Advantages:**\n",
    "- Outputs calibrated probabilities\n",
    "- Interpretable coefficients\n",
    "- Works well for linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_predict_proba(X, w, b):\n",
    "    X = np.asarray(X, float)\n",
    "    z = X @ w + b\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_neg_loglik(params, X, y, eps=1e-12):\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "    w = params[:-1]\n",
    "    b = params[-1]\n",
    "    p = logistic_predict_proba(X, w, b)\n",
    "    p = np.clip(p, eps, 1-eps)\n",
    "    return float(np.sum(-(y*np.log(p) + (1-y)*np.log(1-p))))\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "def fit_logistic(X, y):\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float)\n",
    "    init = np.zeros(X.shape[1] + 1)\n",
    "    res = optimize.minimize(lambda p: logistic_neg_loglik(p, X, y), init, method=\"CG\")\n",
    "    w = res.x[:-1]\n",
    "    b = res.x[-1]\n",
    "    return w, b, res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}