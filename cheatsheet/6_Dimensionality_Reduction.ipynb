{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4773acec",
   "metadata": {},
   "source": [
    "# 6) High Dimensional Phenomena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b764382",
   "metadata": {},
   "source": [
    "### 6.1 Distance Concentration\n",
    "\n",
    "**What is this?**\n",
    "A surprising phenomenon: In high dimensions, distances between random points become very similar!\n",
    "\n",
    "**The Curse of Dimensionality:**\n",
    "As dimension $d$ increases:\n",
    "- All points appear equally far apart\n",
    "- Ratio $\\frac{\\text{max distance}}{\\text{min distance}} \\to 1$\n",
    "- Intuition: In high-D space, there's \"so much room\" that everything spreads out\n",
    "\n",
    "**Why it matters:**\n",
    "- **Nearest neighbor methods fail**: \"Nearest\" and \"farthest\" become meaningless\n",
    "- **Clustering becomes hard**: All points seem equally distant\n",
    "- **Similarity search breaks down**: Need dimension reduction!\n",
    "\n",
    "**Example:** \n",
    "In 2D: Some points close, some far (ratio ~5-10)\n",
    "In 1000D: All points ~same distance (ratio ~1.1)\n",
    "\n",
    "**Takeaway:** High-dimensional data needs special treatment (PCA, manifold learning, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "727bd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_concentration_demo(n=2000, d=2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(0, 1, size=(n, d))\n",
    "    # distances from first point\n",
    "    diffs = X - X[0]\n",
    "    dist = np.linalg.norm(diffs, axis=1)[1:]\n",
    "    return {\n",
    "        \"mean_dist\": float(np.mean(dist)),\n",
    "        \"min_dist\": float(np.min(dist)),\n",
    "        \"max_dist\": float(np.max(dist)),\n",
    "        \"ratio_max_min\": float(np.max(dist)/np.min(dist))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813db43",
   "metadata": {},
   "source": [
    "# 7) Dimensionality Reduction & PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66230ad9",
   "metadata": {},
   "source": [
    "### 7.1 PCA using SVD\n",
    "\n",
    "**What is this?**\n",
    "Principal Component Analysis - finds the directions of maximum variance in your data.\n",
    "\n",
    "**How it works:**\n",
    "1. **Center the data**: Subtract mean from each feature\n",
    "2. **Apply SVD**: $X_c = U \\Sigma V^T$\n",
    "3. **Principal components**: Columns of $V$ (or rows of $V^T$)\n",
    "4. **Variance explained**: Proportional to squared singular values $\\sigma_i^2$\n",
    "\n",
    "**Key outputs:**\n",
    "- **mu**: Mean of original data (for centering)\n",
    "- **Vt**: Principal component directions (rows are PCs)\n",
    "- **s**: Singular values (related to variance)\n",
    "- **evr**: Explained variance ratio per component\n",
    "- **cum_evr**: Cumulative explained variance\n",
    "\n",
    "**Transform:** Project data onto first $k$ components: $Z = X_c V_k$\n",
    "**Inverse:** Reconstruct: $\\hat{X} = Z V_k^T + \\mu$\n",
    "\n",
    "**Use cases:** Dimensionality reduction, visualization, noise removal, feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad4b6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_fit(X, center=True):\n",
    "    X = np.asarray(X, float)\n",
    "    mu = X.mean(axis=0) if center else np.zeros(X.shape[1])\n",
    "    Xc = X - mu\n",
    "    U, s, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    # explained variance ratio\n",
    "    var = s**2\n",
    "    evr = var / var.sum()\n",
    "    return {\"mu\": mu, \"U\": U, \"s\": s, \"Vt\": Vt, \"evr\": evr, \"cum_evr\": np.cumsum(evr)}\n",
    "\n",
    "def pca_transform(X, pca, k):\n",
    "    X = np.asarray(X, float)\n",
    "    Xc = X - pca[\"mu\"]\n",
    "    V = pca[\"Vt\"][:k].T\n",
    "    Z = Xc @ V\n",
    "    return Z\n",
    "\n",
    "def pca_inverse_transform(Z, pca, k):\n",
    "    Z = np.asarray(Z, float)\n",
    "    V = pca[\"Vt\"][:k].T\n",
    "    Xhat = Z @ V.T + pca[\"mu\"]\n",
    "    return Xhat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0264b57",
   "metadata": {},
   "source": [
    "### 7.2 Choosing Number of Components (Keep X% Variance)\n",
    "\n",
    "**What is this?**\n",
    "Determines how many principal components to keep based on desired variance explained.\n",
    "\n",
    "**Algorithm:**\n",
    "Find the smallest $k$ such that cumulative variance \u2265 target (e.g., 90%)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Components: [1,    2,    3,    4,    5]\n",
    "Cum. Var:   [0.5, 0.7, 0.85, 0.92, 0.95]\n",
    "Target = 0.90 \u2192 Choose k=4\n",
    "```\n",
    "\n",
    "**Common targets:**\n",
    "- **90%**: Good balance (retains most information, removes noise)\n",
    "- **95%**: More conservative (less information loss)\n",
    "- **99%**: Very conservative (mostly for compression)\n",
    "\n",
    "**Trade-off:** More components = More information but higher dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b64daca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_k(cum_evr, target=0.90):\n",
    "    cum_evr = np.asarray(cum_evr, float)\n",
    "    return int(np.searchsorted(cum_evr, target) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad12e2",
   "metadata": {},
   "source": [
    "### Scree Plot for Explained Variance\n",
    "\n",
    "**What is this?**\n",
    "A visualization showing variance explained by each principal component, helping decide how many components to keep.\n",
    "\n",
    "**How to read it:**\n",
    "- X-axis: Component number\n",
    "- Y-axis: Explained variance (or variance ratio)\n",
    "- Look for \"elbow\" where curve flattens\n",
    "\n",
    "**Elbow Method:**\n",
    "- Sharp drop initially = Components capture real structure\n",
    "- Flat tail = Components capture noise\n",
    "- Choose k at the elbow (where diminishing returns start)\n",
    "\n",
    "**Alternative: Cumulative plot:**\n",
    "Shows running total of variance explained\n",
    "- Find where curve reaches target (e.g., 90%, 95%)\n",
    "\n",
    "**Example interpretation:**\n",
    "```\n",
    "Components 1-3: Steep drop (keep these)\n",
    "Components 4+: Flat line (likely noise)\n",
    "\u2192 Choose k=3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scree(pca_result, max_components=None):\n",
    "    \"\"\"\n",
    "    Create data for scree plot visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca_result : dict\n",
    "        Result from pca_fit function\n",
    "    max_components : int, optional\n",
    "        Maximum number of components to show\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    component_nums : array\n",
    "        Component numbers (1, 2, 3, ...)\n",
    "    variance_ratios : array\n",
    "        Explained variance ratio for each component\n",
    "    cumulative_variance : array\n",
    "        Cumulative explained variance\n",
    "    \"\"\"\n",
    "    evr = pca_result[\"evr\"]\n",
    "    cum_evr = pca_result[\"cum_evr\"]\n",
    "    \n",
    "    if max_components is not None:\n",
    "        evr = evr[:max_components]\n",
    "        cum_evr = cum_evr[:max_components]\n",
    "    \n",
    "    component_nums = np.arange(1, len(evr) + 1)\n",
    "    \n",
    "    return component_nums, evr, cum_evr\n",
    "\n",
    "def find_elbow_point(variance_ratios, method='max_curvature'):\n",
    "    \"\"\"\n",
    "    Automatically detect elbow point in scree plot.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    variance_ratios : array-like\n",
    "        Explained variance ratios\n",
    "    method : str, default='max_curvature'\n",
    "        Method to find elbow ('max_curvature' or 'threshold')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    elbow_idx : int\n",
    "        Index of elbow point (0-based)\n",
    "    \"\"\"\n",
    "    var = np.asarray(variance_ratios, float)\n",
    "    \n",
    "    if method == 'max_curvature':\n",
    "        # Find point with maximum curvature\n",
    "        # Approximate second derivative\n",
    "        if len(var) < 3:\n",
    "            return 0\n",
    "        \n",
    "        curvature = np.abs(np.diff(var, n=2))\n",
    "        elbow_idx = int(np.argmax(curvature))\n",
    "        return elbow_idx\n",
    "    \n",
    "    elif method == 'threshold':\n",
    "        # Find where variance drops below threshold\n",
    "        threshold = 0.05  # 5% variance\n",
    "        below_threshold = np.where(var < threshold)[0]\n",
    "        if len(below_threshold) > 0:\n",
    "            return int(below_threshold[0])\n",
    "        return len(var) - 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74248b9",
   "metadata": {},
   "source": [
    "### 7.3 Reconstruction Error / Anomaly Detection\n",
    "\n",
    "**What is this?**\n",
    "Uses PCA reconstruction error to detect outliers/anomalies in data.\n",
    "\n",
    "**How it works:**\n",
    "1. **Fit PCA** with $k$ components (keeping most variance)\n",
    "2. **Project & reconstruct**: $\\hat{X} = \\text{PCA}^{-1}(\\text{PCA}(X))$\n",
    "3. **Compute error**: $\\text{error}_i = \\|X_i - \\hat{X}_i\\|$\n",
    "4. **Find anomalies**: Largest errors are outliers\n",
    "\n",
    "**Why it works:**\n",
    "- Normal data follows principal patterns (low reconstruction error)\n",
    "- Anomalies don't fit main patterns (high reconstruction error)\n",
    "- PCA captures \"normal\" structure, anomalies deviate\n",
    "\n",
    "**Use cases:**\n",
    "- Fraud detection\n",
    "- Manufacturing defect detection\n",
    "- Network intrusion detection\n",
    "- Data quality checks\n",
    "\n",
    "**Tip:** Use fewer components (smaller k) for more sensitive anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91cff792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_error(X, Xhat):\n",
    "    X = np.asarray(X, float)\n",
    "    Xhat = np.asarray(Xhat, float)\n",
    "    return np.linalg.norm(X - Xhat, axis=1)\n",
    "\n",
    "def top_anomalies(errors, k=10):\n",
    "    idx = np.argsort(errors)[::-1][:k]\n",
    "    return idx, errors[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7efa86e",
   "metadata": {},
   "source": [
    "### 7.4 PCA Standardization\n",
    "\n",
    "**What is this?**\n",
    "Scales features to have mean=0 and standard deviation=1 before PCA.\n",
    "\n",
    "**Why standardize?**\n",
    "- Features with large scales dominate PCA\n",
    "- Example: Feature A in [0,1000], Feature B in [0,1]\n",
    "  - Without standardization: PC1 \u2248 Feature A direction\n",
    "  - With standardization: Equal importance to both\n",
    "\n",
    "**When to standardize:**\n",
    "- \u2713 Features have different units (meters, dollars, age)\n",
    "- \u2713 Features have very different scales\n",
    "- \u2717 Features already on same scale (e.g., all pixel values 0-255)\n",
    "- \u2717 Scale is meaningful for your problem\n",
    "\n",
    "**Formula:** $X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "**Best practice:** Fit standardization on training data, apply same transform to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61b0d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_fit(X, eps=1e-12):\n",
    "    X = np.asarray(X, float)\n",
    "    mu = X.mean(axis=0)\n",
    "    sd = X.std(axis=0, ddof=0)\n",
    "    sd = np.maximum(sd, eps)\n",
    "    return mu, sd\n",
    "\n",
    "def standardize_apply(X, mu, sd):\n",
    "    X = np.asarray(X, float)\n",
    "    return (X - mu) / sd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1299d0",
   "metadata": {},
   "source": [
    "### 7.5 Whitening Transformation\n",
    "\n",
    "**What is this?**\n",
    "Transforms data so features are uncorrelated AND have unit variance (stricter than standardization).\n",
    "\n",
    "**Formula:** $X_{\\text{whitened}} = X_c \\Sigma^{-1/2} V^T$\n",
    "\n",
    "Where:\n",
    "- $X_c$ = Centered data\n",
    "- $\\Sigma$ = Diagonal matrix of singular values\n",
    "- $V$ = PCA component directions\n",
    "\n",
    "**Effect:**\n",
    "- Removes correlation between features\n",
    "- Makes all features have variance = 1\n",
    "- Spheres the data distribution\n",
    "\n",
    "**When to use:**\n",
    "- Before ICA (Independent Component Analysis)\n",
    "- Some neural networks benefit\n",
    "- When you want truly independent features\n",
    "\n",
    "**Difference from standardization:**\n",
    "- Standardization: Per-feature scaling (doesn't remove correlation)\n",
    "- Whitening: Decorrelates AND scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_whiten(X, pca=None, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Apply whitening transformation using PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Data to whiten\n",
    "    pca : dict, optional\n",
    "        Pre-fitted PCA result. If None, fits PCA on X\n",
    "    eps : float, default=1e-5\n",
    "        Small constant to prevent division by zero\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_whitened : array\n",
    "        Whitened data\n",
    "    pca_result : dict\n",
    "        PCA parameters (for inverse transform)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    \n",
    "    # Fit PCA if not provided\n",
    "    if pca is None:\n",
    "        pca = pca_fit(X, center=True)\n",
    "    \n",
    "    # Center data\n",
    "    X_centered = X - pca[\"mu\"]\n",
    "    \n",
    "    # Whiten: X_white = X_c @ V @ Sigma^(-1)\n",
    "    # This decorrelates and scales to unit variance\n",
    "    V = pca[\"Vt\"].T  # Principal components as columns\n",
    "    s = pca[\"s\"]\n",
    "    \n",
    "    # Prevent division by very small singular values\n",
    "    s_inv = 1.0 / np.maximum(s, eps)\n",
    "    \n",
    "    X_whitened = X_centered @ V @ np.diag(s_inv)\n",
    "    \n",
    "    return X_whitened, pca\n",
    "\n",
    "def pca_unwhiten(X_whitened, pca):\n",
    "    \"\"\"\n",
    "    Inverse whitening transformation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_whitened : array-like\n",
    "        Whitened data\n",
    "    pca : dict\n",
    "        PCA parameters from pca_whiten\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : array\n",
    "        Original-scale data\n",
    "    \"\"\"\n",
    "    X_whitened = np.asarray(X_whitened, float)\n",
    "    \n",
    "    V = pca[\"Vt\"].T\n",
    "    s = pca[\"s\"]\n",
    "    \n",
    "    # Reverse transformation\n",
    "    X_centered = X_whitened @ np.diag(s) @ V.T\n",
    "    X = X_centered + pca[\"mu\"]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4586890",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}